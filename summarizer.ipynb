{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/*\n",
    " * THIS IS SAMPLE CODE\n",
    " *\n",
    " * Copyright 2020 Google\n",
    " *\n",
    " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " * you may not use this file except in compliance with the License.\n",
    " * You may obtain a copy of the License at\n",
    " *\n",
    " * http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " */\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install pytorch\n",
    "%pip install PyMuPDF\n",
    "%pip install ftfy\n",
    "%pip install transformers\n",
    "%pip install bert-extractive-summarizer==0.4.2\n",
    "%pip install tensorflow-hub\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace's transformer library using BERT on pytorch\n",
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "# Load model, model config and tokenizer for SciBert\n",
    "custom_config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config)\n",
    "\n",
    "# HuggingFace's summarizer\n",
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pymupdf library\n",
    "import fitz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to fix utf-encoded text\n",
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements for semantic search with TF sentence encoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import os \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and highlight words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name\n",
    "fname = 'top2vec.pdf'\n",
    "# words to search\n",
    "word_list = ['topic', 'vector', 'semantic', 'transformer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_word(page, text):\n",
    "    \"\"\"Underline each word that contains 'text'.\n",
    "    \"\"\"\n",
    "    found = 0\n",
    "    wlist = page.getTextWords()        # make the word list\n",
    "    for w in wlist:                    # scan through all words on page\n",
    "        if text in w[4]:               # w[4] is the word's string\n",
    "            found += 1                 # count\n",
    "            r = fitz.Rect(w[:4])       # make rect from word bbox\n",
    "            page.addUnderlineAnnot(r)  # underline\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 34 times on page 1\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 3 times on page 1\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 6 times on page 1\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 19 times on page 2\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 11 times on page 2\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 5 times on page 2\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 36 times on page 3\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 35 times on page 3\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 15 times on page 3\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 3 times on page 4\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 13 times on page 4\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 5 times on page 4\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 5 times on page 5\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 31 times on page 5\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 7 times on page 5\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 13 times on page 6\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 22 times on page 6\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 9 times on page 6\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 2 times on page 7\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 7 times on page 7\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 16 times on page 8\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 19 times on page 8\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 8 times on page 8\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 44 times on page 9\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 14 times on page 9\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 3 times on page 9\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 21 times on page 10\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 56 times on page 11\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 5 times on page 11\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 68 times on page 12\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 3 times on page 12\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 8 times on page 12\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 11 times on page 13\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 5 times on page 13\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 1 times on page 13\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 2 times on page 14\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 2 times on page 15\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 1 times on page 17\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 2 times on page 17\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 2 times on page 18\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 1 times on page 18\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 1 times on page 18\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 4 times on page 19\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 2 times on page 19\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 2 times on page 19\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 4 times on page 21\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 1 times on page 22\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 2 times on page 22\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 2 times on page 23\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 1 times on page 23\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 1 times on page 23\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "found 'topic' 3 times on page 24\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "found 'vector' 3 times on page 24\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 2 times on page 24\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n",
      "found 'transformer' 1 times on page 24\n",
      "underlining words containing 'topic' in document 'top2vec.pdf'\n",
      "underlining words containing 'vector' in document 'top2vec.pdf'\n",
      "underlining words containing 'semantic' in document 'top2vec.pdf'\n",
      "found 'semantic' 1 times on page 25\n",
      "underlining words containing 'transformer' in document 'top2vec.pdf'\n"
     ]
    }
   ],
   "source": [
    "new_doc = False                        # indicator if anything found at all\n",
    "\n",
    "for page in doc:                       # scan through the pages\n",
    "    for word in word_list:\n",
    "        print(\"underlining words containing '%s' in document '%s'\" % (word, doc.name))\n",
    "        found = mark_word(page, word)      # mark the page's words\n",
    "        if found:                          # if anything found ...\n",
    "            new_doc = True\n",
    "            print(\"found '%s' %i times on page %i\" % (word, found, page.number + 1))\n",
    "\n",
    "if new_doc:\n",
    "    doc.save(\"marked-\" + doc.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic vs. literal search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "# Load Google Universal Sentence Encoder\n",
    "embed=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "    # similarity is inner product of embedding vectors xTx\n",
    "    corr = np.inner(features, features)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\"\n",
    "    )\n",
    "    g.set_xticklabels(labels, rotation=rotation)\n",
    "    g.set_title(\"Semantic Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_plot(messages_):\n",
    "    message_embeddings_ = embed(messages_)\n",
    "    plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =['big data', 'huge data', 'really large data','cloud computing','aws','azure','gcp','ham','sandwich']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_and_plot(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =['I am a physicist','I studied physics','I am a physician','I studied medicine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_and_plot(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search on paragraphs\n",
    "### Load document as list of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100.13200378417969, 87.67097473144531, 511.31451416015625, 108.41552734375, 'TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS', 0, 0), (240.24697875976562, 147.43089294433594, 371.75347900390625, 170.26512145996094, 'Dimo Angelov dimo.angelov@gmail.com', 1, 0), (277.2959899902344, 199.40899658203125, 334.7088928222656, 214.96270751953125, 'ABSTRACT', 2, 0), (107.86599731445312, 225.84356689453125, 504.30029296875, 368.75750732421875, 'Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present top2vec, which leverages joint document and word semantic embedding to ﬁnd topic vectors. This model does not require stop-word lists, stemming or lemmatization, and it automatically ﬁnds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that top2vec ﬁnds topics which are signiﬁcantly more informative and representative of the corpus trained on than probabilistic generative models.', 3, 0), (72.0, 387.0660095214844, 154.8136749267578, 402.6197204589844, '1 Introduction', 4, 0), (71.69100189208984, 414.9645690917969, 540.004150390625, 470.60650634765625, 'The ability to organize, search and summarize a large volume of text is a ubiquitous problem in natural language processing (NLP). Topic modeling is often used when a large collection of text cannot be reasonably read and sorted through by a person. Given a corpus comprised of many texts, referred to as documents, a topic model will discover the latent semantic structure, or topics, present in the documents. Topics can then be used to ﬁnd high level summaries of a large collection of documents, search for documents of interest, and group similar documents together.', 5, 0), (71.64099884033203, 474.98956298828125, 540.1700439453125, 563.3585205078125, 'A topic is the theme, matter or subject of a text; it is thing being discussed. Topics are often thought of as discrete values, such as politics, science, and religion. However, this is not to the case since any of these topics can be further subdivided into many other sub-topics. Additionally, a topic like politics can overlap with other topics, such as the topic of health, as they can both share the sub-topic of health care. Any of these topics, their combinations or variations can be described by some unique set of weighted words. As such, we assume that topics are continuous, as there are inﬁnitely many combinations of weighted words which can be used to represent a topic. Additionally, we assume that each document has its own topic with a value in that continuum. In this view, the document’s topic is the set of weighted words that are most informative of its unique topic, which can be a combination of the colloquial discrete topics.', 6, 0), (71.64099884033203, 567.7415771484375, 540.0010375976562, 623.3834838867188, 'A useful topic model should ﬁnd topics which represent a high-level summary of the information present in the documents. Each topic’s set of words should represent information contained in the documents. For example, one can infer from a topic containing the words warming, global, temperature, and environment, that the topic is global warming. We deﬁne topic modeling to be the process of ﬁnding topics, as weighted sets of words, that best represent the information of the documents.', 7, 0), (71.64099884033203, 627.7665405273438, 540.0042724609375, 661.5905151367188, 'In the remainder of this section we discuss related work and introduce distributed representations of topics. In Section 2 we describe the top2vec model. Section 3 describes topic information gain and summarizes our experiments, and we conclude in Section 4.', 8, 0), (72.0, 677.8759155273438, 250.79872131347656, 690.8372192382812, '1.1 Traditional Topic Modeling Methods', 9, 0), (71.69100189208984, 699.8855590820312, 540.0042724609375, 722.7994995117188, 'The most widely used topic modeling method is Latent Dirichlet Allocation (LDA) [1]. It is a generative probabilistic model which describes each document as a mixture of topics and each topic as a distribution of words. LDA generalizes', 10, 0), (10.940000534057617, 208.91998291015625, 37.619998931884766, 560.0, 'arXiv:2008.09470v1  [cs.CL]  19 Aug 2020', 11, 0), (72.0, 72.757568359375, 540.0037231445312, 95.67150115966797, 'Probabilistic Latent Semantic Analysis (PLSA) [2] by adding a Dirichlet prior distribution over document-topic and topic-word distributions.', 0, 0), (71.69100189208984, 100.0545654296875, 541.7432861328125, 144.78746032714844, 'LDA and PLSA discretize the continuous topic space into t topics and model documents as mixtures of those t topics. These models assume the number of topics t to be known. The discretization of topics is necessary to model the relationship between documents and words. This is one of the greatest weakness of these models, as the number of topics t or the way to estimate it is rarely known, especially for very large or unfamiliar datasets [3, 4].', 1, 0), (71.64099884033203, 149.1705322265625, 541.7409057617188, 204.81150817871094, 'Each topic produced by these methods is a distribution of word probabilities. As such, the highest probability words in a topic are usually words such as the, and, it and other common words in the language [4]. These common words, also called stop-words, often need to be ﬁltered out in order to make topics interpretable, and extract the informative topic words. Finding the set of stop-words that must be removed is not a trivial problem since it is both language and corpus speciﬁc [5]; a topic model trained on text about dogs will likely treat dog as a stop-word since it is not very informative.', 2, 0), (72.0, 209.195556640625, 541.7466430664062, 264.83648681640625, 'LDA and PLSA use bag-of-words (BOW) representations of documents as input which ignore word semantics. In BOW representation the words Canada and Canadian would be treated as different words, despite their semantic similarity. Stemming and lemmatization techniques aim to address this problem but often make topics harder to understand. Moreover, stemming and lemmatization do not recognize the similarity of words like big and large, which do not share a word stem.', 3, 0), (71.64099884033203, 269.2205810546875, 541.2494506835938, 357.5885009765625, 'The authors of the LDA paper explicitly state: \"We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\" [1]. The objective of probabilistic generative models like LDA and PLSA is to ﬁnd topics which can be used to recreate the original document word distributions with minimal error. However, a large proportion of all text contains uninformative words which may not be considered topical. These models do not differentiate between informative and uninformative words as their goal is to simply recreate the document word distributions. Therefore, the high probability words in topics they ﬁnd do not necessarily correspond to what a user would intuitively think of as being topical.', 4, 0), (72.0, 376.6789245605469, 323.8047790527344, 389.6402587890625, '1.2 Distributed Representations of Words and Documents', 5, 0), (72.0, 399.810546875, 540.3479614257812, 466.36151123046875, 'In neural networks, a distributed representation means each concept learned by the network is represented by many neurons. Each neuron therefore participates in the representation of many concepts. When a neural network’s weights are changed to incorporate new knowledge about a concept, the changes affect the knowledge associated with other concepts that are represented by similar patterns [6]. Distributed representation has the advantage of leading to automatic generalization of the concepts learned. Distributed representations are often central to NLP machine learning techniques for learning vector representations of words and documents.', 6, 0), (71.64099884033203, 470.74456787109375, 540.00439453125, 504.5675048828125, 'Another key idea behind learning vector representations of words and documents is the distributional hypothesis. The essence of the idea is captured by John Rupert Firth who famously said \"You shall know a word by the company it keeps\" [7]. This statement implies that words with similar meanings are used in similar contexts.', 7, 0), (71.64099884033203, 508.9515380859375, 541.746337890625, 586.4104614257812, 'The continuous skip-gram and BOW models [8, 9] known as word2vec, introduced distributed word representations that capture syntactic and semantic word relationships. The word2vec neural network learns word similarity by predicting which adjacent words should be present to a given context word in a sliding window over each document. The learning task of word2vec embraces the idea of distributional semantics, as it learns similar word vectors for words used in similar contexts. It also learns distributed representation of words, in the form of vectors, which facilitates generalization of word representation. The word2vec model generated word vectors produced state-of-the-art results on many linguistics tasks compared to traditional methods [8, 9, 10, 11].', 8, 0), (71.64099884033203, 590.7945556640625, 540.0042114257812, 722.7994995117188, 'There has been interest in methods of ﬁnding distributed word vectors that do not rely on neural networks. It has been shown that the skip-gram version of word2vec is implicitly factorizing a word-context pointwise mutual information (PMI) matrix [12], based on this ﬁnding the authors proposed Shifted Positive PMI word-context representation of words. This has inspired other methods such as GloVe [13], which learn context and word vectors by factorizing a global word-word co-occurrence matrix. Although word2vec implicitly factorizes a word-context PMI matrix, what it explicitly does is maximize the dot product between word vectors for words which co-occur while minimizing dot product between words which do not co-occur. Additionally it uses a neural network which takes advantage of its learned distributed representation of words. This allows the model to learn about all words simultaneously from a single training step on a context word [14]. The ability of word2vec word vectors to capture syntactic and semantic regularities of language that other methods try to recreate is a result of the former points, as is its ability to scale to large corpora [8, 15]. As shown in [10], quantitative comparisons between neural and non-neural word vectors show that neural learned vectors consistently perform better. Results from [12, 16] show that at best non-neural methods achieve', 9, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '2', 10, 0), (71.70099639892578, 72.757568359375, 539.9942016601562, 95.67150115966797, 'results on certain tasks that are on-par with neural methods by replicating hyper-parameters of neural methods like word2vec. These methods, however, lack the ability to scale to large corpora.', 0, 0), (71.53199768066406, 100.0545654296875, 540.247802734375, 188.4235076904297, 'With the goal of overcoming the weaknesses of BOW representations of documents, the distributed paragraph vector was proposed with doc2vec [17]. This model extends word2vec by adding a paragraph vector to the learning task of the neural network. In addition to the context window of words, a paragraph vector is also used to predict which adjacent words should be present. The paragraph vector acts as a memory of the topic of the document; it informs each context window of what information is missing [17]. The doc2vec model can learn distributed representations of varying lengths of text, from sentences to documents. The doc2vec model outperforms BOW models and produces state-of-the-art results on many linguistics tasks compared to traditional methods [17, 18]. The doc2vec model was followed by many works on general language models [19, 20, 21].', 1, 0), (72.0, 238.7179412841797, 254.45506286621094, 251.67929077148438, '1.3 Distributed Representations of Topics', 2, 0), (71.64099884033203, 274.33154296875, 540.0043334960938, 308.1545104980469, 'A semantic space is a spatial representation in which distance represents semantic association [22]. A lot of attention has been given to semantic embedding of words. Speciﬁcally, distributed word vectors generated by models like word2vec which have been shown to capture syntactic and semantic regularities of language [8, 23].', 3, 0), (71.64099884033203, 312.5385437011719, 541.7445678710938, 411.81549072265625, 'The doc2vec model is capable of learning document and word vectors that are jointly embedded in the same space. It has been observed that doing so, or using pre-trained word vectors, improves the quality of the learned document vectors [18]. These jointly embedded document and word vectors are learned such that document vectors are close to word vectors which are semantically similar. This property can be used for information retrieval as word vectors can be used to query for similar documents. It can also be used to ﬁnd which words are most similar to a document, or most representative of a document. As mentioned in [17], the paragraph or document vector acts as a memory of the topic of the document. Thus the most similar word vectors to a document vector are likely the most representative of the document’s topic. This joint document and word embedding is a semantic embedding, since distance in the embedded space measures semantic similarity between the documents and words.', 4, 0), (71.7509994506836, 416.1995544433594, 540.3483276367188, 526.3865356445312, 'In contrast to traditional BOW topic modeling methods, the semantic embedding has the advantage of learning the semantic association between words and documents. We argue that the semantic space itself is a continuous representation of topics, in which each point is a different topic best summarized by its nearest words. In the jointly embedded document and word semantic space, a dense area of documents can be interpreted as many documents that have a similar topic. We use this assumption to propose top2vec, a distributed topic vector which is calculated from dense areas of document vectors. The number of dense areas of documents found in the semantic space is assumed to be the number of prominent topics. The topic vectors are calculated as the centroids of each dense area of document vectors. A dense area is an area of very similar documents, and the centroid, or topic vector, can be thought of as the average document most representative of that area. We leverage the semantic embedding to ﬁnd the words which are most representative of each topic vector by ﬁnding the closest word vectors to each topic vector.', 5, 0), (71.69100189208984, 530.76953125, 540.1719970703125, 619.1384887695312, 'The top2vec model produces jointly embedded topic, document, and word vectors such that distance between them represents semantic similarity. Removing stop-words, lemmatization, stemming, and a priori knowledge of the number of topics are not required for top2vec to learn good topic vectors. This gives top2vec a major advantage over traditional methods. The topic vector can be used to ﬁnd similar documents and words can be used to ﬁnd similar topics. The same vector algebra demonstrated with word2vec [8, 9] can be used between the word, document and topic vectors. The topic vectors allow for topic sizes to be calculated based on each document vector’s nearest topic vector. Additionally topic reduction can be performed on the topic vectors to hierarchically group similar topics and reduce the number of topics discovered.', 6, 0), (71.64099884033203, 623.5215454101562, 540.0043334960938, 722.7994995117188, 'The greatest difference between top2vec and probabilistic generative models is how each models a topic. LDA and PLSA model topics as distributions of words, which are used to recreate the original document word distributions with minimal error. This often necessitates uninformative words which are not topical to have high probabilities in the topics since they make up a large proportion of all text. In contrast a top2vec topic vector in the semantic embedding represents a prominent topic shared among documents. The nearest words to a topic vector best describe the topic and its surrounding documents. This is due to the joint document and word embedding learning task, which is to predict which words are most indicative of a document, which necessitates documents, and therefore topic vectors, to be closest to their most informative words. Our results show that topics found by top2vec are signiﬁcantly more informative and representative of the corpus trained on than those found by LDA and PLSA.', 7, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '3', 8, 0), (72.0, 70.48602294921875, 184.57015991210938, 86.03973388671875, '2 Model Description', 0, 0), (72.0, 96.66893768310547, 216.54733276367188, 109.63027954101562, '2.1 Create Semantic Embedding', 1, 0), (71.64099884033203, 117.7735595703125, 541.7479858398438, 195.2324676513672, 'In order to be able to extract topics, jointly embedded document and word vectors with certain properties are required. Speciﬁcally, we need an embedding where the distance between document vectors and word vectors represents semantic association. Semantically similar documents should be placed close together in the embedding space, and dissimilar documents should be placed further from each other. Additionally words should be close to documents which they best describe. With jointly embedded document and word vectors with these properties, topic vectors can be calculated. This spatial representation of words and documents is called a semantic space [22]. We argue that a semantic space with the outlined properties is a continuous representation of topics. Figure 1 shows an example of a semantic space.', 2, 0), (72.0, 522.530517578125, 539.9996948242188, 545.4445190429688, 'Figure 1: An example of a semantic space. The purple points are documents and the green points are words. Words are closest to documents they best represent and similar documents are close together.', 3, 0), (71.69100189208984, 558.017578125, 541.383056640625, 624.5675048828125, 'To learn jointly embedded document and word vectors we use doc2vec [17, 24]. There are two versions of the model: the Paragraph Vector with Distributed Memory (DM) and Distributed Bag of Words (DBOW). The DM model uses context words and a document vector to predict the target word within context window. The DBOW model uses the document vector to predict words within a context window in the document. Despite DBOW being a simpler model it has been shown to perform better [18]. Our experiments conﬁrm these results and consequently we use the DBOW version of doc2vec.', 4, 0), (71.69100189208984, 628.9515380859375, 540.3499145507812, 684.5924682617188, 'The doc2vec DBOW architecture is very similar to the word2vec skip-gram model which uses the context word to predict surrounding words in the context window. The only difference is that DBOW swaps the context word for the document vector, which is used to predict the surrounding words in the context window. This similarity allows for the training of the two to be interleaved, thus simultaneously learning document and word vectors which are jointly embedded.', 5, 0), (71.64099884033203, 688.9765625, 540.001953125, 724.2935180664062, 'The key insight into how doc2vec and word2vec learn these vectors is understanding how the prediction task works speciﬁcally for DBOW and skip-gram models. The word2vec skip-gram model learns an input word and context word vector for each word in the vocabulary. The word2vec model consists of a matrix Wn,d for input word vectors', 6, 0), (303.3349914550781, 740.6825561523438, 308.3162841796875, 752.6875, '4', 7, 0), (71.64099884033203, 72.757568359375, 541.7437744140625, 263.570068359375, 'and W ′ n,d for context word vectors, where n is the size of the corpus vocabulary, and d is the size of the vectors to be learned for each word. Each row of Wn,d contains a word vector ⃗w ∈ Rd and each row of W ′ n,d contains a context word vector ⃗wc ∈ Rd. For a given context window of size k, there will be k words to the left and k words to the right of the context word. For each of the 2k surrounding words w, their input word vector ⃗w ∈ Wn,d will be used to predict the context vector ⃗wc ∈ W ′ n,d of the context word wc. For each surrounding word w the prediction is softmax(⃗w · W ′ n,d). This generates a probability distribution over the vocabulary, for each word being the context word wc. The learning consists of using back propagation and stochastic gradient descent to update each context word vector in W ′ n,d, and ⃗w from Wn,d, such that the probability of the context vector given the surrounding word, P( ⃗wc|⃗w), is greatest in the probability distribution over the vocabulary. This process is repeated for every context window for all n words. This learning process necessitates semantically similar words to have context word vectors which are close together while making dissimilar words have context word vectors which are distant. This is because in order to maximize the probability P( ⃗wc|⃗w), the value of ⃗w · ⃗wc must be the maximum value in ⃗w · W ′ n,d. This value is maximized when ⃗w is closest to ⃗wc from word all context vectors in W ′ n,d. Therefore the learning process updates ⃗w and W ′ n,d so that ⃗w and ⃗wc are closer together. This can be interpreted as each context word pulling all similar context words towards it in the embedding space, while pushing away all dissimilar words. This results in a semantic space, represented by the context vectors W ′, where all semantically similar words are close together and all dissimilar words are far apart.', 0, 0), (71.69100189208984, 264.12054443359375, 540.0029296875, 345.1230773925781, 'The way the DBOW doc2vec model learns document vectors is similar to the word2vec skip-gram model. The model consists of a matrix Dc,d, where c is the number of documents in the corpus and d is the size of the vectors to be learned for each document. Each row of Dc,d contains a document vector ⃗d ∈ Rd. The model also requires a context word matrix W ′ n,d, which can be pre-trained, randomly initialized, or learned in parallel. For simplicity of the explanation, we will assume a scenario where matrix W ′ n,d has been pre-trained by a word2vec model on the same vocabulary of n words. For each document d in the corpus, the context vector ⃗wc ∈ W ′ n,d of each word in the', 1, 0), (71.69100189208984, 338.8850402832031, 541.741943359375, 464.88751220703125, 'document is used to predict the document’s vector ⃗d ∈ Dc,d. The prediction is softmax( ⃗wc · Dc,d), which generates a probability distribution over the corpus for each document being the document the word is from. The learning consists of using back propagation and stochastic gradient descent to update each document vector in Dc,d and ⃗wc from W ′ n,d, such that the probability of the document given the word, P(⃗d|⃗w), is greatest in the probability distribution over the corpus of documents. This learning process necessitates that document vectors be close to word vectors of words that occur in them and making them distant from word vectors of words that do not occur in them. This can be interpreted as each word attracting documents that are similar to them while repelling documents which are dissimilar to them. This results in a semantic space where documents are closest to the words that best describe them and far from words that are dissimilar to them. Similar documents will be close together in this space as they will be pulled into the same region by similar words. Dissimilar documents will be far apart as they will be attracted into different regions of the semantic space by different words.', 2, 0), (71.53199768066406, 469.27154541015625, 541.7434692382812, 640.9335327148438, 'We argue that the semantic space generated by word2vec and doc2vec is a continuous representation of topics. This claim can be justiﬁed by observing what the learned vector space generated by word2vec represents. This model learns a matrix W ′ n,d, which contains context word vectors of dimension d for all n words it is trained on. Each word vector in this matrix alone has no meaning; it only gives relative similarity to other word vectors in the matrix. We argue that this d dimensional embedding space is a continuous representation of topics deﬁned by the matrix W ′ n,d. The matrix W ′ n,d can be seen as a linear transformation that when applied to a d dimensional vector from the embedding space generates an n dimensional vector. This vector itself is some measure of the strengths of each of the n words in the vocabulary corresponding to the point in the d dimensional space. However what this model has actually learned is how to transform points in the d dimensional space into probability distributions over the n words. Therefore any point ⃗p, in the d dimensional space can be transformed into a probability distribution over the n word vocabulary using softmax(⃗p · W ′ n,d). Thus, any point in the d dimensional space represents a different topic. Each word vector, ⃗wc ∈ W ′ n,d, corresponds to the topic in the d dimension space which has the greatest probability of word wc. In general any point ⃗p in the d dimensional space can be best described semantically by the nearest word vectors, since those are the words that would have the highest probability in its corresponding topic distribution over the n words in the vocabulary.', 3, 0), (71.69100189208984, 645.3165283203125, 540.0010375976562, 722.7764892578125, 'There are several hyper-parameters that have a large impact on the performance of doc2vec [18]. The window size is the number of words left and right of the context word. A window size size of 15 has been found to produce the best results [18], which our experiments support. The doc2vec model can use negative sampling or hierarchical softmax as its output layer. These are both meant to be efﬁcient approximations of the full softmax [9]. We found that in our experiments the hierarchical softmax produces better document vectors. According to [18], the most important hyper-parameter is the sub-sampling threshold, which determines the probability of high frequency words being discarded from a given context window. The suggested sub-sampling threshold value is 105. The smaller this', 4, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '5', 5, 0), (71.7509994506836, 72.757568359375, 540.0028686523438, 172.0344696044922, 'number is, the more likely it is for a high frequency word to be discarded from the context window [9, 18]. A related hyper-parameter is minimum count, which discards all words that have a total frequency that is less than that value from the model all together. This gets rid of extremely rare words which would not contribute to learning the document vectors. In our experiments we found a minimum count of 50 to work best, however this value largely depends on corpus size and its vocabulary. The vector size is the size of the document and word vectors that will be learned, the larger they are the more complex information they can encode. In general, the suggested vector size is 300 [18], with larger data sets larger values will lead to better results, at greater computational cost. The number of training epochs suggested by [18] is 20 to 400, with the higher values for smaller data sets. We found 40 to 400 training epochs to be a good range.', 0, 0), (72.0, 184.7888641357422, 192.85629272460938, 197.75021362304688, '2.2 Find Number of Topics', 1, 0), (71.69100189208984, 205.38653564453125, 541.7438354492188, 250.1184539794922, 'The semantic embedding has the advantage of learning a continuous representation of topics. In the jointly embedded document and word vector space, with the properties outlined in 2.1, documents and words are represented as positions in the semantic space. In this space each document vector can be seen as representing the topic of the document [17]. The word vectors that are nearest to a document vector, are the most semantically descriptive of the document’s topic.', 2, 0), (72.0, 254.5025634765625, 540.0042114257812, 331.96148681640625, 'In the semantic space, a dense area of documents can be interpreted as an area of highly similar documents. This dense area of documents is indicative of an underlying topic that is common to the documents. Since the document vectors represent the topics of the documents, the centroid or average of those vectors can be calculated. This centroid is the topic vector which is most representative of the the dense area of documents it was calculated from. The words that are closest to this topic vector are the words that best describe it semantically. The main assumption behind top2vec is that the number of dense areas of document vectors equals the number of prominent topics. This is a natural way to discretize topics, since a topic is found for each group of documents sharing a prominent topic.', 3, 0), (71.7509994506836, 336.3455505371094, 541.7431030273438, 424.7135009765625, 'In order ﬁnd the dense areas of documents in the semantic space, density based clustering is used on the document vectors, speciﬁcally Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) [25, 26, 27]. However, the \"curse of dimensionality\" which results from the high-dimensional document vectors introduces two main problems. In the high-dimensional semantic embedding space, regularly of 300 dimensions, the document vectors are very sparse. The document vector sparsity makes it difﬁcult to ﬁnd dense clusters and doing so comes at a high computational cost [28]. In order to alleviate these two problems, we perform dimension reduction on the document vectors with the algorithm Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) [29, 30]. In the dimension-reduced space, HDBSCAN can then be used to ﬁnd dense clusters of documents.', 4, 0), (72.0, 436.1229248046875, 274.2108459472656, 449.0842590332031, '2.2.1 Low Dimensional Document Embedding', 5, 0), (71.69100189208984, 455.37554931640625, 541.7433471679688, 532.8355102539062, 'Dimension reduction allows for dense clusters of documents to be found more efﬁciently and accurately in the reduced space. UMAP is a manifold learning technique for dimension reduction with strong theoretical foundations [29, 30]. T-distributed Stochastic Neighbor Embedding (t-SNE) [31] is another popular dimensional reduction technique. We found that t-SNE does not preserve global structure as well as UMAP and it does not scale well to large datasets. Hence, UMAP is chosen for dimension reduction in top2vec, as it preserves local and global structure, and is able to scale to very large datasets. Figure 2 shows UMAP-reduced document vectors; it can be seen that a lot of global and local structure is preserved in the embedding.', 6, 0), (72.0, 537.2185668945312, 540.166748046875, 647.405517578125, 'UMAP has several hyper-parameters that determine how it performs dimension reduction. Perhaps the most important parameter is the number of nearest neighbours, which controls the balance between preserving global structure versus local structure in the low dimensional embedding. Larger values put more emphasis on global over local structure preservation. Since the goal is to ﬁnd dense areas of documents which would be close to each other in the high dimensional space, local structure is more important in this application. We ﬁnd that setting number of nearest neighbours to 15 gives the best results, as this value gives more emphasis on local structure. Another related parameter is the distance metric, which is used to measure the distance between points in the high dimensional space. The often used distance metric for the document vectors is cosine similarity [8, 9], because it measures similarity of documents irrespective of their size. Lastly the embedding dimension must be chosen; we ﬁnd 5 dimensions to give the best results for the downstream task of density based clustering.', 7, 0), (72.0, 658.81494140625, 248.54725646972656, 671.7762451171875, '2.2.2 Find Dense Clusters of Documents', 8, 0), (71.64099884033203, 678.0675659179688, 540.00439453125, 722.7994995117188, 'The goal of density based clustering is to ﬁnd areas of highly similar documents in the semantic space, which indicate an underlying topic. This is performed on the UMAP reduced document vectors. The challenge is that the document vectors will have varying density throughout the semantic space. Additionally there will be sparse areas where documents are highly dissimilar. This can be seen as noise, as there is no prominent underlying topic. In order to overcome these', 9, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '6', 10, 0), (72.0, 290.6005554199219, 539.9962158203125, 313.5144958496094, 'Figure 2: 300 dimensional document vectors from the 20 news groups dataset that are embedded into 2 dimensions using UMAP.', 0, 0), (71.7509994506836, 339.9995422363281, 540.0020751953125, 395.6405029296875, 'challenges, HDBSCAN is used to ﬁnd the dense areas of document vectors, as it was designed to handle both noise and variable density clusters [26]. HDBSCAN assigns a label to each dense cluster of document vectors and assigns a noise label to all document vectors that are not in a dense cluster. The dense areas of identiﬁed document vectors will be used to calculate the topic vectors. Documents that are classiﬁed as noise can be seen as not being descriptive of a prominent topic. Figure 3 shows an example of dense areas of documents identiﬁed by HDBSCAN.', 1, 0), (72.0, 637.8485717773438, 540.0001220703125, 660.7625122070312, 'Figure 3: UMAP-reduced document vectors from the 20 news groups dataset. Each colored area of points is a dense area of documents identiﬁed by HDBSCAN, red points are documents HDBSCAN has labeled as noise.', 2, 0), (71.69100189208984, 678.0675659179688, 541.2489013671875, 722.7994995117188, 'The main hyper-parameter that needs be chosen for HDBSCAN is minimum cluster size; this parameter is at the core of how the algorithm ﬁnds clusters of varying density [26]. This parameter represents the smallest size that should be considered a cluster by the algorithm. We ﬁnd that a minimum cluster size of 15 gives the best results in our experiments, as larger values have a higher chance of merging unrelated document clusters.', 3, 0), (303.385009765625, 740.6825561523438, 308.3663024902344, 752.6875, '7', 4, 0), (72.0, 72.39891815185547, 194.8787078857422, 85.36026000976562, '2.3 Calculate Topic Vectors', 0, 0), (72.0, 95.74993133544922, 318.9629211425781, 108.71127319335938, '2.3.1 Calculate Centroids in Original Dimensional Space', 1, 0), (71.69100189208984, 118.11553955078125, 541.23974609375, 162.8474578857422, 'The dense clusters of documents and noise documents identiﬁed by HDBSCAN in the UMAP-reduced dimension, correspond to locations in the original semantic embedding space. The use of UMAP and HDBSCAN can be seen as a process which labels each document in the semantic embedding space with either a noise label or a label for the dense cluster to which it belongs.', 2, 0), (71.64099884033203, 167.2315673828125, 541.7420043945312, 255.59947204589844, 'Given labels for each cluster of dense documents in the semantic embedding space, topic vectors can be calculated. There are a number of ways that the topic vector can be calculated from the document vectors. The simplest method is to calculate the centroid, i.e. the arithmetic mean of all the document vectors in the same dense cluster. There are other reasonable options such as the geometric mean or using probabilities from the conﬁdence of clusters created by HDBSCAN. We experimented with these techniques and found that they resulted in very similar topic vectors, with almost identical nearest-neighbour word vectors. We speculate that this is mainly due to the sparsity of the high dimensional space. Therefore, we decided to use the simple method of calculating the centroid. Figure 4 shows a visual example of a topic vector being calculated from a dense area of documents.', 3, 0), (72.0, 512.2955322265625, 539.9996948242188, 535.2095336914062, 'Figure 4: The topic vector is the centroid of the dense are of documents identiﬁed by HDBSCAN, which are the purple points. The outliers identiﬁed by HDBSCAN are not used to calculate the centroid.', 4, 0), (71.69100189208984, 553.571533203125, 540.1722412109375, 576.4865112304688, 'The centroid is calculated for each set of document vectors that belong do a dense cluster, generating a topic vector for each set. The number of dense areas found is the number of prominent topics identiﬁed in the corpus.', 5, 0), (72.0, 595.6768798828125, 178.281005859375, 608.63818359375, '2.3.2 Find Topic Words', 6, 0), (71.64099884033203, 618.0425415039062, 541.7420043945312, 684.5924682617188, 'In the semantic space, every point represents a topic that is best described semantically by its nearest word vectors. Therefore the word vectors that are closest to a topic vector are those that are most representative of it semantically. The distance of each word vector to the topic vector will indicate how semantically similar the word is to the topic. The words closest to the topic vector can be seen as the words that are most similar to all documents into the dense area, as the topic vector is the centroid of that area. These words can be used to summarize the common topic of the documents in the dense area. Figure 5 shows an example of a topic vector and the nearest words.', 7, 0), (72.0, 688.9765625, 540.3430786132812, 722.7994995117188, 'Common words appear in most documents and, as such, they are often in a region of the semantic space that is equally distant from all documents. As a result the words closest to a topic vector will rarely be stop-words, which has been conﬁrmed in our experiments. Therefore there is no need for stop-word removal.', 8, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '8', 9, 0), (158.75399780273438, 309.8795471191406, 453.2483825683594, 321.8844909667969, 'Figure 5: The topic words are the nearest word vectors to the topic vector.', 0, 0), (72.0, 350.1828918457031, 284.19342041015625, 363.14422607421875, '2.4 Topic Size and Hierarchical Topic Reduction', 1, 0), (71.64099884033203, 374.3945617675781, 540.0039672851562, 419.1264953613281, 'The topic and document vectors allow for the size of topics to be calculated. The topic vectors can be used to partition the document vectors such that each document vector belongs to its nearest topic vector. This associates each document with exactly one topic, the one which is most semantically similar to the document. The size of each topic is measured as the number of documents that belong to it.', 2, 0), (71.64099884033203, 423.51055908203125, 541.7439575195312, 500.9695129394531, 'An advantage of the topic vectors and the continuous representation of topics in the semantic space is that the number of topics found by top2vec can be hierarchically reduced to any number of topics less than the number initially found. This is done by iteratively merging the smallest topic into its most semantically similar topic until the desired number of topics are reached. This is done by taking a weighted arithmetic mean of the topic vector of the smallest topic and its nearest topic vector, each weighted by their topic size. After each merge, the topic sizes are recalculated for each topic. This hierarchical topic reduction has the advantage of ﬁnding the topics which are most representative of the corpus, as it biases topics with greater size.', 3, 0), (72.0, 524.781005859375, 127.12542724609375, 540.334716796875, '3 Results', 4, 0), (72.0, 555.6239013671875, 195.65576171875, 568.585205078125, '3.1 Topic Information Gain', 5, 0), (71.64099884033203, 579.8355712890625, 539.9986572265625, 613.6585083007812, 'A natural way to evaluate topic models is to score how well the topics describe the documents. This evaluation measures how informative the topics are to a user. We propose using mutual information [32] to measure the information gained about the documents when described by their topic words.', 6, 0), (71.69100189208984, 618.0425415039062, 540.1658325195312, 651.865478515625, 'Traditional topic modeling methods discretize topic space and describe documents as a mixture of those topics. In order to evaluate a set of these topics T generated from documents D, the total information gained is calculated for each document when described with the proportions of topics given by the topic model.', 7, 0), (71.69100189208984, 656.2495727539062, 541.73876953125, 722.7994995117188, 'In contrast, top2vec learns a continuous representation of topics and places documents in that space corresponding to their topic. A topic vector found by top2vec represents the topic common to a group of documents, or the average of their individual topics. In order to evaluate a set of top2vec topics T generated from documents D, the documents are partitioned into sub-sets, with each sub-set corresponding to document vectors with the same nearest topic vector. Thus each document is assigned to exactly one topic. To evaluate these topics, the total information gained is measured for each of the sub-set of documents when described by the words nearest to their topic vector.', 8, 0), (303.5090026855469, 740.6825561523438, 308.49029541015625, 752.6875, '9', 9, 0), (71.69100189208984, 72.757568359375, 541.3821411132812, 84.76250457763672, 'The total information gained, or mutual information, about all documents D when described by all words W, is given by:', 0, 0), (207.3000030517578, 100.81485748291016, 272.5519714355469, 137.99526977539062, 'I(D, W) = �', 1, 0), (257.2389831542969, 124.4994888305664, 273.26849365234375, 136.5850830078125, 'd∈D', 2, 0), (277.8909606933594, 100.81485748291016, 292.2769470214844, 137.99526977539062, '�', 3, 0), (275.1439514160156, 96.23184967041016, 390.6114196777344, 136.508056640625, 'w∈W P(d, w)log � P(d, w)', 4, 0), (352.52801513671875, 117.33206939697266, 396.1624755859375, 127.29466247558594, 'P(d)P(w)', 5, 0), (397.36700439453125, 96.2319107055664, 404.699462890625, 133.41232299804688, '�', 6, 0), (528.384033203125, 110.78558349609375, 540.00048828125, 122.79051971435547, '(1)', 7, 0), (71.69100189208984, 139.67852783203125, 539.9979858398438, 173.5015106201172, 'The contribution of each co-occurrence between a document d and word w to the information gain calculation can be seen as the probability-weighted amount of information (PWI) d and w contribute to the total amount of information [33], given by:', 8, 0), (221.10400390625, 184.97091674804688, 376.8074645996094, 222.15133666992188, 'PWI(d, w) = P(d, w)log � P(d, w)', 9, 0), (338.7229919433594, 206.07205200195312, 382.3574523925781, 216.03465270996094, 'P(d)P(w)', 10, 0), (383.5619812011719, 184.97085571289062, 540.0004272460938, 222.15127563476562, '� (2)', 11, 0), (71.69100189208984, 224.37554931640625, 541.24951171875, 269.1085205078125, 'Topics are distributions over the entire vocabulary W. However, in order to evaluate their usefulness to a user, we evaluate them using the top n words of the topic. For evaluation where each document is assigned to only one topic, each topic t ∈ T, will have a set of n words Wt ⊂ W and documents Dt ⊂ D. The information gained about all documents when described by their corresponding topic is given by:', 12, 0), (183.56300354003906, 285.160888671875, 249.92098999023438, 322.34130859375, 'PWI(T) = �', 13, 0), (235.65200805664062, 308.76849365234375, 248.73135375976562, 320.8540954589844, 't∈T', 14, 0), (254.0590057373047, 285.160888671875, 268.44500732421875, 322.34130859375, '�', 15, 0), (251.58599853515625, 308.8454895019531, 270.42047119140625, 320.93109130859375, 'd∈Dt', 16, 0), (276.4469909667969, 285.160888671875, 290.8329772949219, 322.34130859375, '�', 17, 0), (272.5820007324219, 280.577880859375, 390.28448486328125, 320.8540954589844, 'w∈Wt P(d, w)log � P(d, w)', 18, 0), (352.20001220703125, 301.67803955078125, 395.83447265625, 311.6406555175781, 'P(d)P(w)', 19, 0), (397.03900146484375, 280.5778503417969, 404.3714599609375, 317.7582702636719, '�', 20, 0), (225.0189971923828, 317.5668640136719, 249.92098999023438, 354.7472839355469, '= �', 21, 0), (235.65200805664062, 341.1744689941406, 248.73135375976562, 353.26007080078125, 't∈T', 22, 0), (254.0590057373047, 317.5668640136719, 268.44500732421875, 354.7472839355469, '�', 23, 0), (251.58599853515625, 341.25146484375, 270.42047119140625, 353.3370666503906, 'd∈Dt', 24, 0), (276.44598388671875, 317.5668640136719, 290.83197021484375, 354.7472839355469, '�', 25, 0), (272.58197021484375, 312.9838562011719, 414.34747314453125, 353.26007080078125, 'w∈Wt P(d|w)P ′(w)log � P(d, w)', 26, 0), (376.2640075683594, 334.08404541015625, 419.8984680175781, 344.0466613769531, 'P(d)P(w)', 27, 0), (421.1029968261719, 311.5695495605469, 540.0004272460938, 350.1642761230469, '� (3)', 28, 0), (72.0, 357.6165466308594, 540.00439453125, 417.0910949707031, 'In equation 3, P(w) is the marginal probability of the word w across all documents D. It is used to calculate the log term, which is the pointwise mutual information [34] between w and d. P ′(w) is the probability of topic word w, which is used to calculate the expected mutual information [32], or the information gained about document d given topic word w. The quantity we are measuring is the information gained about each document given its corresponding topic words as a prior. Therefore P ′(w) is 1 and can be omitted [33], which gives rise to:', 29, 0), (196.4259796142578, 429.3108825683594, 262.7829895019531, 466.4913024902344, 'PWI(T) = �', 30, 0), (248.5139923095703, 452.9184875488281, 261.59332275390625, 465.00408935546875, 't∈T', 31, 0), (266.92095947265625, 429.3108825683594, 281.30694580078125, 466.4913024902344, '�', 32, 0), (264.4479675292969, 452.9954833984375, 283.2824401855469, 465.0810852050781, 'd∈Dt', 33, 0), (289.3089599609375, 429.3108825683594, 303.6949462890625, 466.4913024902344, '�', 34, 0), (285.4449462890625, 424.7278747558594, 401.4864196777344, 465.00408935546875, 'w∈Wt P(d|w)log � P(d, w)', 35, 0), (363.4020080566406, 445.8280334472656, 407.0364685058594, 455.7906494140625, 'P(d)P(w)', 36, 0), (408.2409973144531, 424.72784423828125, 415.5734558105469, 461.90826416015625, '�', 37, 0), (528.3839721679688, 439.51654052734375, 540.0004272460938, 451.521484375, '(4)', 38, 0), (71.64099884033203, 468.6435546875, 539.9986572265625, 495.39007568359375, 'Alternatively the equation can be generalized for the case that each document is represented by multiple topics. In this case we replace P ′(w) with P(t), which is the proportion of words to be used to represent document d by topic t:', 39, 0), (188.40792846679688, 507.6098937988281, 255.69290161132812, 544.7903442382812, 'PWI(T) = �', 40, 0), (240.37991333007812, 531.2944946289062, 256.4094543457031, 543.3800659179688, 'd∈D', 41, 0), (258.284912109375, 507.6098937988281, 272.6708984375, 544.7903442382812, '�', 42, 0), (258.40191650390625, 531.217529296875, 271.4802551269531, 543.3031005859375, 't∈T', 43, 0), (278.1999206542969, 507.6098937988281, 292.5859069824219, 544.7903442382812, '�', 44, 0), (274.3359069824219, 503.02685546875, 409.5033264160156, 543.3031005859375, 'w∈Wt P(d|w)P(t)log � P(d, w)', 45, 0), (371.4200134277344, 524.1270751953125, 415.053466796875, 534.0896606445312, 'P(d)P(w)', 46, 0), (416.25799560546875, 503.02685546875, 423.5904541015625, 540.207275390625, '�', 47, 0), (528.3839721679688, 517.7765502929688, 540.0004272460938, 529.781494140625, '(5)', 48, 0), (71.7509994506836, 546.8645629882812, 541.2456665039062, 657.051513671875, 'Using Equations 4 and 5, different sets of topics can be compared. A greater quantity of information gain indicates that the topics t ∈ T are more informative of their corresponding documents. If topics contain words such as the, and, and it or other intuitively uninformative words, they will receive lower information gain values. This is in large part due to the P(d|w) term in the calculation, since the probability of any speciﬁc document given a very common word is very low. Therefore, the information gained is also low. Words that are mostly present in the subset of documents corresponding to the topic lead to higher information gain as they are informative of those documents. Additionally, low values of information gain will be obtained if the topic model assigns topics to the wrong documents. Topic information gain measures the quality of the words in the topic and their assignment to documents. Therefore, Equations 4 and 5 give values that correspond with what is intuitively more informative. We argue that due to its information theoretic derivation, topic information gain is a good measure for evaluating topic models.', 49, 0), (72.0, 668.6429443359375, 306.5892639160156, 681.604248046875, '3.2 LDA, PLSA and Top2Vec Topic Information Gain', 50, 0), (72.0, 688.9765625, 540.347412109375, 722.7994995117188, 'In order to evaluate LDA, PLSA and top2vec topics we train all models on the same documents D and vocabulary W. Since top2vec automatically ﬁnds the number of topics, we compare LDA, PLSA and top2vec on increasing numbers of topics up to the amount discovered by top2vec.', 51, 0), (300.6449890136719, 740.6825561523438, 310.6075744628906, 752.6875, '10', 52, 0), (72.0, 72.757568359375, 541.6541137695312, 107.01847076416016, 'For each comparison between a set of LDA-generated topics, TLDA, PLSA-generated topics, TP LSA and top2vec- generated topics, Ttop2vec, we use the same number of top n topic words and the same number of topics. Thus, for each comparison between TLDA, TP LSA, and Ttop2vec, we ensure the following:', 0, 0), (97.90301513671875, 118.22301483154297, 326.1247863769531, 135.5081329345703, '• |TLDA| = |TP LSA| = |Ttop2vec| = number of topics', 1, 0), (97.90303039550781, 134.175048828125, 363.49383544921875, 152.9541778564453, '• |Wt| = n, ∀Wt ∈ TLDA, TP LSA, Ttop2vec = top n topic words', 2, 0), (72.00003051757812, 156.6349334716797, 206.02688598632812, 169.59628295898438, '3.2.1 20 News Groups Dataset', 3, 0), (71.69100189208984, 176.09954833984375, 540.0018920898438, 220.83250427246094, 'The 20 News Groups dataset [35] contains 18,831 posts labelled with the news group they were posted in. We trained top2vec, LDA and PLSA models on this dataset using the same pre-processing steps. LDA and PLSA models were trained with 10 to 100 topics, with intervals of 10. Hierarchical topic reduction was used on the 103 topics discovered by top2vec.', 4, 0), (71.69100189208984, 225.215576171875, 540.0043334960938, 280.85650634765625, 'To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the topic information gain for models trained on the 20 news groups dataset can is shown in Figure 6. The results show that the top n topic words from top2vec consistently provide more information than PLSA and LDA, with varying topic sizes and with up to the top 1000 topic words. Even when stop-words are ﬁltered from LDA and PLSA. For most topic sizes the top 20 words from top2vec convey as much information as the top 100 from LDA and PLSA.', 5, 0), (71.25299835205078, 285.2405700683594, 540.0017700195312, 329.9725036621094, 'Tables 1 and 2 show the topics for top2vec and LDA models with topic size of 20. LDA was chosen over PLSA as it had higher topic information gain for 20 topics. Topics are ordered by increasing information gain. The topics shown for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by the model. Tables 1 and 2 demonstrate that the topic information gain score corresponds to what is intuitively more informative.', 6, 0), (72.0, 334.3565673828125, 539.999755859375, 400.906494140625, 'In Table 2, LDA topics 2, 3 and 5 appear to contain nonsensical tokens, yet they have a high information gain. The 20 news groups data set contains messages that were sent encrypted or contain source code. When the 20 news groups messages are tokenized, these tokens are treated as words by the models. Thus, LDA has actually found informative tokens for that set of messages. However, that set contains only 23 messages. Therefore, LDA has found 3 different topics out of the requested 20, which only represent 23 messages out of the 18831 total amount the LDA model is trained on. This highlights an advantage of top2vec when ﬁnding the number of topics.', 7, 0), (71.69100189208984, 405.2905578613281, 541.7479248046875, 537.2954711914062, 'Figure 7 shows the semantic embedding of the messages labeled by the news group each message was posted in. This ﬁgure shows that the semantic embedding has learned the similarity of messages by visually demonstrating the continuous representation of topics. Messages from similar newsgroups are in similar regions of the semantic space. The small red points on the very right of Figure 7, are the 23 messages which predominantly contain encrypted content or large quantities of source code. Due to the density of that set of messages, top2vec ﬁnds a topic for those messages. However, when hierarchical topic reduction is performed to reduce the topic size to 20, due to its small size, the topic of the encrypted and source code containing messages is merged into another topic that is most semantically similar to it. The semantic embedding of the messages labeled with the 20 top2vec topics from Table 1 that each belong to is shown in Figure 8. It demonstrates the assignment of the posts to the 20 topics correspond almost exactly to the 20 news groups and that each topic’s top 3 words are very informative of the news group’s actual topic. This visually demonstrates that top2vec ﬁnds topics which are more representative of the corpus as a whole, as conﬁrmed by the topic information gain score in Figure 6.', 8, 0), (71.64099884033203, 541.6785888671875, 540.3630981445312, 651.865478515625, 'Topic information gain measures how informative topic words are of documents. Therefore, low scores are achieved when uninformative topic words are chosen, as well as when topics are assigned either to wrong documents or with incorrect proportions. There are a number of LDA topics in Table 2 that appear to be very coherent and that correspond to speciﬁc news groups. However, they have low scores in comparison to similar top2vec topics in Table 1. This is explained, in part, by LDA’s modeling of documents as a mixture of topics. It models each document with non-zero probabilities of all topics. Therefore each of the messages will have some non-zero proportion of the topics 2, 3 and 5 that were generated from encrypted or source code containing messages. Figure 9 shows the contribution of each LDA topic from Table 2 to all messages. It demonstrates that the most informative topics are highly localized and that the uninformative topics are spread out over many messages. Topic 15 and 17, which both have low information gain, make up a large proportion of most messages. These are topics with very generic words that are found in most documents.', 9, 0), (71.69100189208984, 656.2495727539062, 541.7446899414062, 722.7994995117188, 'The goal of LDA is to ﬁnd topics such that their words recreate the original document word distributions with minimal error. This includes stop-words such as the, and, it and other generic words that would not be considered informative or topical by a user. This explains topic 15 and 17 which are just the generic words that occur in most documents. LDA’s goal can also result in extremely speciﬁc topics, such as 2, 3, and 5, which necessitate other topics to be more general. Figure 9 visually demonstrates the reason that LDA topics produce lower information gain; it ﬁnds many unlocalized and therefore uninformative topics compared to top2vec.', 10, 0), (300.6449890136719, 740.6825561523438, 310.6075744628906, 752.6875, '11', 11, 0), (72.0, 72.757568359375, 540.1651000976562, 139.30747985839844, 'Figure 6 shows that as the number of topics increases, the topic information gain for top2vec is consistently higher than for LDA and PLSA. This is because top2vec topics are more localized in the semantic space and therefore more informative. The number of topics found by top2vec on the 20 News Groups data set is 103, and are even more localized than the 20 topics in Table 1 which were generated from hierarchichal topic reduction. The original topics discovered in the region of topic 7 and 14 are shown in Figure 10 and Figure 11. These topics are even more localized than the reduced topics and therefore more informative as indicated by the information gain scores in Figure 6.', 0, 0), (72.0, 157.0579071044922, 202.31080627441406, 170.01925659179688, '3.2.2 Yahoo Answers Dataset', 1, 0), (71.25299835205078, 178.8465576171875, 541.7474365234375, 223.57847595214844, 'The Yahoo Answers dataset [36, 37], contains 1.3 million labelled posts. The posts are from 10 different topics, with 130,000 posts per topic. The number of topics top2vec found in this dataset are 2,618. Due to the computational cost of training LDA and PLSA models, we were only able to train the models from 10 to 100 topics with intervals of 10. Hierarchical topic reduction was used on the topics discovered by top2vec.', 2, 0), (71.64099884033203, 227.96258544921875, 540.0043334960938, 294.5124816894531, 'To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the topic information gain for models trained on the Yahoo Answers dataset can be seen in Figure 12. These results are consistent with the results from the 20 News Groups dataset. They show that the top n topic words from top2vec consistently provide more information than PLSA and LDA, with varying topic sizes and up to the top 1000 topic words. Even when stop-words are ﬁltered from LDA and PLSA. For most topic sizes, the top 20 words from top2vec convey as much information as the top 100 from LDA and PLSA.', 3, 0), (71.69100189208984, 298.89654541015625, 541.74755859375, 354.5375061035156, 'Tables 3 and 4 show the topics for top2vec and LDA models with a topic size of 10. The topics are ordered by increasing information gain. LDA was chosen over PLSA because it had higher topic information gain for 10 topics. The topics shown for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by the model. This comparison demonstrates the interpretability of the topics and their associated information gain score, showing that the more informative topics receive higher information gain.', 4, 0), (71.64099884033203, 358.92156982421875, 541.2418823242188, 425.47149658203125, 'Figure 13 shows the semantic embedding of Yahoo Answers posts with their true topic labels. This ﬁgure demonstrates that the semantic space has captured the similarity of posts that share a similar topic. Figure 14 shows the posts labelled with the top2vec topics from Table 3. It demonstrates that the assignment of the posts to the 10 topics correspond almost exactly to the 10 topic labels from the Yahoo Answers dataset and that the topic’s top 3 words are very informative of the true topic. This visually demonstrates that top2vec ﬁnds topics that are representative of the corpus as a whole, as conﬁrmed by the topic information gain score in Figure 12.', 5, 0), (72.0, 429.8555603027344, 540.0015258789062, 496.4054870605469, 'Figure 15 shows the strengths of each of the 10 LDA topics from Table 4 across all posts. This visually demonstrates that more informative topics are localized in the semantic space, and that LDA discovers topics that are less localized than top2vec topics. Additionally highly unlocalized LDA topics like 9 and 10, which contain the lowest information gain scores, also contain generic words that would not be considered topical or informative by a user. Figure 15 demonstrates visually that, apart from LDA topics containing less informative words, the reason LDA topics receive lower topic information gain is that they are less localized than top2vec topics and therefore less informative.', 6, 0), (72.0, 517.5230102539062, 143.7431640625, 533.0767211914062, '4 Discussion', 7, 0), (71.53199768066406, 547.1085205078125, 540.3485717773438, 602.74951171875, 'We have described top2vec, an unsupervised learning algorithm that ﬁnds topic vectors in a semantic space of jointly embedded document and word vectors. We have shown that the semantic space is a continuous representation of topics that allows for the calculation of topic vectors from dense areas of highly similar documents, topic size, and for hierarchical topic reduction. The top2vec model also allows for comparing similarity between words, documents and topics based on distance in the semantic space.', 8, 0), (71.53199768066406, 607.133544921875, 540.0036010742188, 662.7744750976562, 'We have proposed a novel method for evaluating topics that uses mutual information to calculate how informative topics are of documents. The topic information gain measures the amount of information gained about the documents when described by their topic words. This measures both the quality of topic words and the assignment of topics to the documents. Our results show that top2vec consistently ﬁnds topics that are more informative and representative of the corpus than LDA and PLSA, for varying sizes of topics and number of top topic words.', 9, 0), (71.69100189208984, 667.1585693359375, 541.6563720703125, 722.7994995117188, 'There are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary advantages are that it automatically ﬁnds the number of topics and ﬁnds topics that are more informative and representa- tive of the corpus. As demonstrated, stop-word lists are not required to ﬁnd informative topic words, making it easy to use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges of traditional methods that use BOW representations of words, which ignore word semantics.', 10, 0), (300.64501953125, 740.6825561523438, 310.60760498046875, 752.6875, '12', 11, 0), (71.64099884033203, 72.757568359375, 540.0020141601562, 128.3984832763672, 'Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to ﬁnd topics that recreate the original documents word distributions with minimal loss. This necessitates these models to place uninformative words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no guarantee that they will ﬁnd topics that are representative of the corpus. The results show they can ﬁnd topics that are extremely speciﬁc or overly broad.', 0, 0), (71.64099884033203, 132.78253173828125, 541.2425537109375, 221.15049743652344, 'In contrast, the words closest to top2vec topic vectors are the words that are most informative of the documents the topic vectors are calculated from. This is due to the learning task that generates joint document and word vectors, which predicts the document a word came from. This learning task necessitates document vectors to be placed close to the words that are most informative of the documents. The continuous representation of topics in the semantic space allows topic vectors to be calculated from dense areas of those documents. Thus top2vec topics are the words that are most informative of a document, rather than the set of words that recreate the documents distribution of words with accurate proportions. We suggest that top2vec is more appropriate for ﬁnding informative and representative topics of a corpus than probabilistic generative models like LDA and PLSA.', 1, 0), (71.69100189208984, 224.68017578125, 310.4536437988281, 237.5394744873047, 'The top2vec code is available as an open-source project1.', 2, 0), (84.65299987792969, 710.6676635742188, 227.1550750732422, 722.51953125, '1https://github.com/ddangelov/Top2Vec', 3, 0), (300.64501953125, 740.6825561523438, 310.60760498046875, 752.6875, '13', 4, 0), (71.69100189208984, 188.2725830078125, 540.0034790039062, 211.18650817871094, 'Table 1: Topic information gain for the top 10 words from top2vec topics trained on the 20 news groups dataset with 20 topics.', 0, 0), (77.97799682617188, 220.17088317871094, 538.230224609375, 233.13223266601562, 'Topic Number Topic Words PWI(T)', 1, 0), (77.97799682617188, 236.08091735839844, 522.4585571289062, 249.04226684570312, '1 pitching, pitchers, pitcher, hitter, batting, hit, hitters, baseball, batters, inning 74.2', 2, 0), (77.97799682617188, 252.65892028808594, 522.4585571289062, 275.9315185546875, '2 bike, ride, riding, bikes, motorcycle, bikers, helmet, riders, countersteering, 71.9 passenger', 3, 0), (77.97799682617188, 280.14593505859375, 522.4586791992188, 293.1072692871094, '3 circuit, voltage, circuits, resistor, signal, khz, impedance, analog, diode, resistors 69.1', 4, 0), (77.97799682617188, 296.7249450683594, 522.4586181640625, 309.686279296875, '4 centris, ram, mhz, quadra, nubus, vram, iisi, lciii, cpu, fpu 62.4', 5, 0), (77.97799682617188, 313.3029479980469, 522.4586181640625, 336.5755310058594, '5 patient, symptoms, patients, doctor, disease, treatment, jxp, therapy, skepticism, 59.1 physician', 6, 0), (77.97799682617188, 340.7899475097656, 522.4586181640625, 353.75128173828125, '6 koresh, fbi, compound, batf, davidians, atf, waco, raid, ﬁre, bd 54.7', 7, 0), (77.97799682617188, 357.3679504394531, 522.458740234375, 370.32928466796875, '7 israel, arab, arabs, israeli, jews, palestinians, israelis, war, peace, occupied 54.3', 8, 0), (77.97799682617188, 373.94696044921875, 522.4586181640625, 386.9082946777344, '8 orbit, space, launch, orbital, satellites, lunar, shuttle, spacecraft, moon, earth 53.7', 9, 0), (77.97799682617188, 390.52496337890625, 522.4586791992188, 403.4862976074219, '9 clipper, nsa, encryption, encrypted, secure, keys, crypto, algorithm, escrow, scheme 51.6', 10, 0), (77.97799682617188, 407.10296630859375, 522.4586791992188, 420.0643005371094, '10 controller, drives, drive, ide, scsi, ﬂoppy, bios, disk, jumpers, esdi 50.3', 11, 0), (77.97799682617188, 423.6819763183594, 522.4586791992188, 436.643310546875, '11 windows, drivers, ati, cica, driver, exe, card, autoexec, mode, ini 50.1', 12, 0), (77.97799682617188, 440.2599792480469, 522.4586791992188, 453.2213134765625, '12 car, engine, cars, ford, brakes, honda, tires, valve, wheel, rear 49.7', 13, 0), (77.97799682617188, 456.8379821777344, 522.4586791992188, 469.79931640625, '13 hockey, playoffs, nhl, game, season, team, playoff, teams, scoring, play 48.0', 14, 0), (77.97799682617188, 473.4159851074219, 522.4586791992188, 486.3773193359375, '14 gun, guns, ﬁrearms, laws, weapons, handgun, crime, amendment, handguns, ﬁrearm 46.6', 15, 0), (77.97799682617188, 489.9949951171875, 522.4585571289062, 502.9563293457031, '15 window, application, xlib, manager, openwindows, motif, server, xview, client, clients 43.6', 16, 0), (77.97799682617188, 506.572998046875, 522.4585571289062, 519.5343017578125, '16 jesus, christ, god, bible, church, scripture, christians, scriptures, christian, heaven 38.9', 17, 0), (77.97799682617188, 523.1510009765625, 522.4586791992188, 536.1123046875, '17 postscript, format, printer, fonts, ﬁles, formats, font, truetype, bitmap, image 36.7', 18, 0), (77.97799682617188, 539.72900390625, 522.4585571289062, 552.6903076171875, '18 shipping, sale, offer, condition, asking, brand, sell, obo, price, selling 36.0', 19, 0), (77.97799682617188, 556.3079833984375, 522.4585571289062, 569.269287109375, '19 atheists, belief, religion, beliefs, god, christianity, truth, religions, believe, atheist 34.4', 20, 0), (77.97799682617188, 572.885986328125, 524.9482421875, 596.1585693359375, '20 please, mail, post, email, posting, address, thanks, reply, interested, appreciate 11.3 ____', 21, 0), (505.02301025390625, 600.3729858398438, 527.4389038085938, 613.3342895507812, '996.6', 22, 0), (300.6449890136719, 740.6825561523438, 310.6075744628906, 752.6875, '14', 23, 0), (71.69100189208984, 191.95855712890625, 539.9951171875, 214.8724822998047, 'Table 2: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the 20 news groups dataset with 20 topics.', 0, 0), (98.7239990234375, 223.85691833496094, 510.78704833984375, 236.81826782226562, 'Topic Number Topic Words PWI(T)', 1, 0), (98.7239990234375, 239.7668914794922, 495.0165100097656, 252.72824096679688, '1 la, pit, gm, det, bos, tor, pts, chi, vs, min 49.9', 2, 0), (98.72399139404297, 256.34490966796875, 495.0164794921875, 269.3062438964844, '2 hz, cx, ww, uw, qs, c_, pl, lk, ck, ah 47.0', 3, 0), (98.72399139404297, 272.92291259765625, 495.0165100097656, 285.8842468261719, '3 ax, max, pl, di, tm, ei, giz, wm, bhj, ey 42.6', 4, 0), (98.72399139404297, 289.5019226074219, 495.0164794921875, 302.4632568359375, '4 db, period, goal, play, pp, shots, st, power, mov, bh 27.9', 5, 0), (98.72399139404297, 306.0799255371094, 495.01654052734375, 319.041259765625, '5 mk, mm, mp, mh, mu, mr, mj, mo, mq, mx 27.7', 6, 0), (98.72399139404297, 322.6579284667969, 495.0165710449219, 335.6192626953125, '6 health, medical, new, study, research, disease, cancer, use, patients, drug 19.0', 7, 0), (98.72399139404297, 339.2359313964844, 495.0164489746094, 352.197265625, '7 armenian, people, said, one, armenians, turkish, went, us, children, turkey 17.3', 8, 0), (98.72399139404297, 355.81494140625, 495.0166015625, 368.7762756347656, '8 dos, windows, drive, card, system, disk, mb, scsi, pc, mac 16.7', 9, 0), (98.72399139404297, 372.3929443359375, 495.0165100097656, 385.3542785644531, '9 ﬁle, program, window, ﬁles, image, jpeg, use, windows, display, color 15.7', 10, 0), (98.72399139404297, 388.970947265625, 495.0164794921875, 401.9322814941406, '10 government, president, law, would, mr, israel, state, rights, fbi, states 13.4', 11, 0), (98.72399139404297, 405.5499572753906, 495.01641845703125, 418.51129150390625, '11 god, jesus, bible, church, christian, christ, christians, faith, lord, man 12.2', 12, 0), (98.72399139404297, 422.1279602050781, 495.01654052734375, 435.08929443359375, '12 game, team, games, hockey, season, teams, league, nhl, new, players 12.0', 13, 0), (98.72399139404297, 438.7059631347656, 495.0165100097656, 451.66729736328125, '13 space, nasa, earth, launch, shuttle, orbit, moon, satellite, solar, mission 11.8', 14, 0), (98.72399139404297, 455.2839660644531, 490.03521728515625, 468.24530029296875, '14 edu, ftp, graphics, available, pub, image, mail, com, version, also 9.7', 15, 0), (98.72399139404297, 471.86297607421875, 490.03515625, 484.8243103027344, '15 would, know, anyone, get, thanks, like, one, please, help, could 8.5', 16, 0), (98.72399139404297, 488.44097900390625, 490.0351867675781, 501.4023132324219, '16 key, use, data, system, one, information, may, encryption, used, number 7.5', 17, 0), (98.72399139404297, 505.01898193359375, 490.03515625, 517.9802856445312, '17 people, would, one, think, know, like, say, even, see, way 7.3', 18, 0), (98.72399139404297, 521.5969848632812, 490.0352478027344, 534.5582885742188, '18 one, car, would, like, get, time, much, also, back, power 7.1', 19, 0), (98.72399139404297, 538.1759643554688, 490.0352783203125, 551.1372680664062, '19 edu, com, please, list, mail, sale, send, email, price, offer 4.0', 20, 0), (98.72399139404297, 554.7539672851562, 492.52386474609375, 578.0265502929688, '20 think, year, would, good, time, last, well, get, one, got 3.6 ___', 21, 0), (477.5799865722656, 582.240966796875, 499.9958190917969, 595.2022705078125, '360.9', 22, 0), (300.6449890136719, 740.6825561523438, 310.6075744628906, 752.6875, '15', 23, 0), (71.64099884033203, 701.0345458984375, 539.9973754882812, 723.948486328125, 'Figure 6: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the 20 News Groups dataset. LDA* and PLSA* have stop-words removed.', 0, 0), (300.64501953125, 740.6825561523438, 310.60760498046875, 752.6875, '16', 1, 0), (71.7509994506836, 365.0695495605469, 539.9992065429688, 387.9834899902344, 'Figure 7: Semantic embedding of 20 news groups messages labeled by news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 0, 0), (71.7509994506836, 692.7525634765625, 539.9959106445312, 715.66650390625, 'Figure 8: The 20 news groups messages labeled with the top2vec topics from Table 1. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 1, 0), (300.6449890136719, 740.6825561523438, 310.6075744628906, 752.6875, '17', 2, 0), (72.0, 664.3995361328125, 539.9983520507812, 698.2224731445312, 'Figure 9: Topic proportion of each LDA topic from Table 2 across all 20 news groups messages in the semantic embedding. The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 0, 0), (300.64501953125, 740.6825561523438, 310.60760498046875, 752.6875, '18', 1, 0), (72.0, 357.320556640625, 540.0032958984375, 391.14349365234375, 'Figure 10: Zoom in of top2vec original topics found in region of topic 7 from Table 1. This region of the semantic space corresponds to the talk.politics.mideast news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 0, 0), (72.0, 685.5755615234375, 540.0030517578125, 719.3984985351562, 'Figure 11: Zoom in of top2vec original topics found in region of topic 14 from Table 1. This region of the semantic space corresponds to the talk.politics.guns news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 1, 0), (300.64501953125, 740.6825561523438, 310.60760498046875, 752.6875, '19', 2, 0), (72.0, 699.5225830078125, 540.0001220703125, 722.4365234375, 'Figure 12: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the Yahoo Answers dataset. LDA* and PLSA* have stop-words removed.', 0, 0), (301.01898193359375, 740.6825561523438, 310.9815673828125, 752.6875, '20', 1, 0), (71.25299835205078, 91.41058349609375, 539.9954833984375, 114.32451629638672, 'Table 3: Topic information gain for the top 10 words from top2vec topics trained on the Yahoo Answers dataset with 10 topics.', 0, 0), (77.97799682617188, 123.30889129638672, 543.689697265625, 136.27023315429688, 'Topic Number Topic Words PWI(T)', 1, 0), (77.97799682617188, 139.21788024902344, 532.8993530273438, 152.17922973632812, '1 overwrite, rebooting, debug, debugging, reboot, executable, compiler, winxp, xp, winnt 112.2', 2, 0), (77.97799682617188, 155.79685974121094, 532.8994750976562, 179.06944274902344, '2 securities, unpaid, equity, purchaser, payment, broker, underwriting, issuer, payable, 104.4 underwriter', 3, 0), (77.97799682617188, 183.2838592529297, 527.9181518554688, 206.5564422607422, '3 regimen, discomfort, inﬂammation, swelling, psoriasis, pufﬁness, inﬂammatory, 98.1 irritation, edema, hypertension', 4, 0), (77.97799682617188, 210.77085876464844, 527.9180908203125, 234.04344177246094, '4 realtionship, realationship, insecurities, conﬁde, hurtful, inlove, clingy, friendship, 92.5 bestfriend, friendships', 5, 0), (77.97799682617188, 238.2588348388672, 527.9181518554688, 251.22018432617188, '5 song, sings, singer, sang, artist, duet, album, lyrics, ballad, vocalist 91.9', 6, 0), (77.97799682617188, 254.8368377685547, 527.9180297851562, 278.10943603515625, '6 scripture, believers, righteousness, righteous, pious, spiritual, spirituality, sinful, 82.2 worldly, discernment', 7, 0), (77.97799682617188, 282.3238525390625, 527.918212890625, 295.2851867675781, '7 team, players, game, teams, scoring, league, teammate, scorers, playoff, defensively 80.0', 8, 0), (77.97799682617188, 298.9028625488281, 527.918212890625, 322.1754455566406, '8 courses, subjects, curriculum, students, teaching, faculty, syllabus, academic, 78.6 undergraduate, baccalaureate', 9, 0), (77.97799682617188, 326.3898620605469, 527.9180908203125, 349.6624450683594, '9 war, leaders, politicians, government, democracy, political, terrorists, terrorism, 64.6 partisan, policies', 10, 0), (77.97799682617188, 353.8768615722656, 527.918212890625, 366.83819580078125, '10 thus, constant, hence, surface, resulting, greater, therefore, becomes, occurs, larger 33.1', 11, 0), (510.48297119140625, 370.81451416015625, 530.4082641601562, 382.8194580078125, '____', 12, 0), (510.48297119140625, 387.03387451171875, 532.89892578125, 399.9952087402344, '837.6', 13, 0), (71.39199829101562, 452.62054443359375, 540.0016479492188, 475.53448486328125, 'Table 4: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the Yahoo Answers dataset with 10 topics.', 14, 0), (92.33300018310547, 484.5189208984375, 517.1781616210938, 497.4802551269531, 'Topic Number Topic Words PWI(T)', 15, 0), (92.33300018310547, 500.4289245605469, 501.4064636230469, 513.3902587890625, '1 team, game, world, win, cup, play, de, football, best, player 23.2', 16, 0), (92.33299255371094, 517.0069580078125, 501.4065246582031, 529.96826171875, '2 computer, yahoo, use, get, click, internet, free, com, need, windows 22.3', 17, 0), (92.33299255371094, 533.5849609375, 501.40643310546875, 546.5462646484375, '3 people, us, country, war, world, would, american, bush, government, america 18.0', 18, 0), (92.33299255371094, 550.1639404296875, 501.4065246582031, 563.125244140625, '4 one, water, two, would, light, number, energy, used, earth, use 16.9', 19, 0), (92.33299255371094, 566.741943359375, 501.4064636230469, 579.7032470703125, '5 body, weight, also, doctor, eat, blood, may, day, get, pain 15.7', 20, 0), (92.33299255371094, 583.3199462890625, 501.4064636230469, 596.28125, '6 www, com, http, ﬁnd, song, name, know, anyone, org, music 14.2', 21, 0), (92.33299255371094, 599.89794921875, 501.40643310546875, 612.8592529296875, '7 get, money, school, would, need, work, pay, good, business, job 13.1', 22, 0), (92.33299255371094, 616.4769287109375, 501.4064636230469, 629.438232421875, '8 god, people, one, life, believe, jesus, word, many, would, us 12.5', 23, 0), (92.33299255371094, 633.054931640625, 496.4250793457031, 646.0162353515625, '9 like, know, get, think, would, want, people, good, really, go 9.1', 24, 0), (92.33299255371094, 649.6329345703125, 496.4252014160156, 662.59423828125, '10 time, like, friend, said, guy, back, would, one, years, got 8.3', 25, 0), (483.97100830078125, 666.569580078125, 503.89617919921875, 678.5745239257812, '____', 26, 0), (483.97100830078125, 682.7899169921875, 506.3868408203125, 695.751220703125, '153.3', 27, 0), (301.0190124511719, 740.6825561523438, 310.9815979003906, 752.6875, '21', 28, 0), (72.0, 364.2635498046875, 540.0005493164062, 387.177490234375, 'Figure 13: Semantic embedding of Yahoo Answers posts with true labels. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 0, 0), (72.0, 692.7525634765625, 539.9976806640625, 715.66650390625, 'Figure 14: Yahoo Answers posts labeled with the top2vec topics from Table 3. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 1, 0), (301.01898193359375, 740.6825561523438, 310.9815673828125, 752.6875, '22', 2, 0), (71.69100189208984, 683.2805786132812, 541.74169921875, 717.103515625, 'Figure 15: Topic proportion of each LDA topic from Table 4 across all Yahoo Answers posts in the semantic embedding. The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded into 2 dimensions using UMAP.', 0, 0), (301.01898193359375, 740.6825561523438, 310.9815673828125, 752.6875, '23', 1, 0), (72.0, 70.48602294921875, 127.54383850097656, 86.03973388671875, 'References', 0, 0), (76.98100280761719, 96.24853515625, 541.2415771484375, 119.16246795654297, '[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March 2003.', 1, 0), (76.98100280761719, 122.612548828125, 540.0020751953125, 145.52647399902344, '[2] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50–57, 1999.', 2, 0), (76.98095703125, 148.9765625, 540.0038452148438, 171.89048767089844, '[3] Jordan L Boyd-Graber and David M Blei. Syntactic topic models. In Advances in neural information processing systems, pages 185–192, 2009.', 3, 0), (76.98100280761719, 175.33953857421875, 539.9984130859375, 198.2534637451172, '[4] S. Syed and M. Spruit. Full-text or abstract? examining topic coherence scores using latent dirichlet allocation. In 2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 165–174, 2017.', 4, 0), (76.98098754882812, 201.70355224609375, 540.1722412109375, 235.52647399902344, '[5] Kai Yang, Yi Cai, Zhenhong Chen, Ho-fung Leung, and Raymond Lau. Exploring topic discriminating power of words in latent dirichlet allocation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2238–2247, 2016.', 5, 0), (76.98100280761719, 238.9765625, 540.0025634765625, 261.8905029296875, '[6] Geoffrey E Hinton et al. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12. Amherst, MA, 1986.', 6, 0), (76.98100280761719, 265.33953857421875, 541.2452392578125, 288.2535095214844, '[7] Henry Widdowson. J.R. Firth, 1957, papers in linguistics 1934–51. International Journal of Applied Linguistics, 17:402 – 413, 10 2007.', 7, 0), (76.98100280761719, 291.70355224609375, 540.1642456054688, 314.61749267578125, '[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space, 2013.', 8, 0), (76.98100280761719, 318.0675354003906, 541.2427978515625, 351.8905029296875, '[9] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.', 9, 0), (72.0, 355.3395690917969, 540.0034790039062, 400.072509765625, '[10] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 238–247, Baltimore, Maryland, June 2014. Association for Computational Linguistics.', 10, 0), (72.0, 403.52154541015625, 539.9995727539062, 426.43548583984375, '[11] Zhuang Bairong, Wang Wenbo, Li Zhiyu, Zheng Chonghui, and Takahiro Shinozaki. Comparative analysis of word embedding methods for dstc6 end-to-end conversation modeling track.', 11, 0), (72.0, 429.8855285644531, 540.0020751953125, 452.79949951171875, '[12] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177–2185, 2014.', 12, 0), (72.0, 456.2485656738281, 541.745361328125, 490.072509765625, '[13] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.', 13, 0), (72.0, 493.52154541015625, 541.745361328125, 516.4354858398438, '[14] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137–1155, 2003.', 14, 0), (71.99998474121094, 519.885498046875, 541.655029296875, 553.70849609375, '[15] Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa- tions. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746–751, 2013.', 15, 0), (72.0, 557.1585693359375, 539.9978637695312, 580.072509765625, '[16] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225, 2015.', 16, 0), (72.0, 583.5215454101562, 541.2424926757812, 606.4354858398438, '[17] Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. ArXiv, abs/1405.4053, 2014.', 17, 0), (72.0, 609.8855590820312, 541.2426147460938, 643.70849609375, '[18] Jey Han Lau and Timothy Baldwin. An empirical evaluation of doc2vec with practical insights into document embedding generation. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 78–86, Berlin, Germany, August 2016. Association for Computational Linguistics.', 18, 0), (72.0, 647.1585693359375, 539.9996337890625, 670.072509765625, '[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.', 19, 0), (71.99996948242188, 673.5215454101562, 540.0007934570312, 696.4354858398438, '[20] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.', 20, 0), (72.00003051757812, 699.8855590820312, 540.3450927734375, 722.7994995117188, '[21] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.', 21, 0), (301.0190124511719, 740.6825561523438, 310.9815979003906, 752.6875, '24', 22, 0), (72.0, 72.757568359375, 541.655029296875, 143.29246520996094, '[22] Thomas L Grifﬁths, Mark Steyvers, and Joshua B Tenenbaum. Topics in semantic representation. Psychological review, 114(2):211, 2007. [23] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa- tions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics.', 0, 0), (71.99996948242188, 145.26556396484375, 541.747314453125, 516.6425170898438, '[24] Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010. ELRA. http://is.muni.cz/publication/884893/en. [25] Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering based on hierarchical density estimates. In Paciﬁc-Asia conference on knowledge discovery and data mining, pages 160–172. Springer, 2013. [26] Leland McInnes and John Healy. Accelerated hierarchical density based clustering. 2017 IEEE International Conference on Data Mining Workshops (ICDMW), Nov 2017. [27] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. The Journal of Open Source Software, 2(11):205, 2017. [28] RB Marimont and MB Shapiro. Nearest neighbour searches and the curse of dimensionality. IMA Journal of Applied Mathematics, 24(1):59–70, 1979. [29] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [30] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation and projection. The Journal of Open Source Software, 3(29):861, 2018. [31] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008. [32] M Cover Thomas and A Thomas Joy. Elements of information theory. New York: Wiley, 3:37–38, 1991. [33] Akiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Processing & Management, 39(1):45–65, 2003. [34] Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29, 1990. [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [36] Jamaal Hay Wenpeng Yin and Dan Roth. Benchmarking zero-shot text classiﬁcation: Datasets, evaluation and entailment approach. In EMNLP, 2019. [37] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.', 1, 0), (301.01898193359375, 740.6825561523438, 310.9815673828125, 752.6875, '25', 2, 0)]\n"
     ]
    }
   ],
   "source": [
    "with fitz.open(fname) as doc:\n",
    "    blocks = []\n",
    "    for page in doc:\n",
    "        blocks += page.getText('blocks')\n",
    "\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS ',\n",
       " 'Dimo Angelov dimo.angelov@gmail.com ',\n",
       " 'ABSTRACT Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present top2vec, which leverages joint document and word semantic embedding to find topic vectors. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that top2vec finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models. ',\n",
       " '1 Introduction The ability to organize, search and summarize a large volume of text is a ubiquitous problem in natural language processing (NLP). Topic modeling is often used when a large collection of text cannot be reasonably read and sorted through by a person. Given a corpus comprised of many texts, referred to as documents, a topic model will discover the latent semantic structure, or topics, present in the documents. Topics can then be used to find high level summaries of a large collection of documents, search for documents of interest, and group similar documents together. ',\n",
       " \"A topic is the theme, matter or subject of a text; it is thing being discussed. Topics are often thought of as discrete values, such as politics, science, and religion. However, this is not to the case since any of these topics can be further subdivided into many other sub-topics. Additionally, a topic like politics can overlap with other topics, such as the topic of health, as they can both share the sub-topic of health care. Any of these topics, their combinations or variations can be described by some unique set of weighted words. As such, we assume that topics are continuous, as there are infinitely many combinations of weighted words which can be used to represent a topic. Additionally, we assume that each document has its own topic with a value in that continuum. In this view, the document's topic is the set of weighted words that are most informative of its unique topic, which can be a combination of the colloquial discrete topics. \",\n",
       " \"A useful topic model should find topics which represent a high-level summary of the information present in the documents. Each topic's set of words should represent information contained in the documents. For example, one can infer from a topic containing the words warming, global, temperature, and environment, that the topic is global warming. We define topic modeling to be the process of finding topics, as weighted sets of words, that best represent the information of the documents. \",\n",
       " 'In the remainder of this section we discuss related work and introduce distributed representations of topics. In Section 2 we describe the top2vec model. Section 3 describes topic information gain and summarizes our experiments, and we conclude in Section 4. ',\n",
       " '1.1 Traditional Topic Modeling Methods ',\n",
       " 'The most widely used topic modeling method is Latent Dirichlet Allocation (LDA) [1]. It is a generative probabilistic model which describes each document as a mixture of topics and each topic as a distribution of words. LDA generalizes ',\n",
       " 'arXiv:2008.09470v1  [cs.CL]  19 Aug 2020 ',\n",
       " 'Probabilistic Latent Semantic Analysis (PLSA) [2] by adding a Dirichlet prior distribution over document-topic and topic-word distributions. ',\n",
       " 'LDA and PLSA discretize the continuous topic space into t topics and model documents as mixtures of those t topics. These models assume the number of topics t to be known. The discretization of topics is necessary to model the relationship between documents and words. This is one of the greatest weakness of these models, as the number of topics t or the way to estimate it is rarely known, especially for very large or unfamiliar datasets [3, 4]. ',\n",
       " 'Each topic produced by these methods is a distribution of word probabilities. As such, the highest probability words in a topic are usually words such as the, and, it and other common words in the language [4]. These common words, also called stop-words, often need to be filtered out in order to make topics interpretable, and extract the informative topic words. Finding the set of stop-words that must be removed is not a trivial problem since it is both language and corpus specific [5]; a topic model trained on text about dogs will likely treat dog as a stop-word since it is not very informative. ',\n",
       " 'LDA and PLSA use bag-of-words (BOW) representations of documents as input which ignore word semantics. In BOW representation the words Canada and Canadian would be treated as different words, despite their semantic similarity. Stemming and lemmatization techniques aim to address this problem but often make topics harder to understand. Moreover, stemming and lemmatization do not recognize the similarity of words like big and large, which do not share a word stem. ',\n",
       " 'The authors of the LDA paper explicitly state: \"We refer to the latent multinomial variables in the LDA model as topics, so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words.\" [1]. The objective of probabilistic generative models like LDA and PLSA is to find topics which can be used to recreate the original document word distributions with minimal error. However, a large proportion of all text contains uninformative words which may not be considered topical. These models do not differentiate between informative and uninformative words as their goal is to simply recreate the document word distributions. Therefore, the high probability words in topics they find do not necessarily correspond to what a user would intuitively think of as being topical. ',\n",
       " '1.2 Distributed Representations of Words and Documents ',\n",
       " \"In neural networks, a distributed representation means each concept learned by the network is represented by many neurons. Each neuron therefore participates in the representation of many concepts. When a neural network's weights are changed to incorporate new knowledge about a concept, the changes affect the knowledge associated with other concepts that are represented by similar patterns [6]. Distributed representation has the advantage of leading to automatic generalization of the concepts learned. Distributed representations are often central to NLP machine learning techniques for learning vector representations of words and documents. \",\n",
       " 'Another key idea behind learning vector representations of words and documents is the distributional hypothesis. The essence of the idea is captured by John Rupert Firth who famously said \"You shall know a word by the company it keeps\" [7]. This statement implies that words with similar meanings are used in similar contexts. ',\n",
       " 'The continuous skip-gram and BOW models [8, 9] known as word2vec, introduced distributed word representations that capture syntactic and semantic word relationships. The word2vec neural network learns word similarity by predicting which adjacent words should be present to a given context word in a sliding window over each document. The learning task of word2vec embraces the idea of distributional semantics, as it learns similar word vectors for words used in similar contexts. It also learns distributed representation of words, in the form of vectors, which facilitates generalization of word representation. The word2vec model generated word vectors produced state-of-the-art results on many linguistics tasks compared to traditional methods [8, 9, 10, 11]. ',\n",
       " 'There has been interest in methods of finding distributed word vectors that do not rely on neural networks. It has been shown that the skip-gram version of word2vec is implicitly factorizing a word-context pointwise mutual information (PMI) matrix [12], based on this finding the authors proposed Shifted Positive PMI word-context representation of words. This has inspired other methods such as GloVe [13], which learn context and word vectors by factorizing a global word-word co-occurrence matrix. Although word2vec implicitly factorizes a word-context PMI matrix, what it explicitly does is maximize the dot product between word vectors for words which co-occur while minimizing dot product between words which do not co-occur. Additionally it uses a neural network which takes advantage of its learned distributed representation of words. This allows the model to learn about all words simultaneously from a single training step on a context word [14]. The ability of word2vec word vectors to capture syntactic and semantic regularities of language that other methods try to recreate is a result of the former points, as is its ability to scale to large corpora [8, 15]. As shown in [10], quantitative comparisons between neural and non-neural word vectors show that neural learned vectors consistently perform better. Results from [12, 16] show that at best non-neural methods achieve ',\n",
       " '2 results on certain tasks that are on-par with neural methods by replicating hyper-parameters of neural methods like word2vec. These methods, however, lack the ability to scale to large corpora. ',\n",
       " 'With the goal of overcoming the weaknesses of BOW representations of documents, the distributed paragraph vector was proposed with doc2vec [17]. This model extends word2vec by adding a paragraph vector to the learning task of the neural network. In addition to the context window of words, a paragraph vector is also used to predict which adjacent words should be present. The paragraph vector acts as a memory of the topic of the document; it informs each context window of what information is missing [17]. The doc2vec model can learn distributed representations of varying lengths of text, from sentences to documents. The doc2vec model outperforms BOW models and produces state-of-the-art results on many linguistics tasks compared to traditional methods [17, 18]. The doc2vec model was followed by many works on general language models [19, 20, 21]. ',\n",
       " '1.3 Distributed Representations of Topics ',\n",
       " 'A semantic space is a spatial representation in which distance represents semantic association [22]. A lot of attention has been given to semantic embedding of words. Specifically, distributed word vectors generated by models like word2vec which have been shown to capture syntactic and semantic regularities of language [8, 23]. ',\n",
       " \"The doc2vec model is capable of learning document and word vectors that are jointly embedded in the same space. It has been observed that doing so, or using pre-trained word vectors, improves the quality of the learned document vectors [18]. These jointly embedded document and word vectors are learned such that document vectors are close to word vectors which are semantically similar. This property can be used for information retrieval as word vectors can be used to query for similar documents. It can also be used to find which words are most similar to a document, or most representative of a document. As mentioned in [17], the paragraph or document vector acts as a memory of the topic of the document. Thus the most similar word vectors to a document vector are likely the most representative of the document's topic. This joint document and word embedding is a semantic embedding, since distance in the embedded space measures semantic similarity between the documents and words. \",\n",
       " 'In contrast to traditional BOW topic modeling methods, the semantic embedding has the advantage of learning the semantic association between words and documents. We argue that the semantic space itself is a continuous representation of topics, in which each point is a different topic best summarized by its nearest words. In the jointly embedded document and word semantic space, a dense area of documents can be interpreted as many documents that have a similar topic. We use this assumption to propose top2vec, a distributed topic vector which is calculated from dense areas of document vectors. The number of dense areas of documents found in the semantic space is assumed to be the number of prominent topics. The topic vectors are calculated as the centroids of each dense area of document vectors. A dense area is an area of very similar documents, and the centroid, or topic vector, can be thought of as the average document most representative of that area. We leverage the semantic embedding to find the words which are most representative of each topic vector by finding the closest word vectors to each topic vector. ',\n",
       " \"The top2vec model produces jointly embedded topic, document, and word vectors such that distance between them represents semantic similarity. Removing stop-words, lemmatization, stemming, and a priori knowledge of the number of topics are not required for top2vec to learn good topic vectors. This gives top2vec a major advantage over traditional methods. The topic vector can be used to find similar documents and words can be used to find similar topics. The same vector algebra demonstrated with word2vec [8, 9] can be used between the word, document and topic vectors. The topic vectors allow for topic sizes to be calculated based on each document vector's nearest topic vector. Additionally topic reduction can be performed on the topic vectors to hierarchically group similar topics and reduce the number of topics discovered. \",\n",
       " 'The greatest difference between top2vec and probabilistic generative models is how each models a topic. LDA and PLSA model topics as distributions of words, which are used to recreate the original document word distributions with minimal error. This often necessitates uninformative words which are not topical to have high probabilities in the topics since they make up a large proportion of all text. In contrast a top2vec topic vector in the semantic embedding represents a prominent topic shared among documents. The nearest words to a topic vector best describe the topic and its surrounding documents. This is due to the joint document and word embedding learning task, which is to predict which words are most indicative of a document, which necessitates documents, and therefore topic vectors, to be closest to their most informative words. Our results show that topics found by top2vec are significantly more informative and representative of the corpus trained on than those found by LDA and PLSA. ',\n",
       " '3 2 Model Description 2.1 Create Semantic Embedding ',\n",
       " 'In order to be able to extract topics, jointly embedded document and word vectors with certain properties are required. Specifically, we need an embedding where the distance between document vectors and word vectors represents semantic association. Semantically similar documents should be placed close together in the embedding space, and dissimilar documents should be placed further from each other. Additionally words should be close to documents which they best describe. With jointly embedded document and word vectors with these properties, topic vectors can be calculated. This spatial representation of words and documents is called a semantic space [22]. We argue that a semantic space with the outlined properties is a continuous representation of topics. Figure 1 shows an example of a semantic space. ',\n",
       " 'Figure 1: An example of a semantic space. The purple points are documents and the green points are words. Words are closest to documents they best represent and similar documents are close together. ',\n",
       " 'To learn jointly embedded document and word vectors we use doc2vec [17, 24]. There are two versions of the model: the Paragraph Vector with Distributed Memory (DM) and Distributed Bag of Words (DBOW). The DM model uses context words and a document vector to predict the target word within context window. The DBOW model uses the document vector to predict words within a context window in the document. Despite DBOW being a simpler model it has been shown to perform better [18]. Our experiments confirm these results and consequently we use the DBOW version of doc2vec. ',\n",
       " 'The doc2vec DBOW architecture is very similar to the word2vec skip-gram model which uses the context word to predict surrounding words in the context window. The only difference is that DBOW swaps the context word for the document vector, which is used to predict the surrounding words in the context window. This similarity allows for the training of the two to be interleaved, thus simultaneously learning document and word vectors which are jointly embedded. ',\n",
       " 'The key insight into how doc2vec and word2vec learn these vectors is understanding how the prediction task works specifically for DBOW and skip-gram models. The word2vec skip-gram model learns an input word and context word vector for each word in the vocabulary. The word2vec model consists of a matrix Wn,d for input word vectors ',\n",
       " '4 and W ′ n,d for context word vectors, where n is the size of the corpus vocabulary, and d is the size of the vectors to be learned for each word. Each row of Wn,d contains a word vector ⃗w ∈ Rd and each row of W ′ n,d contains a context word vector ⃗wc ∈ Rd. For a given context window of size k, there will be k words to the left and k words to the right of the context word. For each of the 2k surrounding words w, their input word vector ⃗w ∈ Wn,d will be used to predict the context vector ⃗wc ∈ W ′ n,d of the context word wc. For each surrounding word w the prediction is softmax(⃗w · W ′ n,d). This generates a probability distribution over the vocabulary, for each word being the context word wc. The learning consists of using back propagation and stochastic gradient descent to update each context word vector in W ′ n,d, and ⃗w from Wn,d, such that the probability of the context vector given the surrounding word, P( ⃗wc|⃗w), is greatest in the probability distribution over the vocabulary. This process is repeated for every context window for all n words. This learning process necessitates semantically similar words to have context word vectors which are close together while making dissimilar words have context word vectors which are distant. This is because in order to maximize the probability P( ⃗wc|⃗w), the value of ⃗w · ⃗wc must be the maximum value in ⃗w · W ′ n,d. This value is maximized when ⃗w is closest to ⃗wc from word all context vectors in W ′ n,d. Therefore the learning process updates ⃗w and W ′ n,d so that ⃗w and ⃗wc are closer together. This can be interpreted as each context word pulling all similar context words towards it in the embedding space, while pushing away all dissimilar words. This results in a semantic space, represented by the context vectors W ′, where all semantically similar words are close together and all dissimilar words are far apart. ',\n",
       " 'The way the DBOW doc2vec model learns document vectors is similar to the word2vec skip-gram model. The model consists of a matrix Dc,d, where c is the number of documents in the corpus and d is the size of the vectors to be learned for each document. Each row of Dc,d contains a document vector ⃗d ∈ Rd. The model also requires a context word matrix W ′ n,d, which can be pre-trained, randomly initialized, or learned in parallel. For simplicity of the explanation, we will assume a scenario where matrix W ′ n,d has been pre-trained by a word2vec model on the same vocabulary of n words. For each document d in the corpus, the context vector ⃗wc ∈ W ′ n,d of each word in the ',\n",
       " \"document is used to predict the document's vector ⃗d ∈ Dc,d. The prediction is softmax( ⃗wc · Dc,d), which generates a probability distribution over the corpus for each document being the document the word is from. The learning consists of using back propagation and stochastic gradient descent to update each document vector in Dc,d and ⃗wc from W ′ n,d, such that the probability of the document given the word, P(⃗d|⃗w), is greatest in the probability distribution over the corpus of documents. This learning process necessitates that document vectors be close to word vectors of words that occur in them and making them distant from word vectors of words that do not occur in them. This can be interpreted as each word attracting documents that are similar to them while repelling documents which are dissimilar to them. This results in a semantic space where documents are closest to the words that best describe them and far from words that are dissimilar to them. Similar documents will be close together in this space as they will be pulled into the same region by similar words. Dissimilar documents will be far apart as they will be attracted into different regions of the semantic space by different words. \",\n",
       " 'We argue that the semantic space generated by word2vec and doc2vec is a continuous representation of topics. This claim can be justified by observing what the learned vector space generated by word2vec represents. This model learns a matrix W ′ n,d, which contains context word vectors of dimension d for all n words it is trained on. Each word vector in this matrix alone has no meaning; it only gives relative similarity to other word vectors in the matrix. We argue that this d dimensional embedding space is a continuous representation of topics defined by the matrix W ′ n,d. The matrix W ′ n,d can be seen as a linear transformation that when applied to a d dimensional vector from the embedding space generates an n dimensional vector. This vector itself is some measure of the strengths of each of the n words in the vocabulary corresponding to the point in the d dimensional space. However what this model has actually learned is how to transform points in the d dimensional space into probability distributions over the n words. Therefore any point ⃗p, in the d dimensional space can be transformed into a probability distribution over the n word vocabulary using softmax(⃗p · W ′ n,d). Thus, any point in the d dimensional space represents a different topic. Each word vector, ⃗wc ∈ W ′ n,d, corresponds to the topic in the d dimension space which has the greatest probability of word wc. In general any point ⃗p in the d dimensional space can be best described semantically by the nearest word vectors, since those are the words that would have the highest probability in its corresponding topic distribution over the n words in the vocabulary. ',\n",
       " 'There are several hyper-parameters that have a large impact on the performance of doc2vec [18]. The window size is the number of words left and right of the context word. A window size size of 15 has been found to produce the best results [18], which our experiments support. The doc2vec model can use negative sampling or hierarchical softmax as its output layer. These are both meant to be efficient approximations of the full softmax [9]. We found that in our experiments the hierarchical softmax produces better document vectors. According to [18], the most important hyper-parameter is the sub-sampling threshold, which determines the probability of high frequency words being discarded from a given context window. The suggested sub-sampling threshold value is 105. The smaller this ',\n",
       " '5 number is, the more likely it is for a high frequency word to be discarded from the context window [9, 18]. A related hyper-parameter is minimum count, which discards all words that have a total frequency that is less than that value from the model all together. This gets rid of extremely rare words which would not contribute to learning the document vectors. In our experiments we found a minimum count of 50 to work best, however this value largely depends on corpus size and its vocabulary. The vector size is the size of the document and word vectors that will be learned, the larger they are the more complex information they can encode. In general, the suggested vector size is 300 [18], with larger data sets larger values will lead to better results, at greater computational cost. The number of training epochs suggested by [18] is 20 to 400, with the higher values for smaller data sets. We found 40 to 400 training epochs to be a good range. ',\n",
       " \"2.2 Find Number of Topics The semantic embedding has the advantage of learning a continuous representation of topics. In the jointly embedded document and word vector space, with the properties outlined in 2.1, documents and words are represented as positions in the semantic space. In this space each document vector can be seen as representing the topic of the document [17]. The word vectors that are nearest to a document vector, are the most semantically descriptive of the document's topic. \",\n",
       " 'In the semantic space, a dense area of documents can be interpreted as an area of highly similar documents. This dense area of documents is indicative of an underlying topic that is common to the documents. Since the document vectors represent the topics of the documents, the centroid or average of those vectors can be calculated. This centroid is the topic vector which is most representative of the the dense area of documents it was calculated from. The words that are closest to this topic vector are the words that best describe it semantically. The main assumption behind top2vec is that the number of dense areas of document vectors equals the number of prominent topics. This is a natural way to discretize topics, since a topic is found for each group of documents sharing a prominent topic. ',\n",
       " 'In order find the dense areas of documents in the semantic space, density based clustering is used on the document vectors, specifically Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) [25, 26, 27]. However, the \"curse of dimensionality\" which results from the high-dimensional document vectors introduces two main problems. In the high-dimensional semantic embedding space, regularly of 300 dimensions, the document vectors are very sparse. The document vector sparsity makes it difficult to find dense clusters and doing so comes at a high computational cost [28]. In order to alleviate these two problems, we perform dimension reduction on the document vectors with the algorithm Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) [29, 30]. In the dimension-reduced space, HDBSCAN can then be used to find dense clusters of documents. ',\n",
       " '2.2.1 Low Dimensional Document Embedding ',\n",
       " 'Dimension reduction allows for dense clusters of documents to be found more efficiently and accurately in the reduced space. UMAP is a manifold learning technique for dimension reduction with strong theoretical foundations [29, 30]. T-distributed Stochastic Neighbor Embedding (t-SNE) [31] is another popular dimensional reduction technique. We found that t-SNE does not preserve global structure as well as UMAP and it does not scale well to large datasets. Hence, UMAP is chosen for dimension reduction in top2vec, as it preserves local and global structure, and is able to scale to very large datasets. Figure 2 shows UMAP-reduced document vectors; it can be seen that a lot of global and local structure is preserved in the embedding. ',\n",
       " 'UMAP has several hyper-parameters that determine how it performs dimension reduction. Perhaps the most important parameter is the number of nearest neighbours, which controls the balance between preserving global structure versus local structure in the low dimensional embedding. Larger values put more emphasis on global over local structure preservation. Since the goal is to find dense areas of documents which would be close to each other in the high dimensional space, local structure is more important in this application. We find that setting number of nearest neighbours to 15 gives the best results, as this value gives more emphasis on local structure. Another related parameter is the distance metric, which is used to measure the distance between points in the high dimensional space. The often used distance metric for the document vectors is cosine similarity [8, 9], because it measures similarity of documents irrespective of their size. Lastly the embedding dimension must be chosen; we find 5 dimensions to give the best results for the downstream task of density based clustering. ',\n",
       " '2.2.2 Find Dense Clusters of Documents ',\n",
       " 'The goal of density based clustering is to find areas of highly similar documents in the semantic space, which indicate an underlying topic. This is performed on the UMAP reduced document vectors. The challenge is that the document vectors will have varying density throughout the semantic space. Additionally there will be sparse areas where documents are highly dissimilar. This can be seen as noise, as there is no prominent underlying topic. In order to overcome these ',\n",
       " '6 Figure 2: 300 dimensional document vectors from the 20 news groups dataset that are embedded into 2 dimensions using UMAP. ',\n",
       " 'challenges, HDBSCAN is used to find the dense areas of document vectors, as it was designed to handle both noise and variable density clusters [26]. HDBSCAN assigns a label to each dense cluster of document vectors and assigns a noise label to all document vectors that are not in a dense cluster. The dense areas of identified document vectors will be used to calculate the topic vectors. Documents that are classified as noise can be seen as not being descriptive of a prominent topic. Figure 3 shows an example of dense areas of documents identified by HDBSCAN. ',\n",
       " 'Figure 3: UMAP-reduced document vectors from the 20 news groups dataset. Each colored area of points is a dense area of documents identified by HDBSCAN, red points are documents HDBSCAN has labeled as noise. ',\n",
       " 'The main hyper-parameter that needs be chosen for HDBSCAN is minimum cluster size; this parameter is at the core of how the algorithm finds clusters of varying density [26]. This parameter represents the smallest size that should be considered a cluster by the algorithm. We find that a minimum cluster size of 15 gives the best results in our experiments, as larger values have a higher chance of merging unrelated document clusters. ',\n",
       " '7 2.3 Calculate Topic Vectors 2.3.1 Calculate Centroids in Original Dimensional Space ',\n",
       " 'The dense clusters of documents and noise documents identified by HDBSCAN in the UMAP-reduced dimension, correspond to locations in the original semantic embedding space. The use of UMAP and HDBSCAN can be seen as a process which labels each document in the semantic embedding space with either a noise label or a label for the dense cluster to which it belongs. ',\n",
       " 'Given labels for each cluster of dense documents in the semantic embedding space, topic vectors can be calculated. There are a number of ways that the topic vector can be calculated from the document vectors. The simplest method is to calculate the centroid, i.e. the arithmetic mean of all the document vectors in the same dense cluster. There are other reasonable options such as the geometric mean or using probabilities from the confidence of clusters created by HDBSCAN. We experimented with these techniques and found that they resulted in very similar topic vectors, with almost identical nearest-neighbour word vectors. We speculate that this is mainly due to the sparsity of the high dimensional space. Therefore, we decided to use the simple method of calculating the centroid. Figure 4 shows a visual example of a topic vector being calculated from a dense area of documents. ',\n",
       " 'Figure 4: The topic vector is the centroid of the dense are of documents identified by HDBSCAN, which are the purple points. The outliers identified by HDBSCAN are not used to calculate the centroid. ',\n",
       " 'The centroid is calculated for each set of document vectors that belong do a dense cluster, generating a topic vector for each set. The number of dense areas found is the number of prominent topics identified in the corpus. ',\n",
       " '2.3.2 Find Topic Words In the semantic space, every point represents a topic that is best described semantically by its nearest word vectors. Therefore the word vectors that are closest to a topic vector are those that are most representative of it semantically. The distance of each word vector to the topic vector will indicate how semantically similar the word is to the topic. The words closest to the topic vector can be seen as the words that are most similar to all documents into the dense area, as the topic vector is the centroid of that area. These words can be used to summarize the common topic of the documents in the dense area. Figure 5 shows an example of a topic vector and the nearest words. ',\n",
       " 'Common words appear in most documents and, as such, they are often in a region of the semantic space that is equally distant from all documents. As a result the words closest to a topic vector will rarely be stop-words, which has been confirmed in our experiments. Therefore there is no need for stop-word removal. ',\n",
       " '8 Figure 5: The topic words are the nearest word vectors to the topic vector. ',\n",
       " '2.4 Topic Size and Hierarchical Topic Reduction ',\n",
       " 'The topic and document vectors allow for the size of topics to be calculated. The topic vectors can be used to partition the document vectors such that each document vector belongs to its nearest topic vector. This associates each document with exactly one topic, the one which is most semantically similar to the document. The size of each topic is measured as the number of documents that belong to it. ',\n",
       " 'An advantage of the topic vectors and the continuous representation of topics in the semantic space is that the number of topics found by top2vec can be hierarchically reduced to any number of topics less than the number initially found. This is done by iteratively merging the smallest topic into its most semantically similar topic until the desired number of topics are reached. This is done by taking a weighted arithmetic mean of the topic vector of the smallest topic and its nearest topic vector, each weighted by their topic size. After each merge, the topic sizes are recalculated for each topic. This hierarchical topic reduction has the advantage of finding the topics which are most representative of the corpus, as it biases topics with greater size. ',\n",
       " '3 Results 3.1 Topic Information Gain ',\n",
       " 'A natural way to evaluate topic models is to score how well the topics describe the documents. This evaluation measures how informative the topics are to a user. We propose using mutual information [32] to measure the information gained about the documents when described by their topic words. ',\n",
       " 'Traditional topic modeling methods discretize topic space and describe documents as a mixture of those topics. In order to evaluate a set of these topics T generated from documents D, the total information gained is calculated for each document when described with the proportions of topics given by the topic model. ',\n",
       " 'In contrast, top2vec learns a continuous representation of topics and places documents in that space corresponding to their topic. A topic vector found by top2vec represents the topic common to a group of documents, or the average of their individual topics. In order to evaluate a set of top2vec topics T generated from documents D, the documents are partitioned into sub-sets, with each sub-set corresponding to document vectors with the same nearest topic vector. Thus each document is assigned to exactly one topic. To evaluate these topics, the total information gained is measured for each of the sub-set of documents when described by the words nearest to their topic vector. ',\n",
       " '9 The total information gained, or mutual information, about all documents D when described by all words W, is given by: ',\n",
       " 'I(D, W) = � d∈D � w∈W P(d, w)log � P(d, w) ',\n",
       " 'P(d)P(w) � (1) The contribution of each co-occurrence between a document d and word w to the information gain calculation can be seen as the probability-weighted amount of information (PWI) d and w contribute to the total amount of information [33], given by: ',\n",
       " 'PWI(d, w) = P(d, w)log � P(d, w) ',\n",
       " 'P(d)P(w) � (2) Topics are distributions over the entire vocabulary W. However, in order to evaluate their usefulness to a user, we evaluate them using the top n words of the topic. For evaluation where each document is assigned to only one topic, each topic t ∈ T, will have a set of n words Wt ⊂ W and documents Dt ⊂ D. The information gained about all documents when described by their corresponding topic is given by: ',\n",
       " 'PWI(T) = � t∈T � d∈Dt � w∈Wt P(d, w)log � P(d, w) ',\n",
       " 'P(d)P(w) � = � t∈T � d∈Dt � w∈Wt P(d|w)P ′(w)log � P(d, w) ',\n",
       " 'P(d)P(w) � (3) In equation 3, P(w) is the marginal probability of the word w across all documents D. It is used to calculate the log term, which is the pointwise mutual information [34] between w and d. P ′(w) is the probability of topic word w, which is used to calculate the expected mutual information [32], or the information gained about document d given topic word w. The quantity we are measuring is the information gained about each document given its corresponding topic words as a prior. Therefore P ′(w) is 1 and can be omitted [33], which gives rise to: ',\n",
       " 'PWI(T) = � t∈T � d∈Dt � w∈Wt P(d|w)log � P(d, w) ',\n",
       " 'P(d)P(w) � (4) Alternatively the equation can be generalized for the case that each document is represented by multiple topics. In this case we replace P ′(w) with P(t), which is the proportion of words to be used to represent document d by topic t: ',\n",
       " 'PWI(T) = � d∈D � t∈T � w∈Wt P(d|w)P(t)log � P(d, w) ',\n",
       " 'P(d)P(w) � (5) Using Equations 4 and 5, different sets of topics can be compared. A greater quantity of information gain indicates that the topics t ∈ T are more informative of their corresponding documents. If topics contain words such as the, and, and it or other intuitively uninformative words, they will receive lower information gain values. This is in large part due to the P(d|w) term in the calculation, since the probability of any specific document given a very common word is very low. Therefore, the information gained is also low. Words that are mostly present in the subset of documents corresponding to the topic lead to higher information gain as they are informative of those documents. Additionally, low values of information gain will be obtained if the topic model assigns topics to the wrong documents. Topic information gain measures the quality of the words in the topic and their assignment to documents. Therefore, Equations 4 and 5 give values that correspond with what is intuitively more informative. We argue that due to its information theoretic derivation, topic information gain is a good measure for evaluating topic models. ',\n",
       " '3.2 LDA, PLSA and Top2Vec Topic Information Gain ',\n",
       " 'In order to evaluate LDA, PLSA and top2vec topics we train all models on the same documents D and vocabulary W. Since top2vec automatically finds the number of topics, we compare LDA, PLSA and top2vec on increasing numbers of topics up to the amount discovered by top2vec. ',\n",
       " '10 For each comparison between a set of LDA-generated topics, TLDA, PLSA-generated topics, TP LSA and top2vec- generated topics, Ttop2vec, we use the same number of top n topic words and the same number of topics. Thus, for each comparison between TLDA, TP LSA, and Ttop2vec, we ensure the following: ',\n",
       " '• |TLDA| = |TP LSA| = |Ttop2vec| = number of topics ',\n",
       " '• |Wt| = n, ∀Wt ∈ TLDA, TP LSA, Ttop2vec = top n topic words ',\n",
       " '3.2.1 20 News Groups Dataset The 20 News Groups dataset [35] contains 18,831 posts labelled with the news group they were posted in. We trained top2vec, LDA and PLSA models on this dataset using the same pre-processing steps. LDA and PLSA models were trained with 10 to 100 topics, with intervals of 10. Hierarchical topic reduction was used on the 103 topics discovered by top2vec. ',\n",
       " 'To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the topic information gain for models trained on the 20 news groups dataset can is shown in Figure 6. The results show that the top n topic words from top2vec consistently provide more information than PLSA and LDA, with varying topic sizes and with up to the top 1000 topic words. Even when stop-words are filtered from LDA and PLSA. For most topic sizes the top 20 words from top2vec convey as much information as the top 100 from LDA and PLSA. ',\n",
       " 'Tables 1 and 2 show the topics for top2vec and LDA models with topic size of 20. LDA was chosen over PLSA as it had higher topic information gain for 20 topics. Topics are ordered by increasing information gain. The topics shown for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by the model. Tables 1 and 2 demonstrate that the topic information gain score corresponds to what is intuitively more informative. ',\n",
       " 'In Table 2, LDA topics 2, 3 and 5 appear to contain nonsensical tokens, yet they have a high information gain. The 20 news groups data set contains messages that were sent encrypted or contain source code. When the 20 news groups messages are tokenized, these tokens are treated as words by the models. Thus, LDA has actually found informative tokens for that set of messages. However, that set contains only 23 messages. Therefore, LDA has found 3 different topics out of the requested 20, which only represent 23 messages out of the 18831 total amount the LDA model is trained on. This highlights an advantage of top2vec when finding the number of topics. ',\n",
       " \"Figure 7 shows the semantic embedding of the messages labeled by the news group each message was posted in. This figure shows that the semantic embedding has learned the similarity of messages by visually demonstrating the continuous representation of topics. Messages from similar newsgroups are in similar regions of the semantic space. The small red points on the very right of Figure 7, are the 23 messages which predominantly contain encrypted content or large quantities of source code. Due to the density of that set of messages, top2vec finds a topic for those messages. However, when hierarchical topic reduction is performed to reduce the topic size to 20, due to its small size, the topic of the encrypted and source code containing messages is merged into another topic that is most semantically similar to it. The semantic embedding of the messages labeled with the 20 top2vec topics from Table 1 that each belong to is shown in Figure 8. It demonstrates the assignment of the posts to the 20 topics correspond almost exactly to the 20 news groups and that each topic's top 3 words are very informative of the news group's actual topic. This visually demonstrates that top2vec finds topics which are more representative of the corpus as a whole, as confirmed by the topic information gain score in Figure 6. \",\n",
       " \"Topic information gain measures how informative topic words are of documents. Therefore, low scores are achieved when uninformative topic words are chosen, as well as when topics are assigned either to wrong documents or with incorrect proportions. There are a number of LDA topics in Table 2 that appear to be very coherent and that correspond to specific news groups. However, they have low scores in comparison to similar top2vec topics in Table 1. This is explained, in part, by LDA's modeling of documents as a mixture of topics. It models each document with non-zero probabilities of all topics. Therefore each of the messages will have some non-zero proportion of the topics 2, 3 and 5 that were generated from encrypted or source code containing messages. Figure 9 shows the contribution of each LDA topic from Table 2 to all messages. It demonstrates that the most informative topics are highly localized and that the uninformative topics are spread out over many messages. Topic 15 and 17, which both have low information gain, make up a large proportion of most messages. These are topics with very generic words that are found in most documents. \",\n",
       " \"The goal of LDA is to find topics such that their words recreate the original document word distributions with minimal error. This includes stop-words such as the, and, it and other generic words that would not be considered informative or topical by a user. This explains topic 15 and 17 which are just the generic words that occur in most documents. LDA's goal can also result in extremely specific topics, such as 2, 3, and 5, which necessitate other topics to be more general. Figure 9 visually demonstrates the reason that LDA topics produce lower information gain; it finds many unlocalized and therefore uninformative topics compared to top2vec. \",\n",
       " '11 Figure 6 shows that as the number of topics increases, the topic information gain for top2vec is consistently higher than for LDA and PLSA. This is because top2vec topics are more localized in the semantic space and therefore more informative. The number of topics found by top2vec on the 20 News Groups data set is 103, and are even more localized than the 20 topics in Table 1 which were generated from hierarchichal topic reduction. The original topics discovered in the region of topic 7 and 14 are shown in Figure 10 and Figure 11. These topics are even more localized than the reduced topics and therefore more informative as indicated by the information gain scores in Figure 6. ',\n",
       " '3.2.2 Yahoo Answers Dataset The Yahoo Answers dataset [36, 37], contains 1.3 million labelled posts. The posts are from 10 different topics, with 130,000 posts per topic. The number of topics top2vec found in this dataset are 2,618. Due to the computational cost of training LDA and PLSA models, we were only able to train the models from 10 to 100 topics with intervals of 10. Hierarchical topic reduction was used on the topics discovered by top2vec. ',\n",
       " 'To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the topic information gain for models trained on the Yahoo Answers dataset can be seen in Figure 12. These results are consistent with the results from the 20 News Groups dataset. They show that the top n topic words from top2vec consistently provide more information than PLSA and LDA, with varying topic sizes and up to the top 1000 topic words. Even when stop-words are filtered from LDA and PLSA. For most topic sizes, the top 20 words from top2vec convey as much information as the top 100 from LDA and PLSA. ',\n",
       " 'Tables 3 and 4 show the topics for top2vec and LDA models with a topic size of 10. The topics are ordered by increasing information gain. LDA was chosen over PLSA because it had higher topic information gain for 10 topics. The topics shown for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by the model. This comparison demonstrates the interpretability of the topics and their associated information gain score, showing that the more informative topics receive higher information gain. ',\n",
       " \"Figure 13 shows the semantic embedding of Yahoo Answers posts with their true topic labels. This figure demonstrates that the semantic space has captured the similarity of posts that share a similar topic. Figure 14 shows the posts labelled with the top2vec topics from Table 3. It demonstrates that the assignment of the posts to the 10 topics correspond almost exactly to the 10 topic labels from the Yahoo Answers dataset and that the topic's top 3 words are very informative of the true topic. This visually demonstrates that top2vec finds topics that are representative of the corpus as a whole, as confirmed by the topic information gain score in Figure 12. \",\n",
       " 'Figure 15 shows the strengths of each of the 10 LDA topics from Table 4 across all posts. This visually demonstrates that more informative topics are localized in the semantic space, and that LDA discovers topics that are less localized than top2vec topics. Additionally highly unlocalized LDA topics like 9 and 10, which contain the lowest information gain scores, also contain generic words that would not be considered topical or informative by a user. Figure 15 demonstrates visually that, apart from LDA topics containing less informative words, the reason LDA topics receive lower topic information gain is that they are less localized than top2vec topics and therefore less informative. ',\n",
       " '4 Discussion We have described top2vec, an unsupervised learning algorithm that finds topic vectors in a semantic space of jointly embedded document and word vectors. We have shown that the semantic space is a continuous representation of topics that allows for the calculation of topic vectors from dense areas of highly similar documents, topic size, and for hierarchical topic reduction. The top2vec model also allows for comparing similarity between words, documents and topics based on distance in the semantic space. ',\n",
       " 'We have proposed a novel method for evaluating topics that uses mutual information to calculate how informative topics are of documents. The topic information gain measures the amount of information gained about the documents when described by their topic words. This measures both the quality of topic words and the assignment of topics to the documents. Our results show that top2vec consistently finds topics that are more informative and representative of the corpus than LDA and PLSA, for varying sizes of topics and number of top topic words. ',\n",
       " 'There are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary advantages are that it automatically finds the number of topics and finds topics that are more informative and representa- tive of the corpus. As demonstrated, stop-word lists are not required to find informative topic words, making it easy to use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges of traditional methods that use BOW representations of words, which ignore word semantics. ',\n",
       " '12 Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to find topics that recreate the original documents word distributions with minimal loss. This necessitates these models to place uninformative words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no guarantee that they will find topics that are representative of the corpus. The results show they can find topics that are extremely specific or overly broad. ',\n",
       " 'In contrast, the words closest to top2vec topic vectors are the words that are most informative of the documents the topic vectors are calculated from. This is due to the learning task that generates joint document and word vectors, which predicts the document a word came from. This learning task necessitates document vectors to be placed close to the words that are most informative of the documents. The continuous representation of topics in the semantic space allows topic vectors to be calculated from dense areas of those documents. Thus top2vec topics are the words that are most informative of a document, rather than the set of words that recreate the documents distribution of words with accurate proportions. We suggest that top2vec is more appropriate for finding informative and representative topics of a corpus than probabilistic generative models like LDA and PLSA. ',\n",
       " 'The top2vec code is available as an open-source project1. ',\n",
       " '1https://github.com/ddangelov/Top2Vec ',\n",
       " '13 Table 1: Topic information gain for the top 10 words from top2vec topics trained on the 20 news groups dataset with 20 topics. ',\n",
       " 'Topic Number Topic Words PWI(T) ',\n",
       " '1 pitching, pitchers, pitcher, hitter, batting, hit, hitters, baseball, batters, inning 74.2 ',\n",
       " '2 bike, ride, riding, bikes, motorcycle, bikers, helmet, riders, countersteering, 71.9 passenger ',\n",
       " '3 circuit, voltage, circuits, resistor, signal, khz, impedance, analog, diode, resistors 69.1 ',\n",
       " '4 centris, ram, mhz, quadra, nubus, vram, iisi, lciii, cpu, fpu 62.4 ',\n",
       " '5 patient, symptoms, patients, doctor, disease, treatment, jxp, therapy, skepticism, 59.1 physician ',\n",
       " '6 koresh, fbi, compound, batf, davidians, atf, waco, raid, fire, bd 54.7 ',\n",
       " '7 israel, arab, arabs, israeli, jews, palestinians, israelis, war, peace, occupied 54.3 ',\n",
       " '8 orbit, space, launch, orbital, satellites, lunar, shuttle, spacecraft, moon, earth 53.7 ',\n",
       " '9 clipper, nsa, encryption, encrypted, secure, keys, crypto, algorithm, escrow, scheme 51.6 ',\n",
       " '10 controller, drives, drive, ide, scsi, floppy, bios, disk, jumpers, esdi 50.3 ',\n",
       " '11 windows, drivers, ati, cica, driver, exe, card, autoexec, mode, ini 50.1 ',\n",
       " '12 car, engine, cars, ford, brakes, honda, tires, valve, wheel, rear 49.7 ',\n",
       " '13 hockey, playoffs, nhl, game, season, team, playoff, teams, scoring, play 48.0 ',\n",
       " '14 gun, guns, firearms, laws, weapons, handgun, crime, amendment, handguns, firearm 46.6 ',\n",
       " '15 window, application, xlib, manager, openwindows, motif, server, xview, client, clients 43.6 ',\n",
       " '16 jesus, christ, god, bible, church, scripture, christians, scriptures, christian, heaven 38.9 ',\n",
       " '17 postscript, format, printer, fonts, files, formats, font, truetype, bitmap, image 36.7 ',\n",
       " '18 shipping, sale, offer, condition, asking, brand, sell, obo, price, selling 36.0 ',\n",
       " '19 atheists, belief, religion, beliefs, god, christianity, truth, religions, believe, atheist 34.4 ',\n",
       " '20 please, mail, post, email, posting, address, thanks, reply, interested, appreciate 11.3 ____ ',\n",
       " '996.6 14 Table 2: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the 20 news groups dataset with 20 topics. ',\n",
       " 'Topic Number Topic Words PWI(T) ',\n",
       " '1 la, pit, gm, det, bos, tor, pts, chi, vs, min 49.9 ',\n",
       " '2 hz, cx, ww, uw, qs, c_, pl, lk, ck, ah 47.0 ',\n",
       " '3 ax, max, pl, di, tm, ei, giz, wm, bhj, ey 42.6 ',\n",
       " '4 db, period, goal, play, pp, shots, st, power, mov, bh 27.9 ',\n",
       " '5 mk, mm, mp, mh, mu, mr, mj, mo, mq, mx 27.7 ',\n",
       " '6 health, medical, new, study, research, disease, cancer, use, patients, drug 19.0 ',\n",
       " '7 armenian, people, said, one, armenians, turkish, went, us, children, turkey 17.3 ',\n",
       " '8 dos, windows, drive, card, system, disk, mb, scsi, pc, mac 16.7 ',\n",
       " '9 file, program, window, files, image, jpeg, use, windows, display, color 15.7 ',\n",
       " '10 government, president, law, would, mr, israel, state, rights, fbi, states 13.4 ',\n",
       " '11 god, jesus, bible, church, christian, christ, christians, faith, lord, man 12.2 ',\n",
       " '12 game, team, games, hockey, season, teams, league, nhl, new, players 12.0 ',\n",
       " '13 space, nasa, earth, launch, shuttle, orbit, moon, satellite, solar, mission 11.8 ',\n",
       " '14 edu, ftp, graphics, available, pub, image, mail, com, version, also 9.7 ',\n",
       " '15 would, know, anyone, get, thanks, like, one, please, help, could 8.5 ',\n",
       " '16 key, use, data, system, one, information, may, encryption, used, number 7.5 ',\n",
       " '17 people, would, one, think, know, like, say, even, see, way 7.3 ',\n",
       " '18 one, car, would, like, get, time, much, also, back, power 7.1 ',\n",
       " '19 edu, com, please, list, mail, sale, send, email, price, offer 4.0 ',\n",
       " '20 think, year, would, good, time, last, well, get, one, got 3.6 ___ ',\n",
       " '360.9 15 Figure 6: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the 20 News Groups dataset. LDA* and PLSA* have stop-words removed. ',\n",
       " '16 Figure 7: Semantic embedding of 20 news groups messages labeled by news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " 'Figure 8: The 20 news groups messages labeled with the top2vec topics from Table 1. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " '17 Figure 9: Topic proportion of each LDA topic from Table 2 across all 20 news groups messages in the semantic embedding. The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " '18 Figure 10: Zoom in of top2vec original topics found in region of topic 7 from Table 1. This region of the semantic space corresponds to the talk.politics.mideast news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " 'Figure 11: Zoom in of top2vec original topics found in region of topic 14 from Table 1. This region of the semantic space corresponds to the talk.politics.guns news group. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " '19 Figure 12: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the Yahoo Answers dataset. LDA* and PLSA* have stop-words removed. ',\n",
       " '20 Table 3: Topic information gain for the top 10 words from top2vec topics trained on the Yahoo Answers dataset with 10 topics. ',\n",
       " 'Topic Number Topic Words PWI(T) ',\n",
       " '1 overwrite, rebooting, debug, debugging, reboot, executable, compiler, winxp, xp, winnt 112.2 ',\n",
       " '2 securities, unpaid, equity, purchaser, payment, broker, underwriting, issuer, payable, 104.4 underwriter ',\n",
       " '3 regimen, discomfort, inflammation, swelling, psoriasis, puffiness, inflammatory, 98.1 irritation, edema, hypertension ',\n",
       " '4 realtionship, realationship, insecurities, confide, hurtful, inlove, clingy, friendship, 92.5 bestfriend, friendships ',\n",
       " '5 song, sings, singer, sang, artist, duet, album, lyrics, ballad, vocalist 91.9 ',\n",
       " '6 scripture, believers, righteousness, righteous, pious, spiritual, spirituality, sinful, 82.2 worldly, discernment ',\n",
       " '7 team, players, game, teams, scoring, league, teammate, scorers, playoff, defensively 80.0 ',\n",
       " '8 courses, subjects, curriculum, students, teaching, faculty, syllabus, academic, 78.6 undergraduate, baccalaureate ',\n",
       " '9 war, leaders, politicians, government, democracy, political, terrorists, terrorism, 64.6 partisan, policies ',\n",
       " '10 thus, constant, hence, surface, resulting, greater, therefore, becomes, occurs, larger 33.1 ',\n",
       " '____ 837.6 Table 4: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the Yahoo Answers dataset with 10 topics. ',\n",
       " 'Topic Number Topic Words PWI(T) ',\n",
       " '1 team, game, world, win, cup, play, de, football, best, player 23.2 ',\n",
       " '2 computer, yahoo, use, get, click, internet, free, com, need, windows 22.3 ',\n",
       " '3 people, us, country, war, world, would, american, bush, government, america 18.0 ',\n",
       " '4 one, water, two, would, light, number, energy, used, earth, use 16.9 ',\n",
       " '5 body, weight, also, doctor, eat, blood, may, day, get, pain 15.7 ',\n",
       " '6 www, com, http, find, song, name, know, anyone, org, music 14.2 ',\n",
       " '7 get, money, school, would, need, work, pay, good, business, job 13.1 ',\n",
       " '8 god, people, one, life, believe, jesus, word, many, would, us 12.5 ',\n",
       " '9 like, know, get, think, would, want, people, good, really, go 9.1 ',\n",
       " '10 time, like, friend, said, guy, back, would, one, years, got 8.3 ',\n",
       " '____ 153.3 21 Figure 13: Semantic embedding of Yahoo Answers posts with true labels. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " 'Figure 14: Yahoo Answers posts labeled with the top2vec topics from Table 3. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " '22 Figure 15: Topic proportion of each LDA topic from Table 4 across all Yahoo Answers posts in the semantic embedding. The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded into 2 dimensions using UMAP. ',\n",
       " '23 References [1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March 2003. ',\n",
       " '[2] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50–57, 1999. ',\n",
       " '[3] Jordan L Boyd-Graber and David M Blei. Syntactic topic models. In Advances in neural information processing systems, pages 185–192, 2009. ',\n",
       " '[4] S. Syed and M. Spruit. Full-text or abstract? examining topic coherence scores using latent dirichlet allocation. In 2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 165–174, 2017. ',\n",
       " '[5] Kai Yang, Yi Cai, Zhenhong Chen, Ho-fung Leung, and Raymond Lau. Exploring topic discriminating power of words in latent dirichlet allocation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2238–2247, 2016. ',\n",
       " '[6] Geoffrey E Hinton et al. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12. Amherst, MA, 1986. ',\n",
       " '[7] Henry Widdowson. J.R. Firth, 1957, papers in linguistics 1934–51. International Journal of Applied Linguistics, 17:402 – 413, 10 2007. ',\n",
       " '[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013. ',\n",
       " '[9] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013. ',\n",
       " \"[10] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 238–247, Baltimore, Maryland, June 2014. Association for Computational Linguistics. \",\n",
       " '[11] Zhuang Bairong, Wang Wenbo, Li Zhiyu, Zheng Chonghui, and Takahiro Shinozaki. Comparative analysis of word embedding methods for dstc6 end-to-end conversation modeling track. ',\n",
       " '[12] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural information processing systems, pages 2177–2185, 2014. ',\n",
       " '[13] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014. ',\n",
       " '[14] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137–1155, 2003. ',\n",
       " '[15] Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa- tions. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746–751, 2013. ',\n",
       " '[16] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225, 2015. ',\n",
       " '[17] Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. ArXiv, abs/1405.4053, 2014. ',\n",
       " '[18] Jey Han Lau and Timothy Baldwin. An empirical evaluation of doc2vec with practical insights into document embedding generation. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 78–86, Berlin, Germany, August 2016. Association for Computational Linguistics. ',\n",
       " '[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. ',\n",
       " '[20] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. ',\n",
       " '[21] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. ',\n",
       " '24 [22] Thomas L Griffiths, Mark Steyvers, and Joshua B Tenenbaum. Topics in semantic representation. Psychological review, 114(2):211, 2007. [23] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa- tions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. ',\n",
       " '[24] Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010. ELRA. http://is.muni.cz/publication/884893/en. [25] Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering based on hierarchical density estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160–172. Springer, 2013. [26] Leland McInnes and John Healy. Accelerated hierarchical density based clustering. 2017 IEEE International Conference on Data Mining Workshops (ICDMW), Nov 2017. [27] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. The Journal of Open Source Software, 2(11):205, 2017. [28] RB Marimont and MB Shapiro. Nearest neighbour searches and the curse of dimensionality. IMA Journal of Applied Mathematics, 24(1):59–70, 1979. [29] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [30] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation and projection. The Journal of Open Source Software, 3(29):861, 2018. [31] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605, 2008. [32] M Cover Thomas and A Thomas Joy. Elements of information theory. New York: Wiley, 3:37–38, 1991. [33] Akiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Processing & Management, 39(1):45–65, 2003. [34] Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29, 1990. [35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. [36] Jamaal Hay Wenpeng Yin and Dan Roth. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In EMNLP, 2019. [37] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649–657, 2015. ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs=[]\n",
    "to_append=\"\"\n",
    "# to prevent short blocks like titles from taking up 1 paragraph, set a threshold below which they are merged with the following block\n",
    "len_threshold=30\n",
    "for block in blocks:\n",
    "    to_append+=fix_text(block[4])+\" \"\n",
    "    if len(to_append)>len_threshold:\n",
    "        paragraphs.append(to_append)\n",
    "        to_append=\"\"\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 Introduction The ability to organize, search and summarize a large volume of text is a ubiquitous problem in natural language processing (NLP). Topic modeling is often used when a large collection of text cannot be reasonably read and sorted through by a person. Given a corpus comprised of many texts, referred to as documents, a topic model will discover the latent semantic structure, or topics, present in the documents. Topics can then be used to find high level summaries of a large collection of documents, search for documents of interest, and group similar documents together. '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed and save\n",
    "Create numerical embeddings for each paragraph and save them to disk for future reference.\n",
    "Embeddings are 512-element numerical arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  0\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  1\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  2\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  3\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  4\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  5\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  6\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  7\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  8\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_8/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  9\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_9/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  10\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_10/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_10/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  11\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_11/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_11/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  12\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_12/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_12/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  13\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_13/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_13/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  14\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_14/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_14/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  15\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_15/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_15/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  16\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_16/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_16/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  17\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_17/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  18\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_18/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_18/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  19\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_19/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_19/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  20\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_20/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_20/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  21\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_21/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_21/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  22\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_22/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_22/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  23\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_23/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_23/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  24\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_24/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_24/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  25\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_25/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_25/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  26\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_26/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_26/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  27\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_27/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_27/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  28\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_28/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_28/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  29\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_29/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_29/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  30\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_30/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_30/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  31\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_31/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_31/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  32\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_32/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_32/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  33\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_33/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_33/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  34\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_34/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_34/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  35\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_35/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_35/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  36\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_36/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_36/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  37\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_37/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_37/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  38\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_38/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  39\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_39/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_39/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  40\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_40/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_40/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  41\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_41/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_41/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  42\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_42/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_42/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  43\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_43/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_43/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  44\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_44/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_44/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  45\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_45/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_45/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  46\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_46/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_46/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  47\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_47/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_47/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  48\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_48/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_48/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  49\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_49/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_49/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  50\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_50/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  51\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_51/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_51/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  52\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_52/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_52/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  53\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_53/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_53/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  54\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_54/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_54/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  55\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_55/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_55/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  56\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_56/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_56/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  57\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_57/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_57/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  58\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_58/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_58/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  59\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_59/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_59/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  60\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_60/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_60/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  61\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_61/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_61/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  62\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_62/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_62/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  63\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_63/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_63/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  64\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_64/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_64/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  65\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_65/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_65/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  66\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_66/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_66/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  67\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_67/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_67/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  68\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_68/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_68/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  69\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_69/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_69/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  70\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_70/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_70/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  71\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_71/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_71/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  72\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_72/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_72/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  73\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_73/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_73/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  74\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_74/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_74/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  75\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_75/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_75/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  76\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_76/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_76/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  77\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_77/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_77/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  78\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_78/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_78/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  79\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_79/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_79/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  80\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_80/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_80/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  81\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_81/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_81/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  82\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_82/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_82/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  83\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_83/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_83/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  84\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_84/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_84/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  85\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_85/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_85/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  86\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_86/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_86/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  87\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_87/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_87/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  88\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_88/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_88/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  89\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_89/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_89/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  90\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_90/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_90/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  91\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_91/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_91/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  92\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_92/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_92/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  93\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_93/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_93/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  94\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_94/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_94/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  95\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_95/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_95/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  96\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_96/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_96/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  97\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_97/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_97/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  98\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_98/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_98/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  99\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_99/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_99/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  100\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  101\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  102\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_102/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_102/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  103\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_103/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_103/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  104\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_104/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_104/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  105\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_105/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_105/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  106\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_106/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_106/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  107\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_107/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_107/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  108\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_108/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_108/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  109\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_109/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_109/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  110\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_110/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_110/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  111\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_111/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_111/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  112\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_112/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_112/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  113\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_113/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_113/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  114\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_114/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_114/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  115\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_115/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_115/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  116\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_116/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_116/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  117\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_117/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_117/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  118\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_118/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_118/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  119\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_119/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_119/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  120\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_120/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_120/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  121\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_121/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_121/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  122\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_122/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_122/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  123\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_123/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_123/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  124\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_124/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_124/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  125\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_125/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_125/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  126\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_126/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_126/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  127\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_127/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_127/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  128\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_128/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_128/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  129\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_129/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_129/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  130\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_130/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_130/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  131\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_131/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_131/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  132\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_132/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_132/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  133\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_133/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_133/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  134\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_134/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_134/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  135\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_135/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_135/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  136\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_136/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_136/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  137\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_137/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_137/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  138\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_138/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_138/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  139\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_139/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_139/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  140\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_140/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_140/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  141\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_141/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_141/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  142\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_142/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_142/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  143\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_143/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_143/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  144\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_144/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_144/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  145\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_145/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_145/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  146\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_146/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_146/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  147\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_147/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_147/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  148\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_148/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_148/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  149\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_149/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_149/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  150\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_150/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_150/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  151\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_151/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_151/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  152\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_152/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_152/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  153\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_153/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_153/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  154\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_154/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_154/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  155\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_155/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_155/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  156\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_156/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_156/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  157\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_157/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_157/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  158\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_158/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_158/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  159\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_159/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_159/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  160\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_160/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_160/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  161\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_161/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_161/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  162\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_162/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_162/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  163\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_163/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_163/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  164\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_164/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_164/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  165\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_165/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_165/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  166\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_166/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_166/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  167\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_167/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_167/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  168\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_168/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_168/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  169\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_169/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_169/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  170\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_170/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_170/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  171\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_171/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_171/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  172\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_172/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_172/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  173\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_173/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_173/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  174\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_174/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_174/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  175\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_175/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_175/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  176\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_176/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_176/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  177\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_177/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_177/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  178\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_178/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_178/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  179\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_179/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_179/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  180\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_180/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_180/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  181\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_181/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_181/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  182\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_182/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_182/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  183\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_183/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_183/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  184\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_184/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_184/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  185\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_185/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_185/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  186\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_186/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_186/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  187\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_187/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_187/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  188\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_188/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_188/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  189\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_189/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_189/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  190\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_190/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_190/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  191\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_191/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_191/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  192\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_192/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_192/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  193\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_193/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_193/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  194\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_194/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_194/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  195\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_195/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_195/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  196\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_196/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_196/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  197\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_197/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_197/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  198\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_198/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_198/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  199\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_199/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_199/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  200\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_200/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_200/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  201\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_201/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_201/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  202\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_202/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_202/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  203\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_203/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_203/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph n.:  204\n",
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_204/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./embeddings/top2vec.pdf/paragraph_204/assets\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(paragraphs)):\n",
    "    print(\"Paragraph n.: \",i)\n",
    "    directory = \"./embeddings/\"+fname+\"/paragraph_\" + str(i)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    m=embed([paragraphs[i]])\n",
    "    exported_m = tf.train.Checkpoint(v=tf.Variable(m))\n",
    "    exported_m.f = tf.function(\n",
    "        lambda  x: exported_m.v * x,\n",
    "        input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "    tf.saved_model.save(exported_m,directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./embeddings/top2vec.pdf/paragraph_0\n",
      "./embeddings/top2vec.pdf/paragraph_1\n",
      "./embeddings/top2vec.pdf/paragraph_2\n",
      "./embeddings/top2vec.pdf/paragraph_3\n",
      "./embeddings/top2vec.pdf/paragraph_4\n",
      "./embeddings/top2vec.pdf/paragraph_5\n",
      "./embeddings/top2vec.pdf/paragraph_6\n",
      "./embeddings/top2vec.pdf/paragraph_7\n",
      "./embeddings/top2vec.pdf/paragraph_8\n",
      "./embeddings/top2vec.pdf/paragraph_9\n",
      "./embeddings/top2vec.pdf/paragraph_10\n",
      "./embeddings/top2vec.pdf/paragraph_11\n",
      "./embeddings/top2vec.pdf/paragraph_12\n",
      "./embeddings/top2vec.pdf/paragraph_13\n",
      "./embeddings/top2vec.pdf/paragraph_14\n",
      "./embeddings/top2vec.pdf/paragraph_15\n",
      "./embeddings/top2vec.pdf/paragraph_16\n",
      "./embeddings/top2vec.pdf/paragraph_17\n",
      "./embeddings/top2vec.pdf/paragraph_18\n",
      "./embeddings/top2vec.pdf/paragraph_19\n",
      "./embeddings/top2vec.pdf/paragraph_20\n",
      "./embeddings/top2vec.pdf/paragraph_21\n",
      "./embeddings/top2vec.pdf/paragraph_22\n",
      "./embeddings/top2vec.pdf/paragraph_23\n",
      "./embeddings/top2vec.pdf/paragraph_24\n",
      "./embeddings/top2vec.pdf/paragraph_25\n",
      "./embeddings/top2vec.pdf/paragraph_26\n",
      "./embeddings/top2vec.pdf/paragraph_27\n",
      "./embeddings/top2vec.pdf/paragraph_28\n",
      "./embeddings/top2vec.pdf/paragraph_29\n",
      "./embeddings/top2vec.pdf/paragraph_30\n",
      "./embeddings/top2vec.pdf/paragraph_31\n",
      "./embeddings/top2vec.pdf/paragraph_32\n",
      "./embeddings/top2vec.pdf/paragraph_33\n",
      "./embeddings/top2vec.pdf/paragraph_34\n",
      "./embeddings/top2vec.pdf/paragraph_35\n",
      "./embeddings/top2vec.pdf/paragraph_36\n",
      "./embeddings/top2vec.pdf/paragraph_37\n",
      "./embeddings/top2vec.pdf/paragraph_38\n",
      "./embeddings/top2vec.pdf/paragraph_39\n",
      "./embeddings/top2vec.pdf/paragraph_40\n",
      "./embeddings/top2vec.pdf/paragraph_41\n",
      "./embeddings/top2vec.pdf/paragraph_42\n",
      "./embeddings/top2vec.pdf/paragraph_43\n",
      "./embeddings/top2vec.pdf/paragraph_44\n",
      "./embeddings/top2vec.pdf/paragraph_45\n",
      "./embeddings/top2vec.pdf/paragraph_46\n",
      "./embeddings/top2vec.pdf/paragraph_47\n",
      "./embeddings/top2vec.pdf/paragraph_48\n",
      "./embeddings/top2vec.pdf/paragraph_49\n",
      "./embeddings/top2vec.pdf/paragraph_50\n",
      "./embeddings/top2vec.pdf/paragraph_51\n",
      "./embeddings/top2vec.pdf/paragraph_52\n",
      "./embeddings/top2vec.pdf/paragraph_53\n",
      "./embeddings/top2vec.pdf/paragraph_54\n",
      "./embeddings/top2vec.pdf/paragraph_55\n",
      "./embeddings/top2vec.pdf/paragraph_56\n",
      "./embeddings/top2vec.pdf/paragraph_57\n",
      "./embeddings/top2vec.pdf/paragraph_58\n",
      "./embeddings/top2vec.pdf/paragraph_59\n",
      "./embeddings/top2vec.pdf/paragraph_60\n",
      "./embeddings/top2vec.pdf/paragraph_61\n",
      "./embeddings/top2vec.pdf/paragraph_62\n",
      "./embeddings/top2vec.pdf/paragraph_63\n",
      "./embeddings/top2vec.pdf/paragraph_64\n",
      "./embeddings/top2vec.pdf/paragraph_65\n",
      "./embeddings/top2vec.pdf/paragraph_66\n",
      "./embeddings/top2vec.pdf/paragraph_67\n",
      "./embeddings/top2vec.pdf/paragraph_68\n",
      "./embeddings/top2vec.pdf/paragraph_69\n",
      "./embeddings/top2vec.pdf/paragraph_70\n",
      "./embeddings/top2vec.pdf/paragraph_71\n",
      "./embeddings/top2vec.pdf/paragraph_72\n",
      "./embeddings/top2vec.pdf/paragraph_73\n",
      "./embeddings/top2vec.pdf/paragraph_74\n",
      "./embeddings/top2vec.pdf/paragraph_75\n",
      "./embeddings/top2vec.pdf/paragraph_76\n",
      "./embeddings/top2vec.pdf/paragraph_77\n",
      "./embeddings/top2vec.pdf/paragraph_78\n",
      "./embeddings/top2vec.pdf/paragraph_79\n",
      "./embeddings/top2vec.pdf/paragraph_80\n",
      "./embeddings/top2vec.pdf/paragraph_81\n",
      "./embeddings/top2vec.pdf/paragraph_82\n",
      "./embeddings/top2vec.pdf/paragraph_83\n",
      "./embeddings/top2vec.pdf/paragraph_84\n",
      "./embeddings/top2vec.pdf/paragraph_85\n",
      "./embeddings/top2vec.pdf/paragraph_86\n",
      "./embeddings/top2vec.pdf/paragraph_87\n",
      "./embeddings/top2vec.pdf/paragraph_88\n",
      "./embeddings/top2vec.pdf/paragraph_89\n",
      "./embeddings/top2vec.pdf/paragraph_90\n",
      "./embeddings/top2vec.pdf/paragraph_91\n",
      "./embeddings/top2vec.pdf/paragraph_92\n",
      "./embeddings/top2vec.pdf/paragraph_93\n",
      "./embeddings/top2vec.pdf/paragraph_94\n",
      "./embeddings/top2vec.pdf/paragraph_95\n",
      "./embeddings/top2vec.pdf/paragraph_96\n",
      "./embeddings/top2vec.pdf/paragraph_97\n",
      "./embeddings/top2vec.pdf/paragraph_98\n",
      "./embeddings/top2vec.pdf/paragraph_99\n",
      "./embeddings/top2vec.pdf/paragraph_100\n",
      "./embeddings/top2vec.pdf/paragraph_101\n",
      "./embeddings/top2vec.pdf/paragraph_102\n",
      "./embeddings/top2vec.pdf/paragraph_103\n",
      "./embeddings/top2vec.pdf/paragraph_104\n",
      "./embeddings/top2vec.pdf/paragraph_105\n",
      "./embeddings/top2vec.pdf/paragraph_106\n",
      "./embeddings/top2vec.pdf/paragraph_107\n",
      "./embeddings/top2vec.pdf/paragraph_108\n",
      "./embeddings/top2vec.pdf/paragraph_109\n",
      "./embeddings/top2vec.pdf/paragraph_110\n",
      "./embeddings/top2vec.pdf/paragraph_111\n",
      "./embeddings/top2vec.pdf/paragraph_112\n",
      "./embeddings/top2vec.pdf/paragraph_113\n",
      "./embeddings/top2vec.pdf/paragraph_114\n",
      "./embeddings/top2vec.pdf/paragraph_115\n",
      "./embeddings/top2vec.pdf/paragraph_116\n",
      "./embeddings/top2vec.pdf/paragraph_117\n",
      "./embeddings/top2vec.pdf/paragraph_118\n",
      "./embeddings/top2vec.pdf/paragraph_119\n",
      "./embeddings/top2vec.pdf/paragraph_120\n",
      "./embeddings/top2vec.pdf/paragraph_121\n",
      "./embeddings/top2vec.pdf/paragraph_122\n",
      "./embeddings/top2vec.pdf/paragraph_123\n",
      "./embeddings/top2vec.pdf/paragraph_124\n",
      "./embeddings/top2vec.pdf/paragraph_125\n",
      "./embeddings/top2vec.pdf/paragraph_126\n",
      "./embeddings/top2vec.pdf/paragraph_127\n",
      "./embeddings/top2vec.pdf/paragraph_128\n",
      "./embeddings/top2vec.pdf/paragraph_129\n",
      "./embeddings/top2vec.pdf/paragraph_130\n",
      "./embeddings/top2vec.pdf/paragraph_131\n",
      "./embeddings/top2vec.pdf/paragraph_132\n",
      "./embeddings/top2vec.pdf/paragraph_133\n",
      "./embeddings/top2vec.pdf/paragraph_134\n",
      "./embeddings/top2vec.pdf/paragraph_135\n",
      "./embeddings/top2vec.pdf/paragraph_136\n",
      "./embeddings/top2vec.pdf/paragraph_137\n",
      "./embeddings/top2vec.pdf/paragraph_138\n",
      "./embeddings/top2vec.pdf/paragraph_139\n",
      "./embeddings/top2vec.pdf/paragraph_140\n",
      "./embeddings/top2vec.pdf/paragraph_141\n",
      "./embeddings/top2vec.pdf/paragraph_142\n",
      "./embeddings/top2vec.pdf/paragraph_143\n",
      "./embeddings/top2vec.pdf/paragraph_144\n",
      "./embeddings/top2vec.pdf/paragraph_145\n",
      "./embeddings/top2vec.pdf/paragraph_146\n",
      "./embeddings/top2vec.pdf/paragraph_147\n",
      "./embeddings/top2vec.pdf/paragraph_148\n",
      "./embeddings/top2vec.pdf/paragraph_149\n",
      "./embeddings/top2vec.pdf/paragraph_150\n",
      "./embeddings/top2vec.pdf/paragraph_151\n",
      "./embeddings/top2vec.pdf/paragraph_152\n",
      "./embeddings/top2vec.pdf/paragraph_153\n",
      "./embeddings/top2vec.pdf/paragraph_154\n",
      "./embeddings/top2vec.pdf/paragraph_155\n",
      "./embeddings/top2vec.pdf/paragraph_156\n",
      "./embeddings/top2vec.pdf/paragraph_157\n",
      "./embeddings/top2vec.pdf/paragraph_158\n",
      "./embeddings/top2vec.pdf/paragraph_159\n",
      "./embeddings/top2vec.pdf/paragraph_160\n",
      "./embeddings/top2vec.pdf/paragraph_161\n",
      "./embeddings/top2vec.pdf/paragraph_162\n",
      "./embeddings/top2vec.pdf/paragraph_163\n",
      "./embeddings/top2vec.pdf/paragraph_164\n",
      "./embeddings/top2vec.pdf/paragraph_165\n",
      "./embeddings/top2vec.pdf/paragraph_166\n",
      "./embeddings/top2vec.pdf/paragraph_167\n",
      "./embeddings/top2vec.pdf/paragraph_168\n",
      "./embeddings/top2vec.pdf/paragraph_169\n",
      "./embeddings/top2vec.pdf/paragraph_170\n",
      "./embeddings/top2vec.pdf/paragraph_171\n",
      "./embeddings/top2vec.pdf/paragraph_172\n",
      "./embeddings/top2vec.pdf/paragraph_173\n",
      "./embeddings/top2vec.pdf/paragraph_174\n",
      "./embeddings/top2vec.pdf/paragraph_175\n",
      "./embeddings/top2vec.pdf/paragraph_176\n",
      "./embeddings/top2vec.pdf/paragraph_177\n",
      "./embeddings/top2vec.pdf/paragraph_178\n",
      "./embeddings/top2vec.pdf/paragraph_179\n",
      "./embeddings/top2vec.pdf/paragraph_180\n",
      "./embeddings/top2vec.pdf/paragraph_181\n",
      "./embeddings/top2vec.pdf/paragraph_182\n",
      "./embeddings/top2vec.pdf/paragraph_183\n",
      "./embeddings/top2vec.pdf/paragraph_184\n",
      "./embeddings/top2vec.pdf/paragraph_185\n",
      "./embeddings/top2vec.pdf/paragraph_186\n",
      "./embeddings/top2vec.pdf/paragraph_187\n",
      "./embeddings/top2vec.pdf/paragraph_188\n",
      "./embeddings/top2vec.pdf/paragraph_189\n",
      "./embeddings/top2vec.pdf/paragraph_190\n",
      "./embeddings/top2vec.pdf/paragraph_191\n",
      "./embeddings/top2vec.pdf/paragraph_192\n",
      "./embeddings/top2vec.pdf/paragraph_193\n",
      "./embeddings/top2vec.pdf/paragraph_194\n",
      "./embeddings/top2vec.pdf/paragraph_195\n",
      "./embeddings/top2vec.pdf/paragraph_196\n",
      "./embeddings/top2vec.pdf/paragraph_197\n",
      "./embeddings/top2vec.pdf/paragraph_198\n",
      "./embeddings/top2vec.pdf/paragraph_199\n",
      "./embeddings/top2vec.pdf/paragraph_200\n",
      "./embeddings/top2vec.pdf/paragraph_201\n",
      "./embeddings/top2vec.pdf/paragraph_202\n",
      "./embeddings/top2vec.pdf/paragraph_203\n",
      "./embeddings/top2vec.pdf/paragraph_204\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((1,512))\n",
    "for i in range(len(paragraphs)):\n",
    "    directory = \"./embeddings/\"+fname+\"/paragraph_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(directory)\n",
    "        imported_m = tf.saved_model.load(directory)\n",
    "        a= imported_m.v.numpy()\n",
    "        #print(a)\n",
    "        #exec(f'load{i} = a')\n",
    "        embeddings=np.concatenate([embeddings,a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are as many embeddings as there are paragraphs and each is a 512-element array. The first one is empty, so drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 512)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=embeddings[1:len(embeddings)]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings can be seen as a database of paragraphs. On a large scale system, you should consider storing them permanently in BigQuery or other mechanism that supports arrays as native types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00566422, -0.02935146,  0.06120373, ...,  0.04055705,\n",
       "        -0.04339721,  0.02022251],\n",
       "       [-0.0429877 ,  0.00689969,  0.03611137, ..., -0.00834908,\n",
       "        -0.07034817, -0.03555313],\n",
       "       [ 0.05260128,  0.03511225,  0.01449342, ...,  0.05155471,\n",
       "         0.00496395, -0.04210687],\n",
       "       ...,\n",
       "       [-0.03725477,  0.01147169, -0.01395804, ...,  0.06737421,\n",
       "        -0.00183064, -0.0466091 ],\n",
       "       [-0.04363855,  0.0078327 , -0.05713132, ...,  0.09173808,\n",
       "        -0.08570419, -0.06860624],\n",
       "       [-0.03068276,  0.04709442, -0.01252738, ...,  0.0087611 ,\n",
       "        -0.05826507, -0.04110563]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a search function\n",
    "Search will rank documents in order of similarity as defined by linear kernel $x^{T}y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchDocument(query):\n",
    "    q =[query]\n",
    "    \n",
    "    # Embed the query \n",
    "    Q_embedded =embed(q)\n",
    "    \n",
    "    # Calculate the similarity. Linear kernel is a generalization of xTx\n",
    "    linear_similarities = linear_kernel(Q_embedded, embeddings).flatten() \n",
    "    \n",
    "    #Sort top 10 indexes by similarity score\n",
    "    top_index_doc = linear_similarities.argsort()[:-11:-1]\n",
    "    linear_similarities.sort()\n",
    "    \n",
    "    a = pd.DataFrame()\n",
    "    for i,index in enumerate(top_index_doc):\n",
    "        a.loc[i,'index'] = str(index)\n",
    "        # Read paragraph with relevant index\n",
    "        a.loc[i,'Paragraph'] = paragraphs[index] \n",
    "    for j,simScore in enumerate(linear_similarities[:-11:-1]):\n",
    "        a.loc[j,'Score'] = simScore\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search \n",
    "Search for similar strings with different meanings and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Probabilistic Latent Semantic Analysis (PLSA) ...</td>\n",
       "      <td>0.501231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>The most widely used topic modeling method is ...</td>\n",
       "      <td>0.396057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>[4] S. Syed and M. Spruit. Full-text or abstra...</td>\n",
       "      <td>0.374829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>The authors of the LDA paper explicitly state:...</td>\n",
       "      <td>0.369781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186</td>\n",
       "      <td>[5] Kai Yang, Yi Cai, Zhenhong Chen, Ho-fung L...</td>\n",
       "      <td>0.351286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS</td>\n",
       "      <td>0.346380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>182</td>\n",
       "      <td>23 References [1] David M. Blei, Andrew Y. Ng,...</td>\n",
       "      <td>0.326645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>The greatest difference between top2vec and pr...</td>\n",
       "      <td>0.325732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>In order find the dense areas of documents in ...</td>\n",
       "      <td>0.310141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>Dimension reduction allows for dense clusters ...</td>\n",
       "      <td>0.289434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                          Paragraph     Score\n",
       "0    10  Probabilistic Latent Semantic Analysis (PLSA) ...  0.501231\n",
       "1     8  The most widely used topic modeling method is ...  0.396057\n",
       "2   185  [4] S. Syed and M. Spruit. Full-text or abstra...  0.374829\n",
       "3    14  The authors of the LDA paper explicitly state:...  0.369781\n",
       "4   186  [5] Kai Yang, Yi Cai, Zhenhong Chen, Ho-fung L...  0.351286\n",
       "5     0    TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS   0.346380\n",
       "6   182  23 References [1] David M. Blei, Andrew Y. Ng,...  0.326645\n",
       "7    27  The greatest difference between top2vec and pr...  0.325732\n",
       "8    42  In order find the dense areas of documents in ...  0.310141\n",
       "9    44  Dimension reduction allows for dense clusters ...  0.289434"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchDocument('dirichlet allocation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>The topic and document vectors allow for the s...</td>\n",
       "      <td>0.174717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>An advantage of the topic vectors and the cont...</td>\n",
       "      <td>0.159371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>In order to be able to extract topics, jointly...</td>\n",
       "      <td>0.157110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>4 Discussion We have described top2vec, an uns...</td>\n",
       "      <td>0.144884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>In the semantic space, a dense area of documen...</td>\n",
       "      <td>0.142814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47</td>\n",
       "      <td>The goal of density based clustering is to fin...</td>\n",
       "      <td>0.129333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>In contrast to traditional BOW topic modeling ...</td>\n",
       "      <td>0.129004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>A semantic space is a spatial representation i...</td>\n",
       "      <td>0.124824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40</td>\n",
       "      <td>2.2 Find Number of Topics The semantic embeddi...</td>\n",
       "      <td>0.122632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>Dimension reduction allows for dense clusters ...</td>\n",
       "      <td>0.114138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                                          Paragraph     Score\n",
       "0    61  The topic and document vectors allow for the s...  0.174717\n",
       "1    62  An advantage of the topic vectors and the cont...  0.159371\n",
       "2    29  In order to be able to extract topics, jointly...  0.157110\n",
       "3    97  4 Discussion We have described top2vec, an uns...  0.144884\n",
       "4    41  In the semantic space, a dense area of documen...  0.142814\n",
       "5    47  The goal of density based clustering is to fin...  0.129333\n",
       "6    25  In contrast to traditional BOW topic modeling ...  0.129004\n",
       "7    23  A semantic space is a spatial representation i...  0.124824\n",
       "8    40  2.2 Find Number of Topics The semantic embeddi...  0.122632\n",
       "9    44  Dimension reduction allows for dense clusters ...  0.114138"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchDocument('resources allocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search function has no reference and only 1 document to search on - it will return something from that document. However, it will have a lower similarity score to the query if the topic is less relevant than the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "Use a pre-trained summarizer with SciBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS\n",
      "Dimo Angelov\n",
      "dimo.angelov@gmail.com\n",
      "ABSTRACT\n",
      "Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a\n",
      "large collection of documents. The most widely used methods are Latent Dirichlet Allocation and\n",
      "Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In\n",
      "order to achieve optimal results they often require the number of topics to be known, custom stop-word\n",
      "lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation\n",
      "of documents which ignore the ordering and semantics of words. Distributed representations of\n",
      "documents and words have gained popularity due to their ability to capture semantics of words and\n",
      "documents. We present top2vec, which leverages joint document and word semantic embedding\n",
      "to find topic vectors. This model does not require stop-word lists, stemming or lemmatization, and\n",
      "it automatically finds the number of topics. The resulting topic vectors are jointly embedded with\n",
      "the document and word vectors with distance between them representing semantic similarity. Our\n",
      "experiments demonstrate that top2vec finds topics which are significantly more informative and\n",
      "representative of the corpus trained on than probabilistic generative models.\n",
      "1\n",
      "Introduction\n",
      "The ability to organize, search and summarize a large volume of text is a ubiquitous problem in natural language\n",
      "processing (NLP). Topic modeling is often used when a large collection of text cannot be reasonably read and sorted\n",
      "through by a person. Given a corpus comprised of many texts, referred to as documents, a topic model will discover the\n",
      "latent semantic structure, or topics, present in the documents. Topics can then be used to find high level summaries of a\n",
      "large collection of documents, search for documents of interest, and group similar documents together.\n",
      "A topic is the theme, matter or subject of a text; it is thing being discussed. Topics are often thought of as discrete\n",
      "values, such as politics, science, and religion. However, this is not to the case since any of these topics can be further\n",
      "subdivided into many other sub-topics. Additionally, a topic like politics can overlap with other topics, such as the topic\n",
      "of health, as they can both share the sub-topic of health care. Any of these topics, their combinations or variations\n",
      "can be described by some unique set of weighted words. As such, we assume that topics are continuous, as there are\n",
      "infinitely many combinations of weighted words which can be used to represent a topic. Additionally, we assume that\n",
      "each document has its own topic with a value in that continuum. In this view, the document's topic is the set of weighted\n",
      "words that are most informative of its unique topic, which can be a combination of the colloquial discrete topics.\n",
      "A useful topic model should find topics which represent a high-level summary of the information present in the\n",
      "documents. Each topic's set of words should represent information contained in the documents. For example, one\n",
      "can infer from a topic containing the words warming, global, temperature, and environment, that the topic is global\n",
      "warming. We define topic modeling to be the process of finding topics, as weighted sets of words, that best represent\n",
      "the information of the documents.\n",
      "In the remainder of this section we discuss related work and introduce distributed representations of topics. In Section 2\n",
      "we describe the top2vec model. Section 3 describes topic information gain and summarizes our experiments, and we\n",
      "conclude in Section 4.\n",
      "1.1\n",
      "Traditional Topic Modeling Methods\n",
      "The most widely used topic modeling method is Latent Dirichlet Allocation (LDA) [1]. It is a generative probabilistic\n",
      "model which describes each document as a mixture of topics and each topic as a distribution of words. LDA generalizes\n",
      "arXiv:2008.09470v1  [cs.CL]  19 Aug 2020\n",
      "Probabilistic Latent Semantic Analysis (PLSA) [2] by adding a Dirichlet prior distribution over document-topic and\n",
      "topic-word distributions.\n",
      "LDA and PLSA discretize the continuous topic space into t topics and model documents as mixtures of those t topics.\n",
      "These models assume the number of topics t to be known. The discretization of topics is necessary to model the\n",
      "relationship between documents and words. This is one of the greatest weakness of these models, as the number of\n",
      "topics t or the way to estimate it is rarely known, especially for very large or unfamiliar datasets [3, 4].\n",
      "Each topic produced by these methods is a distribution of word probabilities. As such, the highest probability words in\n",
      "a topic are usually words such as the, and, it and other common words in the language [4]. These common words, also\n",
      "called stop-words, often need to be filtered out in order to make topics interpretable, and extract the informative topic\n",
      "words. Finding the set of stop-words that must be removed is not a trivial problem since it is both language and corpus\n",
      "specific [5]; a topic model trained on text about dogs will likely treat dog as a stop-word since it is not very informative.\n",
      "LDA and PLSA use bag-of-words (BOW) representations of documents as input which ignore word semantics. In BOW\n",
      "representation the words Canada and Canadian would be treated as different words, despite their semantic similarity.\n",
      "Stemming and lemmatization techniques aim to address this problem but often make topics harder to understand.\n",
      "Moreover, stemming and lemmatization do not recognize the similarity of words like big and large, which do not share\n",
      "a word stem.\n",
      "The authors of the LDA paper explicitly state: \"We refer to the latent multinomial variables in the LDA model as topics,\n",
      "so as to exploit text-oriented intuitions, but we make no epistemological claims regarding these latent variables beyond\n",
      "their utility in representing probability distributions on sets of words.\" [1]. The objective of probabilistic generative\n",
      "models like LDA and PLSA is to find topics which can be used to recreate the original document word distributions\n",
      "with minimal error. However, a large proportion of all text contains uninformative words which may not be considered\n",
      "topical. These models do not differentiate between informative and uninformative words as their goal is to simply\n",
      "recreate the document word distributions. Therefore, the high probability words in topics they find do not necessarily\n",
      "correspond to what a user would intuitively think of as being topical.\n",
      "1.2\n",
      "Distributed Representations of Words and Documents\n",
      "In neural networks, a distributed representation means each concept learned by the network is represented by many\n",
      "neurons. Each neuron therefore participates in the representation of many concepts. When a neural network's weights\n",
      "are changed to incorporate new knowledge about a concept, the changes affect the knowledge associated with other\n",
      "concepts that are represented by similar patterns [6]. Distributed representation has the advantage of leading to automatic\n",
      "generalization of the concepts learned. Distributed representations are often central to NLP machine learning techniques\n",
      "for learning vector representations of words and documents.\n",
      "Another key idea behind learning vector representations of words and documents is the distributional hypothesis. The\n",
      "essence of the idea is captured by John Rupert Firth who famously said \"You shall know a word by the company it\n",
      "keeps\" [7]. This statement implies that words with similar meanings are used in similar contexts.\n",
      "The continuous skip-gram and BOW models [8, 9] known as word2vec, introduced distributed word representations\n",
      "that capture syntactic and semantic word relationships. The word2vec neural network learns word similarity by\n",
      "predicting which adjacent words should be present to a given context word in a sliding window over each document.\n",
      "The learning task of word2vec embraces the idea of distributional semantics, as it learns similar word vectors for\n",
      "words used in similar contexts. It also learns distributed representation of words, in the form of vectors, which facilitates\n",
      "generalization of word representation. The word2vec model generated word vectors produced state-of-the-art results\n",
      "on many linguistics tasks compared to traditional methods [8, 9, 10, 11].\n",
      "There has been interest in methods of finding distributed word vectors that do not rely on neural networks. It has been\n",
      "shown that the skip-gram version of word2vec is implicitly factorizing a word-context pointwise mutual information\n",
      "(PMI) matrix [12], based on this finding the authors proposed Shifted Positive PMI word-context representation of\n",
      "words. This has inspired other methods such as GloVe [13], which learn context and word vectors by factorizing a\n",
      "global word-word co-occurrence matrix. Although word2vec implicitly factorizes a word-context PMI matrix, what\n",
      "it explicitly does is maximize the dot product between word vectors for words which co-occur while minimizing dot\n",
      "product between words which do not co-occur. Additionally it uses a neural network which takes advantage of its\n",
      "learned distributed representation of words. This allows the model to learn about all words simultaneously from a\n",
      "single training step on a context word [14]. The ability of word2vec word vectors to capture syntactic and semantic\n",
      "regularities of language that other methods try to recreate is a result of the former points, as is its ability to scale to large\n",
      "corpora [8, 15]. As shown in [10], quantitative comparisons between neural and non-neural word vectors show that\n",
      "neural learned vectors consistently perform better. Results from [12, 16] show that at best non-neural methods achieve\n",
      "2\n",
      "results on certain tasks that are on-par with neural methods by replicating hyper-parameters of neural methods like\n",
      "word2vec. These methods, however, lack the ability to scale to large corpora.\n",
      "With the goal of overcoming the weaknesses of BOW representations of documents, the distributed paragraph vector\n",
      "was proposed with doc2vec [17]. This model extends word2vec by adding a paragraph vector to the learning task\n",
      "of the neural network. In addition to the context window of words, a paragraph vector is also used to predict which\n",
      "adjacent words should be present. The paragraph vector acts as a memory of the topic of the document; it informs each\n",
      "context window of what information is missing [17]. The doc2vec model can learn distributed representations of\n",
      "varying lengths of text, from sentences to documents. The doc2vec model outperforms BOW models and produces\n",
      "state-of-the-art results on many linguistics tasks compared to traditional methods [17, 18]. The doc2vec model was\n",
      "followed by many works on general language models [19, 20, 21].\n",
      "1.3\n",
      "Distributed Representations of Topics\n",
      "A semantic space is a spatial representation in which distance represents semantic association [22]. A lot of attention\n",
      "has been given to semantic embedding of words. Specifically, distributed word vectors generated by models like\n",
      "word2vec which have been shown to capture syntactic and semantic regularities of language [8, 23].\n",
      "The doc2vec model is capable of learning document and word vectors that are jointly embedded in the same space.\n",
      "It has been observed that doing so, or using pre-trained word vectors, improves the quality of the learned document\n",
      "vectors [18]. These jointly embedded document and word vectors are learned such that document vectors are close to\n",
      "word vectors which are semantically similar. This property can be used for information retrieval as word vectors can be\n",
      "used to query for similar documents. It can also be used to find which words are most similar to a document, or most\n",
      "representative of a document. As mentioned in [17], the paragraph or document vector acts as a memory of the topic\n",
      "of the document. Thus the most similar word vectors to a document vector are likely the most representative of the\n",
      "document's topic. This joint document and word embedding is a semantic embedding, since distance in the embedded\n",
      "space measures semantic similarity between the documents and words.\n",
      "In contrast to traditional BOW topic modeling methods, the semantic embedding has the advantage of learning\n",
      "the semantic association between words and documents. We argue that the semantic space itself is a continuous\n",
      "representation of topics, in which each point is a different topic best summarized by its nearest words. In the jointly\n",
      "embedded document and word semantic space, a dense area of documents can be interpreted as many documents that\n",
      "have a similar topic. We use this assumption to propose top2vec, a distributed topic vector which is calculated from\n",
      "dense areas of document vectors. The number of dense areas of documents found in the semantic space is assumed to\n",
      "be the number of prominent topics. The topic vectors are calculated as the centroids of each dense area of document\n",
      "vectors. A dense area is an area of very similar documents, and the centroid, or topic vector, can be thought of as the\n",
      "average document most representative of that area. We leverage the semantic embedding to find the words which are\n",
      "most representative of each topic vector by finding the closest word vectors to each topic vector.\n",
      "The top2vec model produces jointly embedded topic, document, and word vectors such that distance between them\n",
      "represents semantic similarity. Removing stop-words, lemmatization, stemming, and a priori knowledge of the number\n",
      "of topics are not required for top2vec to learn good topic vectors. This gives top2vec a major advantage over\n",
      "traditional methods. The topic vector can be used to find similar documents and words can be used to find similar\n",
      "topics. The same vector algebra demonstrated with word2vec [8, 9] can be used between the word, document and\n",
      "topic vectors. The topic vectors allow for topic sizes to be calculated based on each document vector's nearest topic\n",
      "vector. Additionally topic reduction can be performed on the topic vectors to hierarchically group similar topics and\n",
      "reduce the number of topics discovered.\n",
      "The greatest difference between top2vec and probabilistic generative models is how each models a topic. LDA and\n",
      "PLSA model topics as distributions of words, which are used to recreate the original document word distributions\n",
      "with minimal error. This often necessitates uninformative words which are not topical to have high probabilities in the\n",
      "topics since they make up a large proportion of all text. In contrast a top2vec topic vector in the semantic embedding\n",
      "represents a prominent topic shared among documents. The nearest words to a topic vector best describe the topic and\n",
      "its surrounding documents. This is due to the joint document and word embedding learning task, which is to predict\n",
      "which words are most indicative of a document, which necessitates documents, and therefore topic vectors, to be closest\n",
      "to their most informative words. Our results show that topics found by top2vec are significantly more informative\n",
      "and representative of the corpus trained on than those found by LDA and PLSA.\n",
      "3\n",
      "2\n",
      "Model Description\n",
      "2.1\n",
      "Create Semantic Embedding\n",
      "In order to be able to extract topics, jointly embedded document and word vectors with certain properties are required.\n",
      "Specifically, we need an embedding where the distance between document vectors and word vectors represents semantic\n",
      "association. Semantically similar documents should be placed close together in the embedding space, and dissimilar\n",
      "documents should be placed further from each other. Additionally words should be close to documents which they best\n",
      "describe. With jointly embedded document and word vectors with these properties, topic vectors can be calculated.\n",
      "This spatial representation of words and documents is called a semantic space [22]. We argue that a semantic space\n",
      "with the outlined properties is a continuous representation of topics. Figure 1 shows an example of a semantic space.\n",
      "Figure 1: An example of a semantic space. The purple points are documents and the green points are words. Words are\n",
      "closest to documents they best represent and similar documents are close together.\n",
      "To learn jointly embedded document and word vectors we use doc2vec [17, 24]. There are two versions of the model:\n",
      "the Paragraph Vector with Distributed Memory (DM) and Distributed Bag of Words (DBOW). The DM model uses\n",
      "context words and a document vector to predict the target word within context window. The DBOW model uses the\n",
      "document vector to predict words within a context window in the document. Despite DBOW being a simpler model it\n",
      "has been shown to perform better [18]. Our experiments confirm these results and consequently we use the DBOW\n",
      "version of doc2vec.\n",
      "The doc2vec DBOW architecture is very similar to the word2vec skip-gram model which uses the context word\n",
      "to predict surrounding words in the context window. The only difference is that DBOW swaps the context word for\n",
      "the document vector, which is used to predict the surrounding words in the context window. This similarity allows for\n",
      "the training of the two to be interleaved, thus simultaneously learning document and word vectors which are jointly\n",
      "embedded.\n",
      "The key insight into how doc2vec and word2vec learn these vectors is understanding how the prediction task works\n",
      "specifically for DBOW and skip-gram models. The word2vec skip-gram model learns an input word and context\n",
      "word vector for each word in the vocabulary. The word2vec model consists of a matrix Wn,d for input word vectors\n",
      "4\n",
      "and W ′\n",
      "n,d for context word vectors, where n is the size of the corpus vocabulary, and d is the size of the vectors to be\n",
      "learned for each word. Each row of Wn,d contains a word vector ⃗w ∈ Rd and each row of W ′\n",
      "n,d contains a context\n",
      "word vector ⃗wc ∈ Rd. For a given context window of size k, there will be k words to the left and k words to the right of\n",
      "the context word. For each of the 2k surrounding words w, their input word vector ⃗w ∈ Wn,d will be used to predict the\n",
      "context vector ⃗wc ∈ W ′\n",
      "n,d of the context word wc. For each surrounding word w the prediction is softmax(⃗w · W ′\n",
      "n,d).\n",
      "This generates a probability distribution over the vocabulary, for each word being the context word wc. The learning\n",
      "consists of using back propagation and stochastic gradient descent to update each context word vector in W ′\n",
      "n,d, and\n",
      "⃗w from Wn,d, such that the probability of the context vector given the surrounding word, P( ⃗wc|⃗w), is greatest in\n",
      "the probability distribution over the vocabulary. This process is repeated for every context window for all n words.\n",
      "This learning process necessitates semantically similar words to have context word vectors which are close together\n",
      "while making dissimilar words have context word vectors which are distant. This is because in order to maximize the\n",
      "probability P( ⃗wc|⃗w), the value of ⃗w · ⃗wc must be the maximum value in ⃗w · W ′\n",
      "n,d. This value is maximized when ⃗w is\n",
      "closest to ⃗wc from word all context vectors in W ′\n",
      "n,d. Therefore the learning process updates ⃗w and W ′\n",
      "n,d so that ⃗w and\n",
      "⃗wc are closer together. This can be interpreted as each context word pulling all similar context words towards it in the\n",
      "embedding space, while pushing away all dissimilar words. This results in a semantic space, represented by the context\n",
      "vectors W ′, where all semantically similar words are close together and all dissimilar words are far apart.\n",
      "The way the DBOW doc2vec model learns document vectors is similar to the word2vec skip-gram model. The\n",
      "model consists of a matrix Dc,d, where c is the number of documents in the corpus and d is the size of the vectors\n",
      "to be learned for each document. Each row of Dc,d contains a document vector ⃗d ∈ Rd. The model also requires a\n",
      "context word matrix W ′\n",
      "n,d, which can be pre-trained, randomly initialized, or learned in parallel. For simplicity of\n",
      "the explanation, we will assume a scenario where matrix W ′\n",
      "n,d has been pre-trained by a word2vec model on the\n",
      "same vocabulary of n words. For each document d in the corpus, the context vector ⃗wc ∈ W ′\n",
      "n,d of each word in the\n",
      "document is used to predict the document's vector ⃗d ∈ Dc,d. The prediction is softmax( ⃗wc · Dc,d), which generates a\n",
      "probability distribution over the corpus for each document being the document the word is from. The learning consists\n",
      "of using back propagation and stochastic gradient descent to update each document vector in Dc,d and ⃗wc from W ′\n",
      "n,d,\n",
      "such that the probability of the document given the word, P(⃗d|⃗w), is greatest in the probability distribution over the\n",
      "corpus of documents. This learning process necessitates that document vectors be close to word vectors of words that\n",
      "occur in them and making them distant from word vectors of words that do not occur in them. This can be interpreted\n",
      "as each word attracting documents that are similar to them while repelling documents which are dissimilar to them.\n",
      "This results in a semantic space where documents are closest to the words that best describe them and far from words\n",
      "that are dissimilar to them. Similar documents will be close together in this space as they will be pulled into the same\n",
      "region by similar words. Dissimilar documents will be far apart as they will be attracted into different regions of the\n",
      "semantic space by different words.\n",
      "We argue that the semantic space generated by word2vec and doc2vec is a continuous representation of topics.\n",
      "This claim can be justified by observing what the learned vector space generated by word2vec represents. This model\n",
      "learns a matrix W ′\n",
      "n,d, which contains context word vectors of dimension d for all n words it is trained on. Each word\n",
      "vector in this matrix alone has no meaning; it only gives relative similarity to other word vectors in the matrix. We\n",
      "argue that this d dimensional embedding space is a continuous representation of topics defined by the matrix W ′\n",
      "n,d. The\n",
      "matrix W ′\n",
      "n,d can be seen as a linear transformation that when applied to a d dimensional vector from the embedding\n",
      "space generates an n dimensional vector. This vector itself is some measure of the strengths of each of the n words in\n",
      "the vocabulary corresponding to the point in the d dimensional space. However what this model has actually learned\n",
      "is how to transform points in the d dimensional space into probability distributions over the n words. Therefore any\n",
      "point ⃗p, in the d dimensional space can be transformed into a probability distribution over the n word vocabulary\n",
      "using softmax(⃗p · W ′\n",
      "n,d). Thus, any point in the d dimensional space represents a different topic. Each word vector,\n",
      "⃗wc ∈ W ′\n",
      "n,d, corresponds to the topic in the d dimension space which has the greatest probability of word wc. In general\n",
      "any point ⃗p in the d dimensional space can be best described semantically by the nearest word vectors, since those\n",
      "are the words that would have the highest probability in its corresponding topic distribution over the n words in the\n",
      "vocabulary.\n",
      "There are several hyper-parameters that have a large impact on the performance of doc2vec [18]. The window size\n",
      "is the number of words left and right of the context word. A window size size of 15 has been found to produce the\n",
      "best results [18], which our experiments support. The doc2vec model can use negative sampling or hierarchical\n",
      "softmax as its output layer. These are both meant to be efficient approximations of the full softmax [9]. We found\n",
      "that in our experiments the hierarchical softmax produces better document vectors. According to [18], the most\n",
      "important hyper-parameter is the sub-sampling threshold, which determines the probability of high frequency words\n",
      "being discarded from a given context window. The suggested sub-sampling threshold value is 105. The smaller this\n",
      "5\n",
      "number is, the more likely it is for a high frequency word to be discarded from the context window [9, 18]. A related\n",
      "hyper-parameter is minimum count, which discards all words that have a total frequency that is less than that value\n",
      "from the model all together. This gets rid of extremely rare words which would not contribute to learning the document\n",
      "vectors. In our experiments we found a minimum count of 50 to work best, however this value largely depends on\n",
      "corpus size and its vocabulary. The vector size is the size of the document and word vectors that will be learned, the\n",
      "larger they are the more complex information they can encode. In general, the suggested vector size is 300 [18], with\n",
      "larger data sets larger values will lead to better results, at greater computational cost. The number of training epochs\n",
      "suggested by [18] is 20 to 400, with the higher values for smaller data sets. We found 40 to 400 training epochs to be a\n",
      "good range.\n",
      "2.2\n",
      "Find Number of Topics\n",
      "The semantic embedding has the advantage of learning a continuous representation of topics. In the jointly embedded\n",
      "document and word vector space, with the properties outlined in 2.1, documents and words are represented as positions\n",
      "in the semantic space. In this space each document vector can be seen as representing the topic of the document [17].\n",
      "The word vectors that are nearest to a document vector, are the most semantically descriptive of the document's topic.\n",
      "In the semantic space, a dense area of documents can be interpreted as an area of highly similar documents. This dense\n",
      "area of documents is indicative of an underlying topic that is common to the documents. Since the document vectors\n",
      "represent the topics of the documents, the centroid or average of those vectors can be calculated. This centroid is the\n",
      "topic vector which is most representative of the the dense area of documents it was calculated from. The words that are\n",
      "closest to this topic vector are the words that best describe it semantically. The main assumption behind top2vec is\n",
      "that the number of dense areas of document vectors equals the number of prominent topics. This is a natural way to\n",
      "discretize topics, since a topic is found for each group of documents sharing a prominent topic.\n",
      "In order find the dense areas of documents in the semantic space, density based clustering is used on the document\n",
      "vectors, specifically Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) [25, 26, 27].\n",
      "However, the \"curse of dimensionality\" which results from the high-dimensional document vectors introduces two main\n",
      "problems. In the high-dimensional semantic embedding space, regularly of 300 dimensions, the document vectors\n",
      "are very sparse. The document vector sparsity makes it difficult to find dense clusters and doing so comes at a high\n",
      "computational cost [28]. In order to alleviate these two problems, we perform dimension reduction on the document\n",
      "vectors with the algorithm Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) [29, 30].\n",
      "In the dimension-reduced space, HDBSCAN can then be used to find dense clusters of documents.\n",
      "2.2.1\n",
      "Low Dimensional Document Embedding\n",
      "Dimension reduction allows for dense clusters of documents to be found more efficiently and accurately in the reduced\n",
      "space. UMAP is a manifold learning technique for dimension reduction with strong theoretical foundations [29, 30].\n",
      "T-distributed Stochastic Neighbor Embedding (t-SNE) [31] is another popular dimensional reduction technique. We\n",
      "found that t-SNE does not preserve global structure as well as UMAP and it does not scale well to large datasets. Hence,\n",
      "UMAP is chosen for dimension reduction in top2vec, as it preserves local and global structure, and is able to scale\n",
      "to very large datasets. Figure 2 shows UMAP-reduced document vectors; it can be seen that a lot of global and local\n",
      "structure is preserved in the embedding.\n",
      "UMAP has several hyper-parameters that determine how it performs dimension reduction. Perhaps the most important\n",
      "parameter is the number of nearest neighbours, which controls the balance between preserving global structure versus\n",
      "local structure in the low dimensional embedding. Larger values put more emphasis on global over local structure\n",
      "preservation. Since the goal is to find dense areas of documents which would be close to each other in the high\n",
      "dimensional space, local structure is more important in this application. We find that setting number of nearest\n",
      "neighbours to 15 gives the best results, as this value gives more emphasis on local structure. Another related parameter\n",
      "is the distance metric, which is used to measure the distance between points in the high dimensional space. The often\n",
      "used distance metric for the document vectors is cosine similarity [8, 9], because it measures similarity of documents\n",
      "irrespective of their size. Lastly the embedding dimension must be chosen; we find 5 dimensions to give the best results\n",
      "for the downstream task of density based clustering.\n",
      "2.2.2\n",
      "Find Dense Clusters of Documents\n",
      "The goal of density based clustering is to find areas of highly similar documents in the semantic space, which indicate an\n",
      "underlying topic. This is performed on the UMAP reduced document vectors. The challenge is that the document vectors\n",
      "will have varying density throughout the semantic space. Additionally there will be sparse areas where documents are\n",
      "highly dissimilar. This can be seen as noise, as there is no prominent underlying topic. In order to overcome these\n",
      "6\n",
      "Figure 2: 300 dimensional document vectors from the 20 news groups dataset that are embedded into 2 dimensions\n",
      "using UMAP.\n",
      "challenges, HDBSCAN is used to find the dense areas of document vectors, as it was designed to handle both noise and\n",
      "variable density clusters [26]. HDBSCAN assigns a label to each dense cluster of document vectors and assigns a noise\n",
      "label to all document vectors that are not in a dense cluster. The dense areas of identified document vectors will be used\n",
      "to calculate the topic vectors. Documents that are classified as noise can be seen as not being descriptive of a prominent\n",
      "topic. Figure 3 shows an example of dense areas of documents identified by HDBSCAN.\n",
      "Figure 3: UMAP-reduced document vectors from the 20 news groups dataset. Each colored area of points is a dense\n",
      "area of documents identified by HDBSCAN, red points are documents HDBSCAN has labeled as noise.\n",
      "The main hyper-parameter that needs be chosen for HDBSCAN is minimum cluster size; this parameter is at the core of\n",
      "how the algorithm finds clusters of varying density [26]. This parameter represents the smallest size that should be\n",
      "considered a cluster by the algorithm. We find that a minimum cluster size of 15 gives the best results in our experiments,\n",
      "as larger values have a higher chance of merging unrelated document clusters.\n",
      "7\n",
      "2.3\n",
      "Calculate Topic Vectors\n",
      "2.3.1\n",
      "Calculate Centroids in Original Dimensional Space\n",
      "The dense clusters of documents and noise documents identified by HDBSCAN in the UMAP-reduced dimension,\n",
      "correspond to locations in the original semantic embedding space. The use of UMAP and HDBSCAN can be seen as a\n",
      "process which labels each document in the semantic embedding space with either a noise label or a label for the dense\n",
      "cluster to which it belongs.\n",
      "Given labels for each cluster of dense documents in the semantic embedding space, topic vectors can be calculated.\n",
      "There are a number of ways that the topic vector can be calculated from the document vectors. The simplest method\n",
      "is to calculate the centroid, i.e. the arithmetic mean of all the document vectors in the same dense cluster. There are\n",
      "other reasonable options such as the geometric mean or using probabilities from the confidence of clusters created\n",
      "by HDBSCAN. We experimented with these techniques and found that they resulted in very similar topic vectors,\n",
      "with almost identical nearest-neighbour word vectors. We speculate that this is mainly due to the sparsity of the high\n",
      "dimensional space. Therefore, we decided to use the simple method of calculating the centroid. Figure 4 shows a visual\n",
      "example of a topic vector being calculated from a dense area of documents.\n",
      "Figure 4: The topic vector is the centroid of the dense are of documents identified by HDBSCAN, which are the purple\n",
      "points. The outliers identified by HDBSCAN are not used to calculate the centroid.\n",
      "The centroid is calculated for each set of document vectors that belong do a dense cluster, generating a topic vector for\n",
      "each set. The number of dense areas found is the number of prominent topics identified in the corpus.\n",
      "2.3.2\n",
      "Find Topic Words\n",
      "In the semantic space, every point represents a topic that is best described semantically by its nearest word vectors.\n",
      "Therefore the word vectors that are closest to a topic vector are those that are most representative of it semantically. The\n",
      "distance of each word vector to the topic vector will indicate how semantically similar the word is to the topic. The\n",
      "words closest to the topic vector can be seen as the words that are most similar to all documents into the dense area, as\n",
      "the topic vector is the centroid of that area. These words can be used to summarize the common topic of the documents\n",
      "in the dense area. Figure 5 shows an example of a topic vector and the nearest words.\n",
      "Common words appear in most documents and, as such, they are often in a region of the semantic space that is equally\n",
      "distant from all documents. As a result the words closest to a topic vector will rarely be stop-words, which has been\n",
      "confirmed in our experiments. Therefore there is no need for stop-word removal.\n",
      "8\n",
      "Figure 5: The topic words are the nearest word vectors to the topic vector.\n",
      "2.4\n",
      "Topic Size and Hierarchical Topic Reduction\n",
      "The topic and document vectors allow for the size of topics to be calculated. The topic vectors can be used to partition\n",
      "the document vectors such that each document vector belongs to its nearest topic vector. This associates each document\n",
      "with exactly one topic, the one which is most semantically similar to the document. The size of each topic is measured\n",
      "as the number of documents that belong to it.\n",
      "An advantage of the topic vectors and the continuous representation of topics in the semantic space is that the number of\n",
      "topics found by top2vec can be hierarchically reduced to any number of topics less than the number initially found.\n",
      "This is done by iteratively merging the smallest topic into its most semantically similar topic until the desired number\n",
      "of topics are reached. This is done by taking a weighted arithmetic mean of the topic vector of the smallest topic and its\n",
      "nearest topic vector, each weighted by their topic size. After each merge, the topic sizes are recalculated for each topic.\n",
      "This hierarchical topic reduction has the advantage of finding the topics which are most representative of the corpus, as\n",
      "it biases topics with greater size.\n",
      "3\n",
      "Results\n",
      "3.1\n",
      "Topic Information Gain\n",
      "A natural way to evaluate topic models is to score how well the topics describe the documents. This evaluation measures\n",
      "how informative the topics are to a user. We propose using mutual information [32] to measure the information gained\n",
      "about the documents when described by their topic words.\n",
      "Traditional topic modeling methods discretize topic space and describe documents as a mixture of those topics. In order\n",
      "to evaluate a set of these topics T generated from documents D, the total information gained is calculated for each\n",
      "document when described with the proportions of topics given by the topic model.\n",
      "In contrast, top2vec learns a continuous representation of topics and places documents in that space corresponding to\n",
      "their topic. A topic vector found by top2vec represents the topic common to a group of documents, or the average of\n",
      "their individual topics. In order to evaluate a set of top2vec topics T generated from documents D, the documents\n",
      "are partitioned into sub-sets, with each sub-set corresponding to document vectors with the same nearest topic vector.\n",
      "Thus each document is assigned to exactly one topic. To evaluate these topics, the total information gained is measured\n",
      "for each of the sub-set of documents when described by the words nearest to their topic vector.\n",
      "9\n",
      "The total information gained, or mutual information, about all documents D when described by all words W, is given by:\n",
      "I(D, W) =\n",
      "�\n",
      "d∈D\n",
      "�\n",
      "w∈W\n",
      "P(d, w)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "(1)\n",
      "The contribution of each co-occurrence between a document d and word w to the information gain calculation can be\n",
      "seen as the probability-weighted amount of information (PWI) d and w contribute to the total amount of information\n",
      "[33], given by:\n",
      "PWI(d, w) = P(d, w)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "(2)\n",
      "Topics are distributions over the entire vocabulary W. However, in order to evaluate their usefulness to a user, we\n",
      "evaluate them using the top n words of the topic. For evaluation where each document is assigned to only one topic,\n",
      "each topic t ∈ T, will have a set of n words Wt ⊂ W and documents Dt ⊂ D. The information gained about all\n",
      "documents when described by their corresponding topic is given by:\n",
      "PWI(T) =\n",
      "�\n",
      "t∈T\n",
      "�\n",
      "d∈Dt\n",
      "�\n",
      "w∈Wt\n",
      "P(d, w)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "=\n",
      "�\n",
      "t∈T\n",
      "�\n",
      "d∈Dt\n",
      "�\n",
      "w∈Wt\n",
      "P(d|w)P ′(w)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "(3)\n",
      "In equation 3, P(w) is the marginal probability of the word w across all documents D. It is used to calculate the log\n",
      "term, which is the pointwise mutual information [34] between w and d. P ′(w) is the probability of topic word w, which\n",
      "is used to calculate the expected mutual information [32], or the information gained about document d given topic word\n",
      "w. The quantity we are measuring is the information gained about each document given its corresponding topic words\n",
      "as a prior. Therefore P ′(w) is 1 and can be omitted [33], which gives rise to:\n",
      "PWI(T) =\n",
      "�\n",
      "t∈T\n",
      "�\n",
      "d∈Dt\n",
      "�\n",
      "w∈Wt\n",
      "P(d|w)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "(4)\n",
      "Alternatively the equation can be generalized for the case that each document is represented by multiple topics. In this\n",
      "case we replace P ′(w) with P(t), which is the proportion of words to be used to represent document d by topic t:\n",
      "PWI(T) =\n",
      "�\n",
      "d∈D\n",
      "�\n",
      "t∈T\n",
      "�\n",
      "w∈Wt\n",
      "P(d|w)P(t)log\n",
      "� P(d, w)\n",
      "P(d)P(w)\n",
      "�\n",
      "(5)\n",
      "Using Equations 4 and 5, different sets of topics can be compared. A greater quantity of information gain indicates\n",
      "that the topics t ∈ T are more informative of their corresponding documents. If topics contain words such as the, and,\n",
      "and it or other intuitively uninformative words, they will receive lower information gain values. This is in large part\n",
      "due to the P(d|w) term in the calculation, since the probability of any specific document given a very common word\n",
      "is very low. Therefore, the information gained is also low. Words that are mostly present in the subset of documents\n",
      "corresponding to the topic lead to higher information gain as they are informative of those documents. Additionally, low\n",
      "values of information gain will be obtained if the topic model assigns topics to the wrong documents. Topic information\n",
      "gain measures the quality of the words in the topic and their assignment to documents. Therefore, Equations 4 and 5\n",
      "give values that correspond with what is intuitively more informative. We argue that due to its information theoretic\n",
      "derivation, topic information gain is a good measure for evaluating topic models.\n",
      "3.2\n",
      "LDA, PLSA and Top2Vec Topic Information Gain\n",
      "In order to evaluate LDA, PLSA and top2vec topics we train all models on the same documents D and vocabulary\n",
      "W. Since top2vec automatically finds the number of topics, we compare LDA, PLSA and top2vec on increasing\n",
      "numbers of topics up to the amount discovered by top2vec.\n",
      "10\n",
      "For each comparison between a set of LDA-generated topics, TLDA, PLSA-generated topics, TP LSA and top2vec-\n",
      "generated topics, Ttop2vec, we use the same number of top n topic words and the same number of topics. Thus, for each\n",
      "comparison between TLDA, TP LSA, and Ttop2vec, we ensure the following:\n",
      "• |TLDA| = |TP LSA| = |Ttop2vec| = number of topics\n",
      "• |Wt| = n, ∀Wt ∈ TLDA, TP LSA, Ttop2vec = top n topic words\n",
      "3.2.1\n",
      "20 News Groups Dataset\n",
      "The 20 News Groups dataset [35] contains 18,831 posts labelled with the news group they were posted in. We trained\n",
      "top2vec, LDA and PLSA models on this dataset using the same pre-processing steps. LDA and PLSA models were\n",
      "trained with 10 to 100 topics, with intervals of 10. Hierarchical topic reduction was used on the 103 topics discovered\n",
      "by top2vec.\n",
      "To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the\n",
      "topic information gain for models trained on the 20 news groups dataset can is shown in Figure 6. The results show that\n",
      "the top n topic words from top2vec consistently provide more information than PLSA and LDA, with varying topic\n",
      "sizes and with up to the top 1000 topic words. Even when stop-words are filtered from LDA and PLSA. For most topic\n",
      "sizes the top 20 words from top2vec convey as much information as the top 100 from LDA and PLSA.\n",
      "Tables 1 and 2 show the topics for top2vec and LDA models with topic size of 20. LDA was chosen over PLSA as it\n",
      "had higher topic information gain for 20 topics. Topics are ordered by increasing information gain. The topics shown\n",
      "for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by the model. Tables\n",
      "1 and 2 demonstrate that the topic information gain score corresponds to what is intuitively more informative.\n",
      "In Table 2, LDA topics 2, 3 and 5 appear to contain nonsensical tokens, yet they have a high information gain. The 20\n",
      "news groups data set contains messages that were sent encrypted or contain source code. When the 20 news groups\n",
      "messages are tokenized, these tokens are treated as words by the models. Thus, LDA has actually found informative\n",
      "tokens for that set of messages. However, that set contains only 23 messages. Therefore, LDA has found 3 different\n",
      "topics out of the requested 20, which only represent 23 messages out of the 18831 total amount the LDA model is\n",
      "trained on. This highlights an advantage of top2vec when finding the number of topics.\n",
      "Figure 7 shows the semantic embedding of the messages labeled by the news group each message was posted in.\n",
      "This figure shows that the semantic embedding has learned the similarity of messages by visually demonstrating the\n",
      "continuous representation of topics. Messages from similar newsgroups are in similar regions of the semantic space.\n",
      "The small red points on the very right of Figure 7, are the 23 messages which predominantly contain encrypted content\n",
      "or large quantities of source code. Due to the density of that set of messages, top2vec finds a topic for those messages.\n",
      "However, when hierarchical topic reduction is performed to reduce the topic size to 20, due to its small size, the topic\n",
      "of the encrypted and source code containing messages is merged into another topic that is most semantically similar\n",
      "to it. The semantic embedding of the messages labeled with the 20 top2vec topics from Table 1 that each belong\n",
      "to is shown in Figure 8. It demonstrates the assignment of the posts to the 20 topics correspond almost exactly to the\n",
      "20 news groups and that each topic's top 3 words are very informative of the news group's actual topic. This visually\n",
      "demonstrates that top2vec finds topics which are more representative of the corpus as a whole, as confirmed by the\n",
      "topic information gain score in Figure 6.\n",
      "Topic information gain measures how informative topic words are of documents. Therefore, low scores are achieved\n",
      "when uninformative topic words are chosen, as well as when topics are assigned either to wrong documents or with\n",
      "incorrect proportions. There are a number of LDA topics in Table 2 that appear to be very coherent and that correspond\n",
      "to specific news groups. However, they have low scores in comparison to similar top2vec topics in Table 1. This is\n",
      "explained, in part, by LDA's modeling of documents as a mixture of topics. It models each document with non-zero\n",
      "probabilities of all topics. Therefore each of the messages will have some non-zero proportion of the topics 2, 3 and 5\n",
      "that were generated from encrypted or source code containing messages. Figure 9 shows the contribution of each LDA\n",
      "topic from Table 2 to all messages. It demonstrates that the most informative topics are highly localized and that the\n",
      "uninformative topics are spread out over many messages. Topic 15 and 17, which both have low information gain, make\n",
      "up a large proportion of most messages. These are topics with very generic words that are found in most documents.\n",
      "The goal of LDA is to find topics such that their words recreate the original document word distributions with minimal\n",
      "error. This includes stop-words such as the, and, it and other generic words that would not be considered informative or\n",
      "topical by a user. This explains topic 15 and 17 which are just the generic words that occur in most documents. LDA's\n",
      "goal can also result in extremely specific topics, such as 2, 3, and 5, which necessitate other topics to be more general.\n",
      "Figure 9 visually demonstrates the reason that LDA topics produce lower information gain; it finds many unlocalized\n",
      "and therefore uninformative topics compared to top2vec.\n",
      "11\n",
      "Figure 6 shows that as the number of topics increases, the topic information gain for top2vec is consistently higher\n",
      "than for LDA and PLSA. This is because top2vec topics are more localized in the semantic space and therefore more\n",
      "informative. The number of topics found by top2vec on the 20 News Groups data set is 103, and are even more\n",
      "localized than the 20 topics in Table 1 which were generated from hierarchichal topic reduction. The original topics\n",
      "discovered in the region of topic 7 and 14 are shown in Figure 10 and Figure 11. These topics are even more localized\n",
      "than the reduced topics and therefore more informative as indicated by the information gain scores in Figure 6.\n",
      "3.2.2\n",
      "Yahoo Answers Dataset\n",
      "The Yahoo Answers dataset [36, 37], contains 1.3 million labelled posts. The posts are from 10 different topics, with\n",
      "130,000 posts per topic. The number of topics top2vec found in this dataset are 2,618. Due to the computational cost\n",
      "of training LDA and PLSA models, we were only able to train the models from 10 to 100 topics with intervals of 10.\n",
      "Hierarchical topic reduction was used on the topics discovered by top2vec.\n",
      "To calculate PWI(TLDA), PWI(TP LSA), and PWI(Ttop2vec), we use the same W and D. A comparison of the\n",
      "topic information gain for models trained on the Yahoo Answers dataset can be seen in Figure 12. These results are\n",
      "consistent with the results from the 20 News Groups dataset. They show that the top n topic words from top2vec\n",
      "consistently provide more information than PLSA and LDA, with varying topic sizes and up to the top 1000 topic\n",
      "words. Even when stop-words are filtered from LDA and PLSA. For most topic sizes, the top 20 words from top2vec\n",
      "convey as much information as the top 100 from LDA and PLSA.\n",
      "Tables 3 and 4 show the topics for top2vec and LDA models with a topic size of 10. The topics are ordered by\n",
      "increasing information gain. LDA was chosen over PLSA because it had higher topic information gain for 10 topics.\n",
      "The topics shown for LDA have stop-words removed, where as the top2vec topics are the exact words discovered by\n",
      "the model. This comparison demonstrates the interpretability of the topics and their associated information gain score,\n",
      "showing that the more informative topics receive higher information gain.\n",
      "Figure 13 shows the semantic embedding of Yahoo Answers posts with their true topic labels. This figure demonstrates\n",
      "that the semantic space has captured the similarity of posts that share a similar topic. Figure 14 shows the posts labelled\n",
      "with the top2vec topics from Table 3. It demonstrates that the assignment of the posts to the 10 topics correspond\n",
      "almost exactly to the 10 topic labels from the Yahoo Answers dataset and that the topic's top 3 words are very informative\n",
      "of the true topic. This visually demonstrates that top2vec finds topics that are representative of the corpus as a whole,\n",
      "as confirmed by the topic information gain score in Figure 12.\n",
      "Figure 15 shows the strengths of each of the 10 LDA topics from Table 4 across all posts. This visually demonstrates\n",
      "that more informative topics are localized in the semantic space, and that LDA discovers topics that are less localized\n",
      "than top2vec topics. Additionally highly unlocalized LDA topics like 9 and 10, which contain the lowest information\n",
      "gain scores, also contain generic words that would not be considered topical or informative by a user. Figure 15\n",
      "demonstrates visually that, apart from LDA topics containing less informative words, the reason LDA topics receive\n",
      "lower topic information gain is that they are less localized than top2vec topics and therefore less informative.\n",
      "4\n",
      "Discussion\n",
      "We have described top2vec, an unsupervised learning algorithm that finds topic vectors in a semantic space of jointly\n",
      "embedded document and word vectors. We have shown that the semantic space is a continuous representation of\n",
      "topics that allows for the calculation of topic vectors from dense areas of highly similar documents, topic size, and for\n",
      "hierarchical topic reduction. The top2vec model also allows for comparing similarity between words, documents and\n",
      "topics based on distance in the semantic space.\n",
      "We have proposed a novel method for evaluating topics that uses mutual information to calculate how informative\n",
      "topics are of documents. The topic information gain measures the amount of information gained about the documents\n",
      "when described by their topic words. This measures both the quality of topic words and the assignment of topics to the\n",
      "documents. Our results show that top2vec consistently finds topics that are more informative and representative of\n",
      "the corpus than LDA and PLSA, for varying sizes of topics and number of top topic words.\n",
      "There are several advantages of top2vec over traditional topic modeling methods like LDA and PLSA. The primary\n",
      "advantages are that it automatically finds the number of topics and finds topics that are more informative and representa-\n",
      "tive of the corpus. As demonstrated, stop-word lists are not required to find informative topic words, making it easy to\n",
      "use on a corpus of any domain or language. The use of distributed representations of words alleviates several challenges\n",
      "of traditional methods that use BOW representations of words, which ignore word semantics.\n",
      "12\n",
      "Traditional topic modeling techniques like LDA and PLSA are generative models; they seek to find topics that recreate\n",
      "the original documents word distributions with minimal loss. This necessitates these models to place uninformative\n",
      "words in topics with high probability, as they make up a large proportion of all documents. Additionally, there is no\n",
      "guarantee that they will find topics that are representative of the corpus. The results show they can find topics that are\n",
      "extremely specific or overly broad.\n",
      "In contrast, the words closest to top2vec topic vectors are the words that are most informative of the documents\n",
      "the topic vectors are calculated from. This is due to the learning task that generates joint document and word vectors,\n",
      "which predicts the document a word came from. This learning task necessitates document vectors to be placed close to\n",
      "the words that are most informative of the documents. The continuous representation of topics in the semantic space\n",
      "allows topic vectors to be calculated from dense areas of those documents. Thus top2vec topics are the words that\n",
      "are most informative of a document, rather than the set of words that recreate the documents distribution of words with\n",
      "accurate proportions. We suggest that top2vec is more appropriate for finding informative and representative topics\n",
      "of a corpus than probabilistic generative models like LDA and PLSA.\n",
      "The top2vec code is available as an open-source project1.\n",
      "1https://github.com/ddangelov/Top2Vec\n",
      "13\n",
      "Table 1: Topic information gain for the top 10 words from top2vec topics trained on the 20 news groups dataset with\n",
      "20 topics.\n",
      "Topic Number\n",
      "Topic Words\n",
      "PWI(T)\n",
      "1\n",
      "pitching, pitchers, pitcher, hitter, batting, hit, hitters, baseball, batters, inning\n",
      "74.2\n",
      "2\n",
      "bike, ride, riding, bikes, motorcycle, bikers, helmet, riders, countersteering,\n",
      "71.9\n",
      "passenger\n",
      "3\n",
      "circuit, voltage, circuits, resistor, signal, khz, impedance, analog, diode, resistors\n",
      "69.1\n",
      "4\n",
      "centris, ram, mhz, quadra, nubus, vram, iisi, lciii, cpu, fpu\n",
      "62.4\n",
      "5\n",
      "patient, symptoms, patients, doctor, disease, treatment, jxp, therapy, skepticism,\n",
      "59.1\n",
      "physician\n",
      "6\n",
      "koresh, fbi, compound, batf, davidians, atf, waco, raid, fire, bd\n",
      "54.7\n",
      "7\n",
      "israel, arab, arabs, israeli, jews, palestinians, israelis, war, peace, occupied\n",
      "54.3\n",
      "8\n",
      "orbit, space, launch, orbital, satellites, lunar, shuttle, spacecraft, moon, earth\n",
      "53.7\n",
      "9\n",
      "clipper, nsa, encryption, encrypted, secure, keys, crypto, algorithm, escrow, scheme\n",
      "51.6\n",
      "10\n",
      "controller, drives, drive, ide, scsi, floppy, bios, disk, jumpers, esdi\n",
      "50.3\n",
      "11\n",
      "windows, drivers, ati, cica, driver, exe, card, autoexec, mode, ini\n",
      "50.1\n",
      "12\n",
      "car, engine, cars, ford, brakes, honda, tires, valve, wheel, rear\n",
      "49.7\n",
      "13\n",
      "hockey, playoffs, nhl, game, season, team, playoff, teams, scoring, play\n",
      "48.0\n",
      "14\n",
      "gun, guns, firearms, laws, weapons, handgun, crime, amendment, handguns, firearm\n",
      "46.6\n",
      "15\n",
      "window, application, xlib, manager, openwindows, motif, server, xview, client, clients\n",
      "43.6\n",
      "16\n",
      "jesus, christ, god, bible, church, scripture, christians, scriptures, christian, heaven\n",
      "38.9\n",
      "17\n",
      "postscript, format, printer, fonts, files, formats, font, truetype, bitmap, image\n",
      "36.7\n",
      "18\n",
      "shipping, sale, offer, condition, asking, brand, sell, obo, price, selling\n",
      "36.0\n",
      "19\n",
      "atheists, belief, religion, beliefs, god, christianity, truth, religions, believe, atheist\n",
      "34.4\n",
      "20\n",
      "please, mail, post, email, posting, address, thanks, reply, interested, appreciate\n",
      "11.3\n",
      "____\n",
      "996.6\n",
      "14\n",
      "Table 2: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the 20 news\n",
      "groups dataset with 20 topics.\n",
      "Topic Number\n",
      "Topic Words\n",
      "PWI(T)\n",
      "1\n",
      "la, pit, gm, det, bos, tor, pts, chi, vs, min\n",
      "49.9\n",
      "2\n",
      "hz, cx, ww, uw, qs, c_, pl, lk, ck, ah\n",
      "47.0\n",
      "3\n",
      "ax, max, pl, di, tm, ei, giz, wm, bhj, ey\n",
      "42.6\n",
      "4\n",
      "db, period, goal, play, pp, shots, st, power, mov, bh\n",
      "27.9\n",
      "5\n",
      "mk, mm, mp, mh, mu, mr, mj, mo, mq, mx\n",
      "27.7\n",
      "6\n",
      "health, medical, new, study, research, disease, cancer, use, patients, drug\n",
      "19.0\n",
      "7\n",
      "armenian, people, said, one, armenians, turkish, went, us, children, turkey\n",
      "17.3\n",
      "8\n",
      "dos, windows, drive, card, system, disk, mb, scsi, pc, mac\n",
      "16.7\n",
      "9\n",
      "file, program, window, files, image, jpeg, use, windows, display, color\n",
      "15.7\n",
      "10\n",
      "government, president, law, would, mr, israel, state, rights, fbi, states\n",
      "13.4\n",
      "11\n",
      "god, jesus, bible, church, christian, christ, christians, faith, lord, man\n",
      "12.2\n",
      "12\n",
      "game, team, games, hockey, season, teams, league, nhl, new, players\n",
      "12.0\n",
      "13\n",
      "space, nasa, earth, launch, shuttle, orbit, moon, satellite, solar, mission\n",
      "11.8\n",
      "14\n",
      "edu, ftp, graphics, available, pub, image, mail, com, version, also\n",
      "9.7\n",
      "15\n",
      "would, know, anyone, get, thanks, like, one, please, help, could\n",
      "8.5\n",
      "16\n",
      "key, use, data, system, one, information, may, encryption, used, number\n",
      "7.5\n",
      "17\n",
      "people, would, one, think, know, like, say, even, see, way\n",
      "7.3\n",
      "18\n",
      "one, car, would, like, get, time, much, also, back, power\n",
      "7.1\n",
      "19\n",
      "edu, com, please, list, mail, sale, send, email, price, offer\n",
      "4.0\n",
      "20\n",
      "think, year, would, good, time, last, well, get, one, got\n",
      "3.6\n",
      "___\n",
      "360.9\n",
      "15\n",
      "Figure 6: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the 20 News\n",
      "Groups dataset. LDA* and PLSA* have stop-words removed.\n",
      "16\n",
      "Figure 7: Semantic embedding of 20 news groups messages labeled by news group. The 300 dimension document\n",
      "vectors are embedded into 2 dimensions using UMAP.\n",
      "Figure 8: The 20 news groups messages labeled with the top2vec topics from Table 1. The 300 dimension document\n",
      "vectors are embedded into 2 dimensions using UMAP.\n",
      "17\n",
      "Figure 9: Topic proportion of each LDA topic from Table 2 across all 20 news groups messages in the semantic\n",
      "embedding. The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded\n",
      "into 2 dimensions using UMAP.\n",
      "18\n",
      "Figure 10: Zoom in of top2vec original topics found in region of topic 7 from Table 1. This region of the semantic\n",
      "space corresponds to the talk.politics.mideast news group. The 300 dimension document vectors are embedded into 2\n",
      "dimensions using UMAP.\n",
      "Figure 11: Zoom in of top2vec original topics found in region of topic 14 from Table 1. This region of the semantic\n",
      "space corresponds to the talk.politics.guns news group. The 300 dimension document vectors are embedded into 2\n",
      "dimensions using UMAP.\n",
      "19\n",
      "Figure 12: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the Yahoo Answers\n",
      "dataset. LDA* and PLSA* have stop-words removed.\n",
      "20\n",
      "Table 3: Topic information gain for the top 10 words from top2vec topics trained on the Yahoo Answers dataset with\n",
      "10 topics.\n",
      "Topic Number\n",
      "Topic Words\n",
      "PWI(T)\n",
      "1\n",
      "overwrite, rebooting, debug, debugging, reboot, executable, compiler, winxp, xp, winnt\n",
      "112.2\n",
      "2\n",
      "securities, unpaid, equity, purchaser, payment, broker, underwriting, issuer, payable,\n",
      "104.4\n",
      "underwriter\n",
      "3\n",
      "regimen, discomfort, inflammation, swelling, psoriasis, puffiness, inflammatory,\n",
      "98.1\n",
      "irritation, edema, hypertension\n",
      "4\n",
      "realtionship, realationship, insecurities, confide, hurtful, inlove, clingy, friendship,\n",
      "92.5\n",
      "bestfriend, friendships\n",
      "5\n",
      "song, sings, singer, sang, artist, duet, album, lyrics, ballad, vocalist\n",
      "91.9\n",
      "6\n",
      "scripture, believers, righteousness, righteous, pious, spiritual, spirituality, sinful,\n",
      "82.2\n",
      "worldly, discernment\n",
      "7\n",
      "team, players, game, teams, scoring, league, teammate, scorers, playoff, defensively\n",
      "80.0\n",
      "8\n",
      "courses, subjects, curriculum, students, teaching, faculty, syllabus, academic,\n",
      "78.6\n",
      "undergraduate, baccalaureate\n",
      "9\n",
      "war, leaders, politicians, government, democracy, political, terrorists, terrorism,\n",
      "64.6\n",
      "partisan, policies\n",
      "10\n",
      "thus, constant, hence, surface, resulting, greater, therefore, becomes, occurs, larger\n",
      "33.1\n",
      "____\n",
      "837.6\n",
      "Table 4: Topic information gain for the top 10 words from LDA topics, after stop-word removal, trained on the Yahoo\n",
      "Answers dataset with 10 topics.\n",
      "Topic Number\n",
      "Topic Words\n",
      "PWI(T)\n",
      "1\n",
      "team, game, world, win, cup, play, de, football, best, player\n",
      "23.2\n",
      "2\n",
      "computer, yahoo, use, get, click, internet, free, com, need, windows\n",
      "22.3\n",
      "3\n",
      "people, us, country, war, world, would, american, bush, government, america\n",
      "18.0\n",
      "4\n",
      "one, water, two, would, light, number, energy, used, earth, use\n",
      "16.9\n",
      "5\n",
      "body, weight, also, doctor, eat, blood, may, day, get, pain\n",
      "15.7\n",
      "6\n",
      "www, com, http, find, song, name, know, anyone, org, music\n",
      "14.2\n",
      "7\n",
      "get, money, school, would, need, work, pay, good, business, job\n",
      "13.1\n",
      "8\n",
      "god, people, one, life, believe, jesus, word, many, would, us\n",
      "12.5\n",
      "9\n",
      "like, know, get, think, would, want, people, good, really, go\n",
      "9.1\n",
      "10\n",
      "time, like, friend, said, guy, back, would, one, years, got\n",
      "8.3\n",
      "____\n",
      "153.3\n",
      "21\n",
      "Figure 13: Semantic embedding of Yahoo Answers posts with true labels. The 300 dimension document vectors are\n",
      "embedded into 2 dimensions using UMAP.\n",
      "Figure 14: Yahoo Answers posts labeled with the top2vec topics from Table 3. The 300 dimension document vectors\n",
      "are embedded into 2 dimensions using UMAP.\n",
      "22\n",
      "Figure 15: Topic proportion of each LDA topic from Table 4 across all Yahoo Answers posts in the semantic embedding.\n",
      "The topics are ordered by decreasing information gain. The 300 dimension document vectors are embedded into 2\n",
      "dimensions using UMAP.\n",
      "23\n",
      "References\n",
      "[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,\n",
      "March 2003.\n",
      "[2] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM\n",
      "SIGIR conference on Research and development in information retrieval, pages 50–57, 1999.\n",
      "[3] Jordan L Boyd-Graber and David M Blei. Syntactic topic models. In Advances in neural information processing\n",
      "systems, pages 185–192, 2009.\n",
      "[4] S. Syed and M. Spruit. Full-text or abstract? examining topic coherence scores using latent dirichlet allocation. In\n",
      "2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 165–174, 2017.\n",
      "[5] Kai Yang, Yi Cai, Zhenhong Chen, Ho-fung Leung, and Raymond Lau. Exploring topic discriminating power\n",
      "of words in latent dirichlet allocation. In Proceedings of COLING 2016, the 26th International Conference on\n",
      "Computational Linguistics: Technical Papers, pages 2238–2247, 2016.\n",
      "[6] Geoffrey E Hinton et al. Learning distributed representations of concepts. In Proceedings of the eighth annual\n",
      "conference of the cognitive science society, volume 1, page 12. Amherst, MA, 1986.\n",
      "[7] Henry Widdowson. J.R. Firth, 1957, papers in linguistics 1934–51. International Journal of Applied Linguistics,\n",
      "17:402 – 413, 10 2007.\n",
      "[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector\n",
      "space, 2013.\n",
      "[9] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words\n",
      "and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119,\n",
      "2013.\n",
      "[10] Marco Baroni, Georgiana Dinu, and Germán Kruszewski. Don't count, predict! a systematic comparison of\n",
      "context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pages 238–247, Baltimore, Maryland, June\n",
      "2014. Association for Computational Linguistics.\n",
      "[11] Zhuang Bairong, Wang Wenbo, Li Zhiyu, Zheng Chonghui, and Takahiro Shinozaki. Comparative analysis of\n",
      "word embedding methods for dstc6 end-to-end conversation modeling track.\n",
      "[12] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in neural\n",
      "information processing systems, pages 2177–2185, 2014.\n",
      "[13] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation.\n",
      "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages\n",
      "1532–1543, 2014.\n",
      "[14] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model.\n",
      "Journal of machine learning research, 3(Feb):1137–1155, 2003.\n",
      "[15] Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa-\n",
      "tions. In Proceedings of the 2013 conference of the north american chapter of the association for computational\n",
      "linguistics: Human language technologies, pages 746–751, 2013.\n",
      "[16] Omer Levy, Yoav Goldberg, and Ido Dagan. Improving distributional similarity with lessons learned from word\n",
      "embeddings. Transactions of the Association for Computational Linguistics, 3:211–225, 2015.\n",
      "[17] Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents. ArXiv, abs/1405.4053,\n",
      "2014.\n",
      "[18] Jey Han Lau and Timothy Baldwin. An empirical evaluation of doc2vec with practical insights into document\n",
      "embedding generation. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 78–86,\n",
      "Berlin, Germany, August 2016. Association for Computational Linguistics.\n",
      "[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\n",
      "transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "[20] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as\n",
      "discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n",
      "[21] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\n",
      "generative pre-training.\n",
      "24\n",
      "[22] Thomas L Griffiths, Mark Steyvers, and Joshua B Tenenbaum. Topics in semantic representation. Psychological\n",
      "review, 114(2):211, 2007.\n",
      "[23] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representa-\n",
      "tions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for\n",
      "Computational Linguistics.\n",
      "[24] Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of\n",
      "the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.\n",
      "ELRA. http://is.muni.cz/publication/884893/en.\n",
      "[25] Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering based on hierarchical density\n",
      "estimates. In Pacific-Asia conference on knowledge discovery and data mining, pages 160–172. Springer, 2013.\n",
      "[26] Leland McInnes and John Healy. Accelerated hierarchical density based clustering. 2017 IEEE International\n",
      "Conference on Data Mining Workshops (ICDMW), Nov 2017.\n",
      "[27] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. The Journal of\n",
      "Open Source Software, 2(11):205, 2017.\n",
      "[28] RB Marimont and MB Shapiro. Nearest neighbour searches and the curse of dimensionality. IMA Journal of\n",
      "Applied Mathematics, 24(1):59–70, 1979.\n",
      "[29] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for\n",
      "dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n",
      "[30] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation\n",
      "and projection. The Journal of Open Source Software, 3(29):861, 2018.\n",
      "[31] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research,\n",
      "9(Nov):2579–2605, 2008.\n",
      "[32] M Cover Thomas and A Thomas Joy. Elements of information theory. New York: Wiley, 3:37–38, 1991.\n",
      "[33] Akiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Processing & Management,\n",
      "39(1):45–65, 2003.\n",
      "[34] Kenneth Ward Church and Patrick Hanks. Word association norms, mutual information, and lexicography.\n",
      "Computational Linguistics, 16(1):22–29, 1990.\n",
      "[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\n",
      "V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:\n",
      "Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n",
      "[36] Jamaal Hay Wenpeng Yin and Dan Roth. Benchmarking zero-shot text classification: Datasets, evaluation and\n",
      "entailment approach. In EMNLP, 2019.\n",
      "[37] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In\n",
      "Advances in neural information processing systems, pages 649–657, 2015.\n",
      "25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with fitz.open(fname) as doc:\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += fix_text(page.getText())\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_RATIO = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an extractive summarizer - it returns sentences from the document that are considered most relevant to the topic being discussed.  \n",
    "Abstractive summarizers will construct new sentences - but are also much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=model(text, SUMMARY_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP2VEC: DISTRIBUTED REPRESENTATIONS OF TOPICS\n",
      "Dimo Angelov\n",
      "dimo.angelov@gmail.com\n",
      "ABSTRACT\n",
      "Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a\n",
      "large collection of documents. Distributed representations of\n",
      "documents and words have gained popularity due to their ability to capture semantics of words and\n",
      "documents. We argue that the semantic space itself is a continuous\n",
      "representation of topics, in which each point is a different topic best summarized by its nearest words. We use this assumption to propose top2vec, a distributed topic vector which is calculated from\n",
      "dense areas of document vectors. This is due to the joint document and word embedding learning task, which is to predict\n",
      "which words are most indicative of a document, which necessitates documents, and therefore topic vectors, to be closest\n",
      "to their most informative words. The key insight into how doc2vec and word2vec learn these vectors is understanding how the prediction task works\n",
      "specifically for DBOW and skip-gram models. The prediction is softmax( ⃗wc · Dc,d), which generates a\n",
      "probability distribution over the corpus for each document being the document the word is from. The main hyper-parameter that needs be chosen for HDBSCAN is minimum cluster size; this parameter is at the core of\n",
      "how the algorithm finds clusters of varying density [26]. We have shown that the semantic space is a continuous representation of\n",
      "topics that allows for the calculation of topic vectors from dense areas of highly similar documents, topic size, and for\n",
      "hierarchical topic reduction. Our results show that top2vec consistently finds topics that are more informative and representative of\n",
      "the corpus than LDA and PLSA, for varying sizes of topics and number of top topic words. The 300 dimension document\n",
      "vectors are embedded into 2 dimensions using UMAP. The topics are ordered by decreasing information gain. This region of the semantic\n",
      "space corresponds to the talk.politics.guns news group. 19\n",
      "Figure 12: Topic information gain comparison between Top2Vec, PLSA, and LDA trained models on the Yahoo Answers\n",
      "dataset. [3] Jordan L Boyd-Graber and David M Blei. Exploring topic discriminating power\n",
      "of words in latent dirichlet allocation. In Proceedings of the 52nd Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pages 238–247, Baltimore, Maryland, June\n",
      "2014. Association for Computational Linguistics. Journal of machine learning research, 3(Feb):1137–1155, 2003. Linguistic regularities in continuous space word representa-\n",
      "tions. Improving language understanding by\n",
      "generative pre-training. [23] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. [30] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation\n",
      "and projection. In\n",
      "Advances in neural information processing systems, pages 649–657, 2015.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
